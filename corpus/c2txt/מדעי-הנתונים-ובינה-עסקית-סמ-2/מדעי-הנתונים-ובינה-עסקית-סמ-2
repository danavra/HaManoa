<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\בוחן-2015\Midterm_15_V01_Part2.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 4  
 
 
 
 
 
 
 נקודות[ 50] 2חלק 
  אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
 
הנהלה והעובדים.  הסכמי שכר בקנדה. הסכם מוגדר כ"טוב" אם נתקבל ע"י ה 40נתון עץ החלטה שנבנה מנתונים של 
 הסכם מוגדר כ"רע" אם אחד הצדדים דחה אותו.
 
 נקודות 10דיוק "חוק הרוב" במסד הנתונים הזה?  _____________ מהו  .א
 נקודות 10_____________ מהו דיוק האימון של העץ הנ"ל ?  .ב
 .  contrib-to-health-planעבור הפיצול עפ"י המשתנה  (Information Gainהרווח האינפורמטיבי )יש לחשב את  .ג
 נקודות. 15
 Information 
Gain: 
 Conditional 
entropy: 
 Entropy: 
 
 נקודות 15יש לבחון את כדאיות הפיצול עפ"י המבחן חי בריבוע.   .ד
Chi-square 
statistic: 
 Degrees of 
Freedom: 
 Conclusion:  
 
DF 1 2 3 4 5 6 
Chi-Square 3.841 5.991 7.815 9.488 11.070 12.592 
Page 4 of 4  
 
 
 
 
 
 
 דף הנוסחאות
Information Theory 
 Entropy H(X) = ∑−𝑝(𝑥)𝑙𝑜𝑔2𝑝(𝑥)  Conditional Entropy H(Y/X) = -   p(x, y)*log p (y/x) 
 Mutual Information I(X;Y) = H(Y) - H(Y/X) =   
yx yp
xyp
yxp
, )(
)/(
log),(  
 Conditional Mutual Information:  I(X;Y/Z) = H(X/Z) - H(X/Y,Z) =  
 Fano’s Inequality: H (Y/ X1… Xn)  H (Pe) + Pe log2 (m-1) 
Decision Trees 
 Confidence Interval for an Error Rate:  
)1(
n
ErrErr
zErr TestTestTest

   
 
Confidence Interval for a difference between error rates: 
 
 Expected information needed to classify a tuple in D (before using A): )(log)( 2
1
i
m
i
i ppDInfo 

  
 Expected information needed to classify a tuple in D (after using A): )(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

 
 Information Gain: (D)InfoInfo(D)Gain(A) A  
 
Chi-Square Statistic: 
 
 Apparent (pessimistic) error rate:  



yx zypzxp
zyxp
zyxp
, )/()/(
)/,(
log),,(
2
22
1
11 )1()1(ˆ
n
ErrErr
n
ErrErr
zd TestTestTestTest



 
))1)(1((~
'
)'( 2
1
2
1
0




cv
e
eo
H
v
i ij
ijij
c
j

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\בוחן-2016\Midterm_16.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 4  
 
 
 
 
 
 
 נקודות[ 50] 2חלק 
  אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
זיהוי "החלקיק ניסוי להמכיל תוצאות סימולציה של  HIGGS Data Setתצפיות מתוך מאגר הנתונים  20נתונות 
חלקיקים יתר ( ל1מסייעת לפיסיקאים להבדיל בין בוזון היגס )סיווג =  m_bb התכונה הרציפה  .)בוזון היגס( האלוהי"
 (. 0)סיווג = 
Record_ID 0 1 2 3 4 5 6 7 8 9 
Class 1 0 1 0 0 1 1 1 0 1 
m_bb 0.9 0.4 0.2 0.7 1.1 1.5 0.8 0.4 1.1 0.9 
           
Record_ID 10 11 12 13 14 15 16 17 18 19 
Class 0 0 1 0 1 0 1 1 0 0 
m_bb 2 0.7 0.9 0.3 1.1 0.4 0.7 0.9 1.3 3.4 
 
של משתנה המטרה  twoing-ומדד ה Gini-, מדד ה(Information Gainיש לחשב את הרווח האינפורמטיבי ) .א
"Class עבור נקודת הפיצול הבאה של המשתנה "m_bb :0.7 .30 נקודות. 
Interval Prob. 
(Interval) Prob. (0) Prob. (1) 
Entropy Information 
Gain 
Gini 
Index 
Gini 
Drop 
Twoing 
<= 0.7            
> 0.7            
Total         
 
 נקודות 10. 95%ביטחון של -ברמתהעץ המתקבל כתוצאה מהפיצול הנ"ל סמך לדיוק -יש לבנות רווח בר .ב
 
 נקודות. 5עבורו כדאי לגזום את העץ הנ"ל.  αיש לחשב את הערך המינימלי של מקדם הסיבוכיות  .ג
  
 נקודות. 5מתקיים עבור העץ שבניתם בסעיף א'.  Fanoשווין -יש לבדוק האם אי .ד
 
(:חובהפירוט החישוב )  
(:חובהפירוט החישוב )  
(:חובהפירוט החישוב )  
Page 4 of 4  
 
 
 
 
 
 
 דף הנוסחאות
Information Theory 
 Entropy H(X) = ∑−𝑝(𝑥)𝑙𝑜𝑔2𝑝(𝑥)  Conditional Entropy H(Y/X) = -   p(x, y)*log p (y/x) 
 Mutual Information I(X;Y) = H(Y) - H(Y/X) =   
yx yp
xyp
yxp
, )(
)/(
log),(  
 Conditional Mutual Information:  I(X;Y/Z) = H(X/Z) - H(X/Y,Z) =  
 Fano’s Inequality: H (Y/ X1… Xn)  H (Pe) + Pe log2 (m-1) 
Decision Trees 
 Confidence Interval for an Error Rate:  
)1(
n
ErrErr
zErr TestTestTest

   
 
Confidence Interval for a difference between error rates: 
 
 Expected information needed to classify a tuple in D (before using A): )(log)( 2
1
i
m
i
i ppDInfo 

  
 Expected information needed to classify a tuple in D (after using A): )(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

 
 Information Gain: (D)InfoInfo(D)Gain(A) A  
 
Chi-Square Statistic: 
 
 Apparent (pessimistic) error rate:  
 Entropy induced by threshold T: )(
||
||
)(
||
||
);,( 2
2
1
1 SEnt
S
S
SEnt
S
S
STAE   
 Split Information: )
||
||
(log
||
||
)( 2
1 D
D
D
D
DSplitInfo
j
v
j
j
A  

 
 Gini Index: 



n
j
p jTgini
1
21)(
 
 Twoing Splitting Rule: 
2
)/()/(
4







j
RL
RL tjptjp
pp
 
 Cost-complexity function (CART): TTRTR
~
)()(    



yx zypzxp
zyxp
zyxp
, )/()/(
)/,(
log),,(
2
22
1
11 )1()1(ˆ
n
ErrErr
n
ErrErr
zd TestTestTestTest



 
))1)(1((~
'
)'( 2
1
2
1
0




cv
e
eo
H
v
i ij
ijij
c
j

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\בוחן-2017---פתרון-חלק-2\2017_sol.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
 החלק החישובי פתרון  – 2017בוחן 
 סעיף א' 
 נתונים: 
    Class (Buy car) 
  
Sunroof Air Bags No Yes 
0 0 10 5 
0 1 5 10 
1 2 5 0 
1 0 0 40 
0 1 30 20 
0 2 15 15 
1 0 10 5 
1 1 25 5 
 תשובות סופיות: 
  Sunroof Air bags Class 
Entropy / Split 
Info 0.993 1.480 1.000 
Cond. Entropy 0.993 0.925   
Info. Gain 0.007 0.075   
Info. Gain Ratio 0.007 0.050   
 : sunroofספירת מקרים עבור 
sunroof No Yes TOTAL 
0 60 50 110 
1 40 50 90 
TOTAL 100 100 200 
 :air bagספירת מקרים עבור 
AIR BAG No Yes TOTAL 
0 20 50 70 
1 60 35 95 
2 20 15 35 
TOTAL 100 100 200 
 חישוב השורה הראשונה בטבלה: 
Split info (sunroof) = -(110/200)*log2(110/200)-(90/200)*log2(90/200)=0.993 
Split info (air bag) = -(70/200)*log2(70/200)-(95/200)*log2(95/200)-
(35/200)*log2(35/200)=1.480 
Entropy (class) = -(100/200)*log2(100/200)-(100/200)*log2(100/200)=1 
 חישוב השורה השנייה בטבלה: 
Cond entropy (sunroof) = (110/200)*[-(60/110)*log2(60/110)-(50/110)* 
log2(50/110)]+(90/200)*[-(40/90)*log2(40/90)-(50/90)*log2(50/90)]=0.993 
Cond entropy (air bag) = (70/200)*[-(20/70)*log2(20/70)-(50/70)* log2(50/70)]+(95/200)*[-
(60/95)*log2(60/95)-(35/95)*log2(35/95)]+(35/200)*[-(20/35)*log2(20/35)-
(15/35)*log2(15/35)]=0.925 
 חישוב השורה השלישית בטבלה: 
Information gain (sunroof) = entropy(class) – cond entropy (sunroof) = 1 – 0.993 = 0.007 
Information gain (air bag) = entropy (class) – cond entropy (air bag) = 1 – 0.925 = 0.075 
 חישוב השורה הרביעית בטבלה: 
Gain ratio (sunroof) = information gain (sunroof) / split info (sunroof) = 0.007/0.993 = 0.007 
Gain ratio (air bag) = information gain (air bag) / split info (air bag) = 0.075/1.480 = 0.050 
 
הגבוה  gain ratio-מכיוון שלו יש את ערך ה air bagנבחר לפצל את קודקוד השורש לפי המשתנה 
 (.sunroofעבור  0.007לעומת  air bagעבור  0.050יותר )
 ,1 ,0הפיצול השונים )עם ציון ערכי  air bagעץ תקין צריך להכיל קודקוד שורש, פיצול לפי המשתנה 
 (, ועלי העץ צריכים להכיל את התפלגות משתנה המטרה. 2
 סעיף ב' 
 לפני ואחרי גיזום העץ. עבור העץ שנבחר בסעיף א'  pessimistic errorיש לחשב 
Pessimistic error before pruning:  
q(T) = [(20+35+15) + (0.5)*3]/200 = 71.5/200 = 0.357 
pessimistic error after pruning:  
q(v) = (100+0.5)/200 = 100.5/200 = 0.502 
 : sunroofבמידה ונעשתה טעות ובסעיף א' נבחר המשתנה 
Pessimistic error before pruning:  
q(T) = [(50+40) + (0.5)*2]/200 = 91/200 = 0.455 
 
 
 
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2008-מועד-א'\Final_08_A.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 2 of 2 
 
  ]נקודות 60[ 2'  שאלה מס
 !יש לפרט את כל החישובים וההנחות: חשוב
  
  :י הפירוט הבא"ממדיות עפ-נתונות שמונה תצפיות דו
Observation A1 A2 A3 B1 B2 B3 C1 C2 
x1 2 2 8 5 7 6 1 4 
x2 10 5 4 8 5 4 2 9 
 
ל  באמצעות האלגוריתם "עבור התצפיות הנ של אשכולות העץ המלאיש לבנות את   .א
AGNES (Agglomerative Nesting) נקודות 30. האוקלידימרחק תוך שימוש ב.  
 והאיטרציה האחרונה הראשונהאחרי האיטרציה יש לחשב את האנטרופיה של כל אשכול   .ב
 class (. 20(מייצגות את סיווג התצפית  A, B, Cשהאותיות  בהנחהשל האלגוריתם 
 .תנקודו
 10. הראשונה בלבדאחרי האיטרציה  של כל אשכול) centroid(יש לחשב את מרכז הכובד   .ג
 .נקודות
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2008-מועד-ב'\Final_08_B.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
  ]נקודות 60[ 2'  שאלה מס
 !יש לפרט את כל החישובים וההנחות: חשוב
  
  :}{A, B, C, D, Eפריטים הטרנזקציות על  8נתונות 
tid items
1 {A,B}
2 {A,B,C}
3 {B,C,D}
4 {B,C}
5 {A,B,C,D}
6 {B,D}
7 {B,E}
8 {B,C,D,E}
 
בהינתן  Apriori באלגוריתם שימוש ךתו (frequent itemsets) השכיחים הצירופים כל את מצואליש   .א
minimum support  נק 20. (אלגוריתםהציינו כל שלב של   .35% -שווה ל( 
 בהינתן 'בסעיף א םתאשמצ צירופיםמ (Association Rules)החוקים  כל אתלחשב יש   .ב
minimum confidence נק 10. (70% - שווה ל( 
. של יתר הפריטים טרנזקציות הנתונותהבהינתן  C קניית פריטללבנות מודל מנבא אתם צריכים שנניח   .ג
לצורך הרצת  תציגו את הטרנזקציות באיזה אופן. ID3להשתמש באלגוריתם  עליכםנניח גם ש
 )נק 10(? האלגוריתם
הפעלת החלטה המתקבל מהראשונה של עץ הרמה ה תציגו את, 'בסעיף ג שהגדרתםייצוג הבהינתן   .ד
 )נק pre-pruning) .20ללא  Information Gainמבוסס על ה ID3האלגוריתם 
   
  
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2009-מועד-א'\Final_09_A01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 4 
 
 [נקודות 60] 'חלק ב
בטבלה  מציינת את מספר העובדים " count"העמודה .  נתונה טבלת סיכום של נתוני עובדים
י השורה "עפ, למשל(.  משכורת+ קבוצת גיל +  ססטאטו+ מחלקה )בעלי מאפיינים מסוימים 
נמצאים בסטאטוס בכיר , (sales)עובדים שייכים למחלקת המכירות  03, הראשונה בטבלה
(senior) , 46ומקבלים משכורת בין  03-03הם בניk 50-לk סטאטוס "משתנה המטרה הוא  .בשנה
 (.Status" )העובד
Department Status Age Salary Count 
Sales Senior 31..35 46k..50k 30 
Sales Junior 26..30 26k..30k 40 
Sales Junior 31..35 31k..35k 40 
Systems Junior 21..25 46k..50k 20 
Systems Senior 31..35 66k..70k 5 
Systems Junior 26..30 46k..50k 3 
Systems Senior 41..45 66k..70k 3 
Marketing Senior 36..40 46k..50k 10 
Marketing Junior 31..35 41k..45k 4 
Secretary Senior 46..50 36k..40k 4 
Secretary Junior 26..30 26k..30k 6 
 
: אחד המשתנים הבאיםשל סטאטוס העובד בהינתן  האנטרופיה המותניתיש לחשב את  .א
 .נקודות 51 .בכל שורה תשובה אחת בלבדיש לסמן  .קבוצת גיל או משכורת, מחלקה
Department 0.049 0.497 0.850 0.899 תשובה 
Age 0.047 0.474 0.486 0.540 תשובה 
Salary 0.362 0.380 0.501 0.538 ובהתש  
 
 .נקודות 1 .תשובה אחת בלבדיש לסמן  ?ל"על נתוני האימון הנ" חוק הרוב"מהו דיוק  .ב
 תשובה  1.000 0.805 0.685 0.500
 
אחד המשתנים  על תהמבוססבעל רמה אחת בלבד מהם הסיווגים החזויים בעץ החלטה  .ג
 51 .בכל טבלה שורה אחת בלבדיש לסמן  ?קבוצת גיל או משכורת, מחלקה: הבאים
 .נקודות
 Department: 
 תשובה
 Sales Systems Marketing Secretary 
1 Senior Senior Senior Junior 
2 Senior Senior Junior Junior 
3 Senior Junior Junior Senior 
4 Junior Junior Senior Junior 
 
Page 4 of 4 
 
Age: 
 תשובה
 21..25 26..30 31..35 36..40 41..45 46..50 
1 Junior Junior Senior Senior Senior Senior 
2 Junior Junior Junior Senior Senior Senior 
3 Senior Senior Junior Junior Junior Junior 
4 Senior Junior Junior Senior Senior Senior 
 
Salary: 
 תשובה
 26k..30k 31k..35k 36k..40k 41k..45k 46k..50k 66k..70k 
1 Junior Junior Senior Junior Senior Senior 
2 Junior Junior Junior Senior Senior Senior 
3 Junior Junior Senior Senior Senior Senior 
4 Junior Junior Junior Senior Junior Senior 
 
אחד  יש לחשב את דיוק האימון של עץ החלטה בעל רמה אחת בלבד המבוססת על .ד
בכל  אחת בלבד תשובהיש לסמן . קבוצת גיל או משכורת, מחלקה: המשתנים הבאים
 .נקודות 51 .שורה
Department 0.685 0.709 0.721 0.788  תשובה 
Age 0.733 0.788 0.807 0.823  תשובה 
Salary 0.758 0.861 0.904 0.953  תשובה 
 
 (NN-k)השכנים הקרובים ביותר  kיש לסווג את התצפיות הבאות באמצעות האלגוריתם  .ה
 :(נקודות 51)תוך שימוש בהנחות הבאות 
3) k=1 
" התאמה פשוטה"כל המשתנים נומינליים והמרחק ביניהם נמדד באמצעות  (2
(Simple matching.) 
 (.count = 1) תצפית אחת בלבדל מייצגת "כל שורה בטבלת הנתונים הנ (0
 בכל שורה תשובה אחת בלבדיש לסמן 
Record Department Age Salary Senior Junior שני הסיווגים  
1 Sales 31..35 26k..30k       תשובה 
2 Sales 26..30 26k..30k       תשובה 
3 Systems 31..35 66k..70k       תשובה 
4 Marketing 36..40 46k..50k       תשובה 
5 Marketing 31..35 66k..70k       תשובה 
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2009-מועד-ב'\Final_09_B01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 6 
 
 [נקודות 60]' חלק ב
העמודות הפריטים בטבלה  . כל טרנזקציה מייצגת קניה אחת. נתונה טבלת סיכום של נתוני קניות
טרנזקציה \בקניה, י השורה הראשונה בטבלה"עפ, למשל.  מציינות את הנוכחות הפריטים בקניה
 .יפס'חלב וצ, קטשופ, בירה, אחת נקנו לחם
ID Bread (Br) Beer (B) Ketchup (K) Milk (M) Cola (C) Chips (Ch) 
1 1 1 1 1 0 1 
2 0 1 0 0 1 1 
3 1 0 0 0 1 1 
4 0 0 0 1 1 0 
5 1 0 1 0 1 1 
6 1 1 1 0 0 1 
7 1 1 0 0 1 0 
8 1 1 0 1 1 1 
9 1 0 1 1 1 0 
10 1 0 1 0 0 0 
11 0 0 1 1 0 0 
 
. עבור זוג הכי שכיח support-הזוג הפריטים הכי נדיר ואת  עבור support-היש לחשב את  .א
 .נקודות 01. בכל שורה תשובה אחת בלבדיש לסמן 
 
נדיר הכי זוג  תשובה 0% 11% 22% 33% 
שכיח הכי זוג  תשובה 22% 33% 45% 23% 
 
יש ? מהו גודל של קבוצת הפריטים השכיחה הכי גדולה min support = 30%בהינתן  .ב
 .נקודות 5. תשובה אחת בלבדלסמן 
 
  תשובה 4 1 3 2
 
יש לסמן  Bread  Beer ^ Chips?: של החוק הבא support-וה confidence-מהם ה .ג
 .נקודות 01. בכל שורה תשובה אחת בלבד
 
support 18% 22% 0% 100% תשובה  
confidence 38% 63% 75% 50% תשובה  
 
 
 
 
 
Page 4 of 6 
 
 
 5 .אחת בלבד תשובהיש לסמן ? טרנזקציות הראשונות 3הנכון עבור ( FP-tree)מהו העץ  .ד
 .נקודות
 FP-tree תשובה
1  
2  
3  
4  
 
 
{}
Br
B
K
M
C
Ch
B:1
Br:2
C:1
K:1
M:1
Ch:1
B:1
Ch:1
C:1
Ch:1
{}
Br
B
K
M
C
Ch
B:2
Br:2
C:2
K:1
M:1
Ch:1 Ch:2
{}
Br
C
K
Ch
B
M
Br:2
B:1
K:1
Ch:1
M:1
C:1 C:1
B:1
Ch:1 Ch:1
{}
Br
C
K
Ch
B
M
Br:2
B:1
K:1
Ch:1
M:1
C:2
B:1
Ch:2
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2010-מועד-א'\Final_10_A01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 4 
 
 [נקודות 50] 'חלק ב
 !אם צוין אחרת יש להציג את כל התוצאות עם שלוש ספרות אחרי נקודה עשרונית אלא
 
מטרת הטבלה לחזות את הסתברות הנטישה .  תנתונים של לקוחות בחברה סלולארינתונה טבלת 
(churn )של לקוח. 
' מס
 15 14 13 12 11 11 9 8 7 6 5 4 3 2 1 רשומה
דקות 
שיחה 
בשנה 
 2093 994 2534 1407 294 2268 595 798 490 1141 1176 77 1680 658 1029 הקודמת
דקות 
שיחה 
בשנה 
 3560 228 292 1204 0 0 2044 2892 2060 164 2912 0 2952 3156 3216 האחרונה
 כן כן כן כן כן לא לא לא לא לא לא לא לא לא לא נטישה 
 
 
של משתנה המטרה  twoing-מדד הו Gini-מדד ה, האנטרופיה המותניתיש לחשב את  .א
: "דקות שיחה בשנה האחרונה"שתי נקודות הפיצול הבאות של המשתנה  עבור "נטישה"
 .נקודות 42. 1214, 228
Threshold Conditional 
Entropy 
Gini Index Twoing 
228    
1204    
 
 
 
 
מה יהיה השינוי בהסתברות הנטישה כתוצאה מירידת , י החישובים בסעיף הקודם"עפ .ב
 .נקודות 2 ?סףערך מתחת לכל  בשנה האחרונהשיחה הדקות 
Threshold Churn Probability Change  
228  
1204  
 
  
Page 4 of 4 
 
דקות שיחה "ו" דקות שיחה בשנה האחרונה" יםיש לנרמל את כל הערכים של המשתנ .ג
 אוכלוסיהתקן של התוך חישוב סטיית  z-score normalizationבשיטת " בשנה הקודמת
כל לסכם את הערכים המנורמלים של  יש בעמודה האחרונה.  בגודל טבלת הנתונים
 .נקודות 01. משתנה
' מס
 15 14 13 12 11 11 9 8 7 6 5 4 3 2 1 רשומה
 כ"סה
דקות 
שיחה 
בשנה 
 הקודמת
               
 
דקות 
שיחה 
בשנה 
 האחרונה
               
 
 
 
 
 :בעל הפרמטרים הבאים Linear SVMבעזרת המודל  המנורמלותיש לסווג את התצפיות  .ד
Total number of Support Vectors: 15 
Bias (offset): -0.346 
w[ בשנה הקודמתדקות שיחה  ] = 0.429 
w[0.506- = [דקות שיחה בשנה האחרונה 
 
 ". -0"י "הנטישה מיוצגת ע-בשעה שאי" 0"המודל מניח שסיווג הנטישה הוא : שימו לב
 נקודות 8. יש לסכם את מספר השגיאות של המודל בעמודה האחרונה
' מס
 15 14 13 12 11 11 9 8 7 6 5 4 3 2 1 רשומה
כ "סה
 שגיאות
 חזויסיווג 
               
 
כן )שגיאה 
 (לא/
               
 
 
 
 .95%ביטחון של -ברמת Linear SVM  המודל לדיוקסמך -יש לחשב את רווח בר .ה
 .נקודות 2
Lower Bound of Accuracy Upper Bound of Accuracy 
  
 
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2010-מועד-ב'\Final_10_B01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 5 
 
 [נקודות 50] 'חלק ב
 !יש להציג את כל התוצאות עם שלוש ספרות אחרי נקודה עשרונית אלא אם צוין אחרת
 
מטרת הטבלה לחזות את הסתברות הנטישה .  תנתונים של לקוחות בחברה סלולארינתונה טבלת 
(churn ) לקוחשל. 
' מס
 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 רשומה
דקות 
שיחה 
בשנה 
 3560 228 292 1204 0 0 2044 2892 2060 164 2912 0 2952 3156 3216 האחרונה
 כן כן כן כן כן לא לא לא לא לא לא לא לא לא לא נטישה 
 
 (09%מובהקות של ) התפלגות חי בריבוע
DF 1 2 3 4 5 6 
Chi Square (09%) 2.706 4.605 6.251 7.779 9.236 10.645 
 
ל תוך שימוש ברמת ביטחון של "הנ טבלת הנתונים עבור (IFN)עמומה -יש לבנות רשת אינפו
דקות שיחה בשנה "הפיצול הבאות של המשתנה נקודות את  אך ורקלבחון  עליכם  .%90
 .3156 ,2044 ,1204 ,228: "האחרונה
 :נקודות 81יש לפרט את חישובי האלגוריתם בטבלה הבאה  .א
 
כמות 
תצפיות 
לפני 
 הפיצול
האינטרוואל 
 הראשון
 (-עד  -מ)
האינטרוואל 
 השני
 (-עד  -מ)
הסתברות 
הנטישה 
באינטרוואל 
 הראשון
הסתברות 
הנטישה 
באינטרוואל 
 השני
האינפורמציה 
ההדדית 
 המותנית
G
לפצל  2
/ כן )
 (לא
        
        
        
        
        
        
        
        
        
        
 
  
Page 4 of 5 
 
יש להציג את הרשת שבניתם בצורה גראפית תוך מספור כל הקודקודים וציון הסיווג  .ב
  :נקודות 89!( מגבולות המסגרתלחרוג  מבלי)החזוי בכל קודקוד טרמינלי 
 
 
 
של כל החוקים המתקבלים ( connection weights) משקולות החיבוריש לחשב את  .ג
 :נקודות 1מהרשת 
קודקוד ' מס חוק' מס
 טרמינלי
דקות שיחה 
בשנה 
  -מ) האחרונה
 (-עד
סיווג 
נטישה 
 (לא/ כן )
הסתברות 
 הסיווג
 משקולת החיבור
      
      
      
      
      
      
      
      
      
כ "סה
 משקולות
     
  
Page 5 of 5 
 
 (zα = 1.96) 95%ביטחון של -ברמת IFN  לדיוק המודלסמך -יש לחשב את רווח בר .ד
 .נקודות 4
Lower Bound of Accuracy Upper Bound of Accuracy 
  
 
 
לשלושה אינטרוואלים  "דקות שיחה בשנה האחרונה" האת כל הערכים של המשתנ לחלקיש 
בעמודה   .ולהחליק אותם לפי הממוצע של כל אינטרוואל (equal depth)בעלי עומק שווה 
 .נקודות 89. המוחלקיםהערכים  של לחשב את סטיית התקן יש האחרונה
' מס
 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 רשומה
סטיית 
 תקן
דקות 
שיחה 
בשנה 
 האחרונה
               
 
 
 
 
 
 
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2011-מועד-א'\Final_11A.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 1 of 2  
 
 
 
[ נקודות 50] 1חלק 
 יש לענות על כל השאלות 
  נקודות 5 –משקל של כל שאלה 
  גבי שאלון הבחינה-על במקום המיועד לכךברור  יד-בכתב את התשובה לרשוםיש 
 בלבד
  תקבל ציון של אפס( במידה ונדרשת הנמקה)תשובה לא מנומקת 
 .Overfittingיש להסביר בקצרה מהי תופעת  .א
___________________________________________________________________ 
 ___________________________________________________________________
 הן שכיחות (itemset) קבוצת פריטים אם כל תתי הקבוצות האפשריות של: לא נכון/נכון .ב
frequent)) ,אז גם ה-itemset  עצמו הואfrequent.   בקצרה את תשובתכם לנמקיש. 
___________________________________________________________________ 
 ___________________________________________________________________
חד מהצד הימני של החוק לצד א פריטאם נעביר , Association Rules-ב: לא נכון/נכון .ג
מעבר : דוגמא.  )של החוק לעולם לא יהיה נמוך יותר confidence-אז ה, השמאלי של החוק
 .יש לנמק בקצרה את תשובתכם . (X,YZלחוק   XY,Zמחוק 
___________________________________________________________________ 
 ___________________________________________________________________
 ?Naïve Bayes Classifierמהי ההנחה הבסיסית של  .ד
___________________________________________________________________ 
 ___________________________________________________________________
 11/07/2011: תאריך הבחינה
 מרק לסט' פרופ:  שם המרצה
 כריית נתונים ומחסני נתונים:  שם הקורס
-372: מספר הקורס 1-3105 
 'א:  מועד  ' ב:  סמסטר   2011:  שנה
 שעות 3: משך הבחינה
 מחשבון+  (מצורף לבחינה)נוסחאות דף :  חומר עזר
 1' גרסה מס
 !מותר להשתמש במחשבון
Page 2 of 2  
 
 
 .התכונות של אנתרופיה שלושג את יהציש ל .ה
1. ______________________________________________________________ 
2. ______________________________________________________________ 
3. ______________________________________________________________ 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2011-מועד-ב'\Final_11_B01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 1 of 6  
 
 
 
 [נקודות 50] 1חלק 
 יש לענות על כל השאלות 
  נקודות 5 –משקל של כל שאלה 
  גבי שאלון הבחינה-על במקום המיועד לכךברור  יד-בכתב את התשובה לרשוםיש 
 בלבד
  תקבל ציון של אפס( במידה ונדרשת הנמקה)תשובה לא מנומקת 
למצוא בבסיס נתונים של  שהייתם מצפיםשתי דוגמאות של תבניות הסתברותיות  להציגיש  .א
 .חברת נסיעות גדולה
1. _______________________________________________________________ 
2. _______________________________________________________________ 
 knowledge discovery in)י נתונים מה השלב הראשון בתהליך של גילוי ידע בבסיס .ב
databases)? 
___________________________________________________________________ 
של החוק לצד  השמאליאחד מהצד  פריטאם נעביר , Association Rules-ב: לא נכון/נכון .ג
מעבר : דוגמא.  )ה נמוך יותרשל החוק לעולם לא יהי confidence-אז ה, של החוק הימני
 .יש לנמק בקצרה את תשובתכם . (XY,Z  לחוק  X,YZמחוק
___________________________________________________________________ 
___________________________________________________________________ 
יש לנמק בקצרה . בדיקת השערות סטטיסטית מהווה פעולה של כריית מידע :לא נכון/נכון .ד
 .את תשובתכם
___________________________________________________________________ 
___________________________________________________________________ 
 2011/0/22/: תאריך הבחינה
 מרק לסט' פרופ : שם המרצה
 כריית נתונים ומחסני נתונים : שם הקורס
 3103-1-3/2 :מספר הקורס
 'א:  מועד  ' ב:  סמסטר   2011:  שנה
 שעות 3 :משך הבחינה
 מחשבון+  (מצורף לבחינה)נוסחאות דף  : חומר עזר
 1' גרסה מס
 !מותר להשתמש במחשבון
Page 2 of 6  
 
 
. ים של כמות הגשמים היורדים ברחבי הארץשנתי-עליכם לבנות מחסן נתונים עבור נתונים רב .ה
 ?מחסן זהתרשים הכוכב של ב( facts)והעובדות ( dimensions)מה יהיו המימדים 
 ___________________________________________________(: לפחות אחת)עובדות 
 _______________________________________________ (: לפחות שניים)מימדים 
_______________________________________________ 
 ?של מחסני נתונים"( נדיפות-אי)" Non-volatilityמשמעות התכונה מה  .ו
___________________________________________________________________ 
___________________________________________________________________ 
 ?נתוןהמחושבת עבור משתנה מקרי  של אנתרופיה הערכיםמהו תחום  .ז
______________________________________________________________ 
 ?ן שני אשכולותהמרחק בילחישוב  complete link-ו single linkמה הבדל בין השיטות  .ח
___________________________________________________________________ 
___________________________________________________________________ 
___________________________________________________________________ 
 (missing data)לטיפול בערכים חסרים שונות  שיטות לפחות שתייש לציין  .ט
1. ______________________________________________________________ 
2. ______________________________________________________________ 
 ?(Info-Fuzzy Networks)עמומות -במודלים של רשתות אינפו( שכבה)מה מאפיין כל רמה  .י
___________________________________________________________________ 
___________________________________________________________________ 
___________________________________________________________________ 
Page 3 of 6  
 
 [נקודות 50] 2חלק 
  אלא אם צוין אחרת שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם! 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
 :הבא המסוכםנתון בסיס הנתונים 
 
# id Age Gender Income 
Marital 
Status 
Car Type Count 
Family Luxury Sports 
1 25-33 Male 10,000 Married 10 5 7 
2 34-45 Female 12,000 Single 6 8 9 
3 46-55 Female 16,000 Divorced 5 4 19 
4 56+ Female 7,000 Single 4 6 10 
5 25-33 Male 16,000 Widowed 20 4 7 
6 34-45 Male 12,000 Divorced 8 9 10 
7 46-55 Female 7,000 Married 4 2 0 
8 56+ Male 7,000 Single 3 1 7 
9 46-55 Male 12,000 Widowed 9 5 1 
10 56+ Female 10,000 Divorced 3 7 31 
11 56+ Female 7,000 Married 42 2 1 
12 25-33 Male 10,000 Single 20 3 2 
13 34-45 Male 16,000 Single 0 10 13 
14 46-55 Male 12,000 Divorced 1 5 8 
15 25-33 Female 7,000 Married 17 0 2 
 
Attribute Type Use 
# id Numeric ID 
Age Nominal Input 
Gender Nominal Input 
Income Numeric Input 
Marital Status Nominal Input 
Car Type  Nominal Decision Class 
 
 :שימו לב
 משתנה ה-#id הוא לשימוש בסעיף ו' . 
  העמודות"“Car Type Count המשתנה  מייצגות עבור כל אחד מהערכים האפשריים שלCar 
Type (Family, Luxury, Sports)  לדוגמא. המקורי בבסיס הנתונים המופעיםאת מספר ,
 :בבסיס הנתונים המסוכם id=1#עבור הרשומה 
- Car Type: Family = 10   רשומות בבסיס הנתונים המקורי כמו הרשומה  10יש
 Familyוהסיווג שלהן הינו  id=1#שבה 
Page 4 of 6  
 
 נקודות Car Type .01 -ו Age יםיש לחשב  אנתרופיה בלתי מותנית עבור המשתנ .א
 :ותהבא אותבטבל יםחישובהנא להציג את 
 
Age Count Probability Entropy 
 
Car Type Count Probability Entropy 
        
 
        
        
 
        
        
 
        
        
 
        
        
 
        
Total       
 
Total       
 
 
" Car Type" הסיווגשל משתנה  (Information Gain)שב את הרווח האינפורמטיבי יש לח .ב
יש לציין בעיגול את ". Income" הרציף עבור כל נקודות הפיצול האפשריות של המשתנה
 .נקודות 01. נקודת הפיצול הטובה ביותר
  
Left 
Interval   
Right 
Interval   Total Information 
Threshold Percentage Entropy Percentage Entropy Entropy Gain 
              
              
              
              
              
 
 min-max normalization בשיטת Income היש לנרמל את כל הערכים של המשתנ .ג
 . נקודות 5. שבין אפס לאחדלתחום 
# id Income Normalized Income 
 
# id Income 
Normalized 
Income 
1 10,000   
 
9 12,000   
2 12,000   
 
10 10,000   
3 16,000   
 
11 7,000   
4 7,000   
 
12 10,000   
5 16,000   
 
13 16,000   
6 12,000   
 
14 12,000   
7 7,000   
 
15 7,000   
8 7,000   
 
Min = 
Max = 
    
Page 5 of 6  
 
         .נקודות K=1    .01כאשר , K-NNידי שימוש באלגוריתם -סווג את הרשומה הבאה על .ד
          
# id Age Gender Income Marital Status 
Car Type 
61 46-55 Female 00288 Married 
                                                 
 :בהנחות הבאותעליך להשתמש 
  תצפית אחתיש להתייחס לכל שורה בטבלת הנתונים המסוכמת כאילו שהיא מייצגת 
 .חוק הרובי "שהסיווג שלה נקבע עפ
 חישוב המרחקים: 
 .בלבד Income, Marital Status: חישוב המרחק יתבסס על המשתנים -
י "עפשנקבעו  הערכים המנורמליםבין חשב את המרחק  –עבור משתנה נומרי  -
 .בסעיף הקודםהשיטה המוגדרת 
 .Simple matchingחשב את המרחק על סמך  –עבור משתנה נומינלי  -
 
# 
id Income 
Marital 
Status Car Type income-distance 
marital status-
distance total distance 
1 10,000 Married         
2 12,000 Single         
3 16,000 Divorced         
4 7,000 Single         
5 16,000 Widowed         
6 12,000 Divorced         
7 7,000 Married         
8 7,000 Single         
9 12,000 Widowed         
10 10,000 Divorced         
11 7,000 Married         
12 10,000 Single         
13 16,000 Single         
14 12,000 Divorced         
15 7,000 Married         
16 
Minimal 
distance           
 
בדומה לסעיף ) מייצגת תצפית אחת בלבדבטבלת הנתונים המסוכמת בהנחה שכל רשומה  .ה
לניתוח  k-meansשל האלגוריתם  בלבד שונההרא איטרציההאת  לבצעיש , (הקודם
 נקודות Age, Gender, Marital Status. 05: הבאיםהמשתנים הנומינליים אשכולות על 
 :בהנחות הבאותעליך להשתמש 
 K=4 (מספר האשכולות שווה לארבעה.) 
Page 6 of 6  
 
  1' נתונה בטבלה של האיטרציה מס לאשכולותשל התצפיות  הראשוניתהחלוקה 
 "(.old cluster"בעמודה )
 לחישוב המרחקים יש להשתמש ב-Simple Matching ,בין תצפית לאשכול המרחק 
 (.centroid) תצפית לווקטור המייצג את האשכול ביןכמרחק חושב י
  הערכים של הווקטור המייצג את האשכול(centroid) לפי ה ייקבעו-majority rule . 
יש לקבוע את הערך המייצג , מסוים משתנההערכים של שכיחות במידה ויש שוויון בין 
 :לדוגמא .הנמוך ביותר id#-לפי הערך של התצפית עם ה
# id Age Gender 
1 25-33 Male 
2 34-45 Female 
3 46-55 Female 
Centroid 25-33 Female 
 
 (:centroids)הווקטורים המייצגים 
Cluster Age Gender Marital Status 
1       
2       
3       
4       
 
 :1 'מס איטרציה
# id Age Gender 
Marital 
Status 
old 
cluster 
Distance to 
cluster1 
Distance to 
cluster2 
Distance to 
cluster3 
Distance to 
cluster4 
new 
cluster 
1 25-33 Male Married 1 
     5 25-33 Male Widowed 1 
     9 46-55 Male Widowed 1 
     13 34-45 Male Single 1 
     2 34-45 Female Single 2 
     6 34-45 Male Divorced 2 
     10 56+ Female Divorced 2 
     14 46-55 Male Divorced 2 
     3 46-55 Female Divorced 3 
     7 46-55 Female Married 3 
     11 56+ Female Married 3 
     15 25-33 Female Married 3 
     4 56+ Female Single 4 
     8 56+ Male Single 4 
     12 25-33 Male Single 4 
      
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2012-מועד-א'\Final_12_A01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 6  
 
 [נקודות 50] 2חלק 
  אלא אם צוין אחרת שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם! 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
 
 .קרא בעיון את השאלה ואת הנתונים הבאים
 :נתון בסיס הנתונים הבא
Purchase Vehicle Dataset 
First Hand Sunroof Air bags 
Class (Buy car) 
No Yes 
No No 0 10 5 
No No 1 5 10 
No Yes 2 5 0 
No Yes 0 0 40 
Yes No 1 30 20 
Yes No 2 15 15 
Yes Yes 0 10 5 
Yes Yes 1 25 5 
 
Attribute Type Use 
First Hand Nominal Input 
Sunroof Nominal Input 
Air bags Numeric Input 
Class Nominal Target 
 
 (:59%של  מובהקות-ברמתבריבוע  חי ערכי טבלת (
DF 1 2 3 4 5 6 
Chi Square (95%) 3.841 5.991 7.815 9.488 11.070 12.592 
 
 :שימו לב
 Class מייצג עבור כל אחד מהסיווגים האפשריים של: yes, no  בבסיס הנתונים המופעיםאת מספר .
-רשומות שמסווגות ל 9-ו no-רשומות שמסווגות ל 10יש , עבור השורה הראשונה בטבלה, לדוגמא
yes. 
 :יש לענות על כל הסעיפים הבאים
"  Purchase Vehicle"עבור בסיס הנתונים המסוכם ( IFN)עמומה  -אינפו ברשת לבנות את הרמה הראשונהיש .א
התוצאות הסופיות  חישובי הביניים ואת עבור כל משתנה יש להציג את . 59%תוך שימוש ברמת בטחון של 
Page 4 of 6  
 
לא דוחים את השערת /האם דוחים)ומסקנה   MI, G2, χ2 ,(אם רלבנטי)ערכי פיצול , שם המאפיין :הבאות
 .(האפס
 נקודות 20. בטבלת התוצאות הסופיות סמן בעיגול את המשתנה הנבחר לפיצול של השכבה
 חישובי ביניים
First Hand 
 
כמות 
תצפיות 
No 
Cond. 
Prob. 
Joint 
Prob. 
כמות 
תצפיות 
Yes 
Cond. 
Prob. 
Joint 
Prob. Total 
Cond. 
Prob. 
         
         Total 
         
Sunroof 
 
כמות 
תצפיות 
No 
Cond. 
Prob. 
Joint 
Prob. 
כמות 
תצפיות 
Yes 
Cond. 
Prob. 
Joint 
Prob. Total 
Cond. 
Prob. 
         
         Total 
         
Air bags 
 
כמות 
תצפיות 
No 
Cond. 
Prob. 
Joint 
Prob. 
כמות 
תצפיות 
Yes 
Cond. 
Prob. 
Joint 
Prob. Total 
Cond. 
Prob. 
         
         Total 
         
Air bags 
 
כמות 
תצפיות 
No 
Cond. 
Prob. 
Joint 
Prob. 
כמות 
תצפיות 
Yes 
Cond. 
Prob. 
Joint 
Prob. Total 
Cond. 
Prob. 
         
         Total 
         
Air bags 
 
כמות 
תצפיות 
No 
Cond. 
Prob. 
Joint 
Prob. 
כמות 
תצפיות 
Yes 
Cond. 
Prob. 
Joint 
Prob. Total 
Cond. 
Prob. 
         
         Total 
         
Page 5 of 6  
 
 התוצאות הסופיות
 מאפיין
ערכי 
 פיצול
MI G2 χ2 לא לדחות / לדחות ) מסקנהH0) 
 
 
    
 
 
    
 
 
    
 
 
    
 
 
     
 נקודות 10. והציגו בצורה גרפית את הרשת' חשבו את המשקולות של הצלעות עבור המודל מסעיף א .ב
 
No. Rule Weight 
   
   
   
   
      
   
 
 :גרף
 
 
 
 
 
 
 
 
 
 
 
 
 נקודות 5. 'את שגיאת האימון של המודל מסעיף ב וחשב .ג
 
 )%( שגיאת אימון
 
 
 
 
Page 6 of 6  
 
אם הערך של , כלומר, מייצגים טרנזקציות"  Purchase Vehicle"נניח כי המאפיינים של בסיס הנתונים  .ד
להתחשב רק בטרנזקציות שמכילות   יש .סימן שהמאפיין מופיע בטרנזקציה, 0-מ או גדול" Yes"הוא המאפיין 
 !פריט אחד לפחות
אשר מקיימים את  של שני פריטים ומעלהולהחזיר את כל החוקים  Apriori-יש להשתמש באלגוריתם ה 
 .min support = 20%, min confidence = 40%:                האילוצים
 נקודות 15
 
Rule Support Confidence 
   
   
   
   
   
    
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2012-מועד-ב'\Final_12_B01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 5  
 
 נקודות[ 50] 2חלק 
  אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
( כתלות בצריכת חשמל בחמשת הימים הקודמים. S0נתונה טבלה של צריכת חשמל ביום מסוים )
" 1-הסיווג "  וכו'. ”S-2“-, הצריכה לפני יומיים מסומנת ב”S-1“-ב תהצריכה ביום הקודם מסומנ
 " מייצג צריכה גבוהה.1מייצג צריכה נמוכה בשעה שהסיווג "
i S-1 S-2 S-3 S-4 S-5 S0 
1 6.26 8.67 9.03 8.80 8.64 -1 
2 5.40 6.26 8.67 9.03 8.80 1 
3 8.78 5.40 6.26 8.67 9.03 1 
4 8.68 8.78 5.40 6.26 8.67 1 
5 8.70 8.68 8.78 5.40 6.26 1 
6 8.77 8.70 8.68 8.78 5.40 1 
7 8.52 8.77 8.70 8.68 8.78 -1 
8 8.28 8.52 8.77 8.70 8.68 1 
9 8.91 8.28 8.52 8.77 8.70 1 
10 8.55 8.91 8.28 8.52 8.77 -1 
11 5.92 8.55 8.91 8.28 8.52 -1 
12 5.42 5.92 8.55 8.91 8.28 -1 
13 6.39 5.42 5.92 8.55 8.91 -1 
14 6.56 6.39 5.42 5.92 8.55 -1 
15 6.60 6.56 6.39 5.42 5.92 -1 
16 6.12 6.60 6.56 6.39 5.42 -1 
17 5.65 6.12 6.60 6.56 6.39 1 
18 9.05 5.65 6.12 6.60 6.56 1 
19 9.11 9.05 5.65 6.12 6.60 1 
20 8.81 9.11 9.05 5.65 6.12 1 
 
 לענות על כל הסעיפים הבאים:יש 
 מסוימתת פיצול י נקודכל אחד מחמשת משתני הקלט חולק לשני אינטרוואלים עפ" .א
(Threshold) כל נקודת הפיצול של ל המתייחסת. יש לציין בטבלה הבאה את שיטת החלוקה
 נקודות 5משתנה )רוחב שווה / עומק שווה(. 
  S-1 S-2 S-3 S-4 S-5 
Threshold 7.25 8.28 8.28 7.21 8.52 
Discretization method           
Page 4 of 5  
 
 
של משתנה  twoing-מדד הו ( Gini (plitSGini-מדד ה, האנטרופיה המותניתלחשב את  יש .ב
. שצוינו בסעיף הקודםעבור נקודות הפיצול הראשונים משתני הקלט  לשניביחס  S0המטרה 
יש לציין את הערך האופטימאלי של כל מדד ולבחור במשתנה הטוב ביותר לפיצול כמו כן, 
 נקודות 20ביחס לכל מדד. 
 אין למלא משבצות בעלות רקע אפור!
 
  S-1 S-2 
Optimal 
value 
The 
best 
Threshold 7.25 8.28     
Prob (<= Threshold)         
Prob (-1 / <= Threshold)         
Entropy (<= Threshold)         
Gini (<= Threshold)         
Prob (-1 / > Threshold)         
Entropy (> Threshold)         
Gini (> Threshold)         
Conditional entropy         
Gini Split         
Twoing          
 
המופיעים  בעזרת מקדמי  Linear SVMמודל את המשקולות של משתני הקלט ב יש לחשב .ג
 נקודות 00.  בסעיף הבאהנתונה בטבלה 
  S-1 S-2 S-3 S-4 S-5 
Wj 
     
 (:נקודות 00)הבאה  יש למלא את העמודות הריקות הבאות בטבלה .ד
 וקטור תמיכה )כן / לא( (1
 wTxi + b הפונקציה (2
 b = -3.950יש להניח שהמקדם .  הסיווג החזוי )כן / לא( (3
Page 5 of 5  
 
 )כן / לא(שגיאה  (4
 
i 
Actual 
S0 alpha 
Support 
vector 
(Yes/No) w
Txi + b 
Predicted 
S0 Error 
1 -1 99.27 
  
    
2 1 269.21 
  
    
3 1 1.04 
  
    
4 1 189.00 
  
    
5 1 16.87 
  
    
6 1 36.58 
  
    
7 -1 183.96 
  
    
8 1 127.11 
  
    
9 1 67.37 
  
    
10 -1 166.71 
  
    
11 -1 0.00 
  
    
12 -1 76.92 
  
    
13 -1 94.99 
  
    
14 -1 97.51 
  
    
15 -1 142.70 
  
    
16 -1 96.47 
  
    
17 1 202.54 
  
    
18 1 3.94 
  
    
19 1 32.55 
  
    
20 1 12.32 
  
    
 
-ברמתשלו  סמך-בררווח ואת ה Linear SVMשל המודל  באחוזים דיוק האימוןיש לחשב את  .ה
 .נקודות 5 .55%ביטחון של 
Accuracy (%) Lower Bound (%)  Upper Bound  (%) 
   
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2014\Final_14_B01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Page 3 of 6  
 
 נקודות[ 50] 2חלק 
 אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתת עם יש להציג את כל התוצאו 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
 : להלן נתונים של צריכת חשמל שעתית במשך יממה אחת
Hour Tcons-1 Tcons-2 Tcons-1-2 Tcons-24 Class 
00:00 46 61 -15 36 Low 
01:00 40 46 -6 31 Low 
02:00 33 40 -7 62 Low 
03:00 34 33 1 23 Low 
04:00 29 34 -5 33 Low 
05:00 38 29 9 38 Low 
06:00 39 38 1 51 High 
07:00 115 39 76 59 High 
08:00 59 115 -56 52 High 
09:00 94 59 35 58 High 
10:00 58 94 -36 46 Low 
11:00 38 58 -20 49 High 
12:00 101 38 63 48 Low 
13:00 48 101 -53 50 High 
14:00 105 48 57 52 Low 
15:00 52 105 -53 45 High 
16:00 58 52 6 64 High 
17:00 68 58 10 80 High 
18:00 78 68 10 155 High 
19:00 73 78 -5 65 High 
20:00 67 73 -6 68 High 
21:00 76 67 9 65 High 
22:00 73 76 -3 61 High 
23:00 59 73 -14 46 Low 
 
  העמודהTcons-1 מייצגת את הצריכה השעתית בשעה הקודמת 
  העמודהTcons-2 הקודמת שלפני השעה מייצגת את הצריכה השעתית בשעה 
  העמודהTcons-1-2  בשעתיים הקודמותת והשעתי ותהצריכההפרש בין מייצגת את 
  העמודהTcons-24 מייצגת את הצריכה השעתית באותה שעה ביום הקודם 
  העמודהClass )מייצגת את הצריכה השעתית החזויה )נמוכה / גבוהה 
 כל השאלות הבאות מתייחסות לטבלה הנ"ל 
Page 4 of 6  
 
 עומק שווהבעלי  לשני אינטרוולים Tcons-2-1יש לבצע דיסקרטיזציה של המשתנה  .א
(equal depth  .)5 'נק 
Min Max 
Interval 
Code  Hour Tcons-1-2 Code  Hour Tcons-1-2 Code 
    0  00:00 -15    12:00 63   
    1  01:00 -6    13:00 -53   
    02:00 -7    14:00 57   
    03:00 1    15:00 -53   
    04:00 -5    16:00 6   
    05:00 9    17:00 10   
    06:00 1    18:00 10   
    07:00 76    19:00 -5   
    08:00 -56    20:00 -6   
    09:00 35    21:00 9   
    10:00 -36    22:00 -3   
    11:00 -20    23:00 -14   
 
 שווה רוחבבעלי  אינטרוולים לשלושה Tcons-24לבצע דיסקרטיזציה של המשתנה ש י .ב
(equal width  .)5 'נק 
 
Min Max 
Interval 
Code  Hour Tcons-24 Code  Hour Tcons-24 Code 
    0  00:00 36    12:00 48   
    1  01:00 31    13:00 50   
    2  02:00 62    14:00 52   
    03:00 23    15:00 45   
    04:00 33    16:00 64   
    05:00 38    17:00 80   
    06:00 51    18:00 155   
    07:00 59    19:00 65   
    08:00 52    20:00 68   
    09:00 58    21:00 65   
    10:00 46    22:00 61   
    11:00 49    23:00 46   
 
Page 5 of 6  
 
של משתנה המטרה  Twoing-( ומדד הGini (Gini Split-יש לחשב את האנטרופיה המותנית, מדד ה .ג
Class  1ביחס למשתני הקלט-Tcons 2-ו-Tcons 1וקיבלו שמות  שעברו דיסקרטיזציה-Code 2-ו-Code 
-)אנטרופיה מותנית ו. כמו כן, יש לציין את הערך האופטימאלי של שני המדדים הראשונים בהתאמה
Gini .נקודות 02( ולבחור במשתנה הטוב ביותר לפיצול ביחס לכל מדד 
 נא להציג את החישובים בטבלה הבאה )אין למלא משבצות בצבע אפור(:
Hour Class Code-1 Code-2    Code-1 Code-2 
00:00 Low 0 1  Count (0)     
01:00 Low 0 0  Count (1)     
02:00 Low 0 0  Count (2)     
03:00 Low 0 0  Prob (0)     
04:00 Low 0 0  Prob (1)     
05:00 Low 0 0  Prob (2)     
06:00 High 0 0  Count (Low; 0)     
07:00 High 1 0  Prob (Low/0)     
08:00 High 0 2  Count (High; 0)     
09:00 High 1 1  Prob (High/0)     
10:00 Low 0 2  Ent (0)     
11:00 High 0 1  Gini (0)     
12:00 Low 1 0  Count (Low; 1)     
13:00 High 0 2  Prob (Low/1)     
14:00 Low 1 0  Count (High; 1)     
15:00 High 0 2  Prob (High/1)     
16:00 High 0 1  Ent (1)     
17:00 High 1 1  Gini (1)     
18:00 High 1 1  Count (Low; 2)     
19:00 High 1 2  Prob (Low/2)     
20:00 High 1 2  Count (High; 2)     
21:00 High 1 1  Prob (High/2)     
22:00 High 1 2  Ent (2)     
23:00 Low 0 2  Gini (2)     
     Cond. Entropy     
     Gini Split     
     Abs (0)     
     Abs (1)     
     Twoing     
 
Best Conditional Entropy:  _____________   Selected attribute:  ___________________ 
Best Gini Split:   _____________ Selected attribute:  ___________________ 
Page 6 of 6  
 
( יש לחשב את מרכז cluster( מייצג השתייכות של כל תצפית לאשכול )Classבהנחה שמשתנה המטרה ) .ד
( של כל אחד משני centroidהכובד )
 נקודות. 5האשכולות. 
 
למרכזים שחושבו  שאינם קשורים) Low-ו  Highבהינתן שני מרכזים התחלתיים של שני האשכולות  .ה
 .אוקלידיהבמרחק יש להשתמש  .MEANS-Kאלגוריתם ה של איטרציה אחתבסעיף הקודם(, יש לבצע 
 .נקודות 55
  Tcons-1 Tcons-2 Tcons-24 
Centroid Low 50 45 35 
Centroid High 70 75 65 
 
Hour Tcons-1 Tcons-2 Tcons-24 Class 
Distance 
to Low 
Distance 
to High 
New 
class 
Change (Yes 
/ No) 
00:00 46 61 36 Low     
01:00 40 46 31 Low     
02:00 33 40 62 Low     
03:00 34 33 23 Low     
04:00 29 34 33 Low     
05:00 38 29 38 Low     
06:00 39 38 51 High     
07:00 115 39 59 High     
08:00 59 115 52 High     
09:00 94 59 58 High     
10:00 58 94 46 Low     
11:00 38 58 49 High     
12:00 101 38 48 Low     
13:00 48 101 50 High     
14:00 105 48 52 Low     
15:00 52 105 45 High     
16:00 58 52 64 High     
17:00 68 58 80 High     
18:00 78 68 155 High     
19:00 73 78 65 High     
20:00 67 73 68 High     
21:00 76 67 65 High     
22:00 73 76 61 High     
23:00 59 73 46 Low     
 
  Tcons-1 Tcons-2 Tcons-24 
Centroid Low    
Centroid High    
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2015\Final_15_B.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
 
2 
 
 
 נקודות[ 75] 2חלק 
  אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות  
  ללא בדיקהטיוטות החישוב ייגרסו 
 UCI Machine Learningהנמצא במאגר ICUכל השאלות בחלק הזה מבוססות על בסיס הנתונים 
Repository חודשים שאושפזה ביחידה לטיפול נמרץ  5.8.  הנתונים מתיחסים לתינוקת בת
(Intensive Care Unitופגע במערכת הנשימה א הגיב לטיפול אנטיביוטי( בעקבות זיהום חמור של  .
שעות בהן החולה הייתה  12-בסיס הנתונים כולל מספר פרמטרים שנמדדו בתדירות גבוהה במשך כ
ידנו לתשעה אינטרוולים כאשר בכל -מחוברת למכונת הנשמה מלאכותית.  תקופת הניטור חולקה על
 אינטרוול חושבו המשתנים הבאים:
 Average7 – משתנה קלט( צע דופק במהלך האינטרוולממו( 
 Slope7 –  קלט משתנה) האינטרוול במהלךמגמת שינוי של דופק) 
 Average19 – קלט משתנה) האינטרוול במהלך ממוצע לחץ דם) 
 Slope19 – קלט משתנה) האינטרוול במהלך מגמת שינוי של לחץ דם) 
 Balance – ידנו כמשתנה המטרה(-)הוגדר על מאזן נוזלים בסיום האינטרוול 
 להלן ערכי המשתנים שחושבו בכל אינטרוול:
Interval Average7 Slope7 Average19 Slope19 Balance 
1 176.8 -0.023 68.8 -0.012 74.0 
2 168.0 -0.113 63.7 -0.119 96.0 
3 167.9 0.000 65.6 0.046 123.9 
4 163.5 -0.046 72.8 0.149 140.9 
5 163.5 0.012 76.7 -0.117 138.4 
6 163.6 -0.034 68.7 -0.098 139.6 
7 163.6 -0.080 62.9 -0.111 137.1 
8 161.7 -0.002 70.3 0.295 136.8 
9 168.8 0.307 78.6 -0.129 86.0 
 
עפ"י  לשלושה אינטרווליםלאחר שחולק  Average7לחשב את האנטרופיה של המשתנה יש   .א
 (נקודות 51)שתי שיטות דיסקרטיזציה: רוחב שווה ועומק שווה. 
Interval Average7 
Interval ID  
Equal width 
Interval ID  
Equal depth 
1 176.8   
2 168.0   
3 167.9   
4 163.5   
5 163.5   
6 163.6   
7 163.6   
8 161.7   
9 168.8   
 
3 
 
 
Interval Prob. 
Equal width 
Prob. 
Equal depth 
1   
2   
3   
Entropy   
 
 Distance-weighted k-NN( בשיטת Balanceיש לחשב את הערך החזוי של משתנה המטרה ) .ב
. יש (נקודות 02)כנתוני האימון  6 – 1תוך שימוש בתצפיות מס'  9 – 7עבור תצפיות מס' 
 .k = 2 -להשתמש בנתונים המנורמלים המופיעים בטבלה הבאה וב
Record Average7 Slope7 Average19 Slope19 Balance 
1 1.000 0.214 0.376 0.276 74.0 
2 0.417 0.000 0.051 0.024 96.0 
3 0.411 0.269 0.172 0.413 123.9 
4 0.119 0.160 0.631 0.656 140.9 
5 0.119 0.298 0.879 0.028 138.4 
6 0.126 0.188 0.369 0.073 139.6 
7 0.126 0.079 0.000 0.042 137.1 
8 0.000 0.264 0.471 1.000 136.8 
9 0.470 1.000 1.000 0.000 86.0 
 
 
Euclidean 
Distance   Weight   
  Rec 7 Rec 8 Rec 9 Rec 7 Rec 8 Rec 9 
Dist_1       
Dist_2       
Dist_3       
Dist_4       
Dist_5       
Dist_6       
Predicted 
Balance          
Actual 
Balance          
Absolute 
Error          
 
 
 
4 
 
ע"מ לסווג את משתנה  SVM with Polynomial Kernel, Degree = 2יש להשתמש במודל   .ג
 .  להלן פירוט המודל:(נקודות 51) 2 -ו 1המטרה )שעבר דיסקרטיזציה( בתצפיות מס' 
Record i Balance Alpha 
K(x_i, 
x_1) 
K(x_i, 
x_2) 
Alpha*y_i*K 
X_1 
Alpha*y_i*K 
X_2 
1 -1 0.902     
2 -1 0.902     
3 1 0.902     
4 1 0.000     
5 1 0.902     
6 1 0.615     
7 1 0.000     
8 1 0.000     
9 -1 0.615     
       
       
Predicted 
Balance   
    
 
b = 1.134 
 ___________________  מה מספר וקטורי התמיכה במודל?
 Slope7בעל שתי שכבות.  השכבה הראשונה תפוצל עפ"י המשתנה  IFNיש לבנות מודל  .ד
ללא קשר .  יש לפצל כל אחד מהמשתנים Slope19עפ"י המשתנה  –והשכבה השניה 
 .  נקודות הפיצול של כל משתנה מפורטות להלן.מובהקות סטטיסטיתלתוצאות הבדיקות של 
 . נקודות 51
Record Slope7 Slope19 Balance 
1 -0.023 -0.012 -1 
2 -0.113 -0.119 -1 
3 0.000 0.046 1 
4 -0.046 0.149 1 
5 0.012 -0.117 1 
6 -0.034 -0.098 1 
7 -0.080 -0.111 1 
8 -0.002 0.295 1 
9 0.307 -0.129 -1 
 
 
 
8 
 
Slope7 
j'/ j -1 Cond. Joint 1 Cond. Joint Total Cond.   
< -0.002           
>=-0.002           
Total            
                    
Conditional 
Mutual 
Information                   
j' / j -1     1     Total    
< -0.002            
>=-0.002           
Total                
Total Layer 0          
          
Slope19          
j'/ j -1 Cond. Joint 1 Cond. Joint Total Cond.   
< 0           
>= 0           
Total           
                    
Conditional 
Mutual 
Information                   
j' / j -1     1     Total    
< 0           
>= 0          
Total          
                    
j'/ j -1 Cond. Joint 1 Cond. Joint Total Cond.   
< 0           
>= 0           
Total           
                    
Conditional 
Mutual 
Information                   
j' / j -1     1     Total    
< 0           
>= 0          
Total          
Total Layer 1                  
 
 
 
6 
 
שנתקבל כולל ההסתברות של כל אחד מהסיווגים  IFNיש להציג את התרשים של המודל  .ה
 . נקודות 52בכל אחד מהקודקודים הטרמינליים. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\מבחן-2016\Final_16S_A_V01.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
 
 
Page 1 of 8 
 
 
 
 
 6/20107/17תאריך הבחינה: 
 פרופ' מרק לסט שם המרצה: 
 כריית נתונים ומחסני נתונים שם הקורס: 
 1053-1-372 מספר הקורס:
  א'מועד:    ב' סמסטר:   (6201) ותשע"שנה:  
 שעות 3 משך הבחינה:
 + מחשבון דף נוסחאות )מצורף לבחינה( חומר עזר: 
 
 
Page 2 of 8 
 
 
 נקודות[ 50] 1חלק 
  כל השאלותיש לענות על 
  נקודות 5 –משקל של כל שאלה 
  בתשובה הנכונה ביותריש לבחור 
  גבי שאלון הבחינה בלבד-עלבמקום המיועד לכך יד ברור -בכתב לנמק בקצרה את התשובהיש . 
ניתן להשתמש בטיוטה לצורך עריכת התשובה. תשובה לא מנומקת )גם אם הבחירה נכונה( 
 תקבל ציון של אפס
 
 ? ) / regressionprediction (רגרסיהחיזוי / משימה של   מהווה אינה איזו משימה .א
 שער החליפין בין דולר לשקלחיזוי  (1
  חיזוי המועמד הזוכה בבחירות לנשיאות ארה"ב (2
 חולים-משך אשפוז של חולה בביתחיזוי  (3
 חיזוי ציון ממוצע של סטודנט בלימודים באוניברסיטה (4
הסבר: 
__________________________________________________________________________ 
 ? RapidMiner( של תכונה בתוכנת roleכתפקיד ) איננו מוגדרמה  .ב
1) Attribute 
2) Id 
3) Label 
4) Real 
הסבר: 
__________________________________________________________________________ 
 (binning) לאינטרוולים חלוקה .ג
 הנתונים שונות את מגדילה (1
 הנתונים שונות את מקטינה (2
 חריגים נתונים מזהה (3
 הנתונים את מנקה (4
הסבר: 
__________________________________________________________________________ 
 האינפורמציה באיזה טווח נמצאתעליכם לחזות את הצלחת המועמד/ת ללימודים אקדמיים.   .ד
 ציון פסיכומטרי לבין ציון מאוני ממוצע באוניברסיטה? בין המקסימלית ההדדית
1) [0, 1] 
2) [6, 7] 
3) [9, 10] 
4) [90, 100] 
הסבר: 
__________________________________________________________________________ 
 -ל שווה בינארי סיווג משתנה של Gini-ה מדדהערך המקסימלי של  .ה
1) 0.25 
2) 0.50 
3) 0.75 
4) 1.00 
הסבר: 
__________________________________________________________________________ 
 
 
Page 3 of 8 
 
 
 IFNברשת  Minimum significance level הפרמטר של בערכו היהעלי .ו
 ברשת השכבות מספר את להגדיל עשויה (1
 ברשת הקודקודים מספר את להגדיל עשויה (2
 שתי התשובות נכונות (3
 אף תשובה אינה נכונה (4
הסבר: 
__________________________________________________________________________ 
 ברשת בייסיאנית? ( hidden variable) חבוי משתנה על יודעים אנחנו מה .ז
 קיים שהמשתנה רק יודעים (1
 אחרים משתנים על משפיע הוא ואיך קיים שהמשתנה רק יודעים (2
 שלו הערך את (3
 אף תשובה אינה נכונה (4
הסבר: 
__________________________________________________________________________ 
 :אומר Apriori עקרון .ח
 שכיחה להיות חייבת שכיחה פריטים קבוצת של קבוצה-תת (1
 שכיחה להיות חייבת שכיחה פריטים קבוצת של על-קבוצת (2
 נכונות התשובות שתי  (3
 נכונה אינה תשובה אף (4
הסבר: 
__________________________________________________________________________ 
 היא נתונים מחסני של"( נדיפות-אי)" Non-volatility התכונה משמעות .ט
 במחסן לנתונים לגשת ניתן לא (1
 למחסן נתונים להוסיף ניתן לא (2
 במחסן נתונים לעדכן ניתן לא (3
 מהמחסן נתונים למחוק ניתן לא (4
הסבר: 
__________________________________________________________________________ 
 -ל תורם( aggregation / summary tables) סיכום בטבלאות השימוש .י
 השאילתות הרצת מהירות (1
 במחסן הנתונים איכות (2
 במחסן הנתונים היקף צמצום (3
 אף תשובה אינה נכונה (4
הסבר: 
__________________________________________________________________________ 
 
 
 
Page 4 of 8 
 
 
 נקודות[ 50] 2חלק 
  אלא אם צוין אחרת! שלוש ספרות אחרי נקודה עשרוניתיש להציג את כל התוצאות עם 
  גבי שאלון הבחינה בלבד-עליש לרשום את כל התשובות 
  ללא בדיקהטיוטות החישוב ייגרסו 
 
 :הארץ מאזורי באחד שנרשמו אדמה רעידות של אמיתיים נתונים להלן
No YEAR Max_Magnitude Avg_ Magnitude Num_Events Class 
1 1996 4.5 2.952 14 1 
2 1997 5.5 2.900 22 0 
3 1998 3.5 2.700 12 0 
4 1999 4.2 2.771 17 1 
5 2000 4.3 2.910 10 1 
6 2001 4.3 3.108 12 0 
7 2002 4.2 2.667 12 0 
8 2003 4.2 2.775 16 1 
9 2004 4.5 3.038 8 0 
10 2005 4.2 2.714 7 1 
11 2006 4.3 3.065 20 0 
12 2007 4 2.713 16 0 
13 2008 4.2 2.642 12 0 
14 2009 4.2 2.750 12 0 
15 2010 4.1 2.783 12 1 
   העמודהMax_Magnitude  של רעידות אדמה במהלך שנה  המקסימליתמייצגת את העוצמה
 מסוימת
  העמודהAvg_ Magnitude  של רעידות אדמה במהלך שנה  הממוצעתמייצגת את העוצמה
 מסוימת
  העמודהNum_Events  שנה מסוימתבמהלך את כמות הרעידות שנרשמו מייצגת 
  העמודהClass 0) העוקבתהשנה  במהלך מייצגת את העוצמה המקסימלית של רעידות אדמה – 
 משתנה המטרהומהווה את מעל לחציון( – 1מתחת לחציון, 
ושלוש תכונות קלט  0.10המובהקות של  -הורץ על טבלת הנתונים הנ"ל עם רמת IFNהאלגוריתם  .א
.    candidate input attributes )  :Max_Magnitude  ,Avg_ Magnitude   ,Num_Eventsמועמדות )
.  התכונה פוצלה ע"י  Avg_ Magnitudeעבור השכבה הראשונה של הרשת נבחרה התכונה 
.  נא  (,3.038] , (3.038 ,2.714], (2.714 ,2.642] האלגוריתם לשלושת האינטרוולים הבאים:
( בין השכבה הראשונה של הרשת mutual informationלחשב את האינפורמציה ההדדית )
 נקודות. 15 למשתנה המטרה.
 
 
 
Page 5 of 8 
 
 
j' / j 0 Cond. Joint 1 Cond. Joint Total Cond. 
[2.642, 2.714)         
[2.714, 3.038)         
> 3.038         
Total         
 
Mutual Information 
j' / j 0 1 Total 
[2.642, 2.714)    
[2.714, 3.038)    
> 3.038    
Total     
 
עבור האינפורמציה ההדדית שחושבה  Likelihood-Ratio Statisticיש לחשב את הסטטיסטי  .ב
 נקודות. 5בסעיף הקודם ואת מספר דרגות החופש שלה.  
 DF =   G2 =  
 
 0לטווח שבין  Max_Magnitude , Avg_ Magnitude ,  Num_Eventsיש לנרמל את התכונות  .ג
 נקודות. min-max  .10בשיטת  1-ל
  Max_Magnitude Avg_ Magnitude Num_Events 
Min    
Max    
 הערכים המנורמלים:
No YEAR Max_Magnitude Avg_ Magnitude Num_Events 
1 1996    
2 1997    
3 1998    
4 1999    
5 2000    
6 2001    
7 2002    
8 2003    
9 2004    
10 2005    
11 2006    
12 2007    
13 2008    
14 2009    
15 2010    
 
 
 
Page 6 of 8 
 
 
של  בערכים המנורמליםתוך שימוש  means-kיש להריץ את האיטרציה הראשונה של האלגוריתם  .ד
יש לחשב את מרכזי  שלוש התכונות שחישבתם לעיל.  אין להתייחס ליתר התכונות בטבלה!
 נקודות 20 האשכולות לפני ואחרי ביצוע האיטרציה.
 
No YEAR Old 
cluster 
Distance to 1 Distance to 2 New 
cluster 
1 1996 1    
2 1997 1    
3 1998 1    
4 1999 1    
5 2000 1    
6 2001 1    
7 2002 1    
8 2003 2    
9 2004 2    
10 2005 2    
11 2006 2    
12 2007 2    
13 2008 2    
14 2009 2    
15 2010 2    
 
 :האיטרציהלפני 
  Max_Magnitude Avg_ Magnitude Num_Events 
Centroid 1       
Centroid 2       
  
 אחרי האיטרציה:
  Max_Magnitude Avg_ Magnitude Num_Events 
Centroid 1       
Centroid 2       
 
 
Page 7 of 8 
 
 
 הבחינה הסופית –דף נוסחאות 
 'ב, סמסטר ע"ותש  -: כריית נתונים ומחסני נתונים  3105-1-372
Information Theory 
 Conditional Entropy H(Y/X) = -   
p(x,y)*log p (y/x) 
 Mutual Information I(X;Y) =  
 
yx yp
xyp
yxp
, )(
)/(
log),(  
Conditional Mutual Information 
I(X;Y/Z) = 
 
 Fano’s Inequality: H (Y/ X1… Xn)  
H (Pe) + Pe log2 (m-1) 
 The MDL Principle 
)}/()({minarg
21
hDLhLh CC
Hh
MDL 
  
Decision Trees 
 Expected information needed to 
classify a tuple in D (after using A): 
)(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

 
 Expected number of records in Ci, 
for class j: 

 


c
j
ijc
j
j
j
ij o
e
e
e
1
1
'  
 Chi-Square Statistic:  
 
 Apparent (pessimistic) error rate: 
 
 Entropy induced by threshold T: 
)(
||
||
)(
||
||
);,( 2
2
1
1 SEnt
S
S
SEnt
S
S
STAE 
 
 Split Information: 






 
 ||
||
log
||
||
)( 2
1 D
D
D
D
DSplitInfo
j
v
j
j
A
 
 Gini index: 



n
j
p jTgini
1
21)(
 
 Gini split (T): 
)()()( 2
2
1
1
Tgini
N
N
Tgini
N
NTginisplit 
 
 Twoing Splitting Rule: 
2
)/()/(
4







j
RL
RL tjptjp
pp
 
 Cost-complexity function (CART): 
TTRTR
~
)()(    
IFN 
 IFN Conditional mutual 
information at a node z:  
MI (Ai’ ; Ai / z) = 
j
M
j
M
ij i j
i j
ij
i j ij
ii
P V V z
P V z
P V z P V z'
' '
' '
' '
'
( ; ; ) log
( / )
( / ) ( / )



 
0
1
0
1
 
 IFN Likelihood-Ratio Statistic:  
)/; ()2(ln2)/; ( '
*
'
2 zAAMIEzAAG iiii 
 
 Conditional Mutual Information in 
a Layer i’: 
)/;();(
)(
''
'
zAAMIAAMI i
truezSplit
Layerz
iii
i



  
 IFN Connection Weight: 
)(
)/(
log);( = 
ij
ij
ij
ij
z
VP
zVP
zVPw   
 Conditional Mutual 
Information (Split) 
),/(),/(
),/;(
log);;(
1
0
2
1 zSCPzSSP
zSCSP
zCSP
ty
ty
ty
M
t y
i



 
 
Bayesian Learning 
 Naïve Bayes Classifier: 



n
k
CixkPCPC i
C
NB
i 1
)|(*)(maxarg  
 m-estimate: 
mn
mpnc


 



yx zypzxp
zyxp
zyxp
, )/()/(
)/,(
log),,(
))1)(1((~
'
)'( 2
1
2
1
0




cv
e
eo
H
v
i ij
ijij
c
j

))1)(()1)(((~| '
22
0
 zNTzNIG iiH 
 
 
Page 8 of 8 
 
 
 Laplacian-estimate: 
Kn
nc

1
 
 Joint probability in Bayesian 
network:  



n
i
iin XParentsxPxxP
1
1 ))(|(),...,(
 
k - Nearest Neighbors 
 Distance-weighted k-NN: 



k
i
ii
Vv
xfvwqf
1
))(,(maxarg)(ˆ   
2)(
1
iq
i
xxd
w

  
  
SVM 
 Linear SVM: 
      
1)( bxwy j
T
j  
 Nonlinear SVM: 
bxxKyxg ji
SVi
iij 

),()(   
 Polynomial kernel: 
 
Clustering 
 Distance measure for symmetric 
binary variables:  
dcba
cb jid

),(  
 Distance measure for asymmetric 
binary variables:  
cba
cb jid

),(  
 Distance measure for nominal 
variables: 
p
mpjid ),(  
 Distance measure for variables of 
mixed types: 
)(
1
)()(
1),(
f
ij
p
f
f
ij
f
ij
p
f
d
jid






  
 
 Rank for an ordinal variable: 
1
1



f
if
if M
r
z  
 Cluster centroid: N
tN
i ip
mC
)(
1


 
Data Preparation 
 min-max normalization:  
AAA
AA
A
minnewminnewmaxnew
minmax
minv
v _)__(' 


 
 z-score normalization: 
A
A
devstand
meanv
v
_
'

  
 normalization by decimal scaling: 
j
v
v
10
'  
 Simple Moving Average: 
k
YYY
Y ktttt
11
1
...ˆ 



 
 Weighted Moving Average: 
1...:
...ˆ
11
11111




kttt
ktktttttt
wwwwhere
YwYwYwY
 
 Exponential Moving Average: 
11 )1(   ttt FYF   
 
( , ) (1 )T pi j i jK  x x x x
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\בחנים-ומבחנים-משנים-קודמות\שאלות-מעבדה-לדוגמא\LabQuestions--Example.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות בכרייה- שאלות:
בRapid Miner, מהו אופרטור מקונן? 
אופרטור אשר מכיל בתוכו תת תהליך, המורכב מכמה אופרטורים
אופרטור אשר הוא חלק נפרד מהתהליך העיקרי 
אופרטור שמבצע cross validation 
אופרטור שמבצע feature selection

אלו שיטות נלמדו לבדיקת טיב המודל?
Cross validation
קובץ test set
חלוקת לפי אחוזים
שימוש בכל הtrain set
כל התשובות נכונות

מה ההבדל בין NaN לבין Na בשפת R?
NaN מתייחס ל – impossible values (Not a number) ואילו Na מתייחס לערכים חסרים או not available.
NaN מתייחס לערכים חסרים או not available ואילו Na מתייחס ל- impossible values (Not a number)
NaN ו-Na מתייחסים לערכים חסרים שמקורם בערך תוצאת חלוקה ב-0.
אף תשובה לא נכונה.

מה מהמשפטים הבאים נכון לגבי יכולתיה של הסיפרייה Pandas ב-Python?
open source project
handling of missing data
slicing, fancy indexing, and sub-setting of large data sets
Predictive Modeling
Plots
כל התשובות נכונות
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\A-Compact-and-Accurate-Model-for-Classification\k0203.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
A Compact and Accurate Model
for Classification
Mark Last, Member, IEEE Computer Society, and Oded Maimon
Abstract—We describe and evaluate an information-theoretic algorithm for data-driven induction of classification models based on a
minimal subset of available features. The relationship between input (predictive) features and the target (classification) attribute is
modeled by a tree-like structure termed an information network (IN). Unlike other decision-tree models, the information network uses
the same input attribute across the nodes of a given layer (level). The input attributes are selected incrementally by the algorithm to
maximize a global decrease in the conditional entropy of the target attribute. We are using the prepruning approach: When no attribute
causes a statistically significant decrease in the entropy, the network construction is stopped. The algorithm is shown empirically to
produce much more compact models than other methods of decision-tree learning while preserving nearly the same level of
classification accuracy.
Index Terms—Knowledge discovery in databases, data mining, classification, dimensionality reduction, feature selection, decision
trees, information theory, Information theoretic network.

1 INTRODUCTION
THE process of Knowledge Discovery in Databases (KDD) isdefined by Fayyad et al. [8] as “the nontrivial process of
identifying valid, novel, potentially useful, and ultimately
understandable patterns in data.” A pattern is an expression,
which describes the data at some level of abstraction. An
example of a very simple pattern used by many credit card
companies is most students are profitable customers. Data
mining is the core step of the KDD process, which is
concerned with a computationally efficient enumeration of
patterns presenting in a database. Classification is a primary
data mining task aimed at learning a function that classifies a
database record into one of several predefined classes (e.g.,
classes of profitable versus nonprofitable customers) based
on the values of the record attributes (age, occupation, etc.).
Common classification methods, like backpropagation,
Naı̈ve Bayes Classifier, and C4.5, are designed to optimize
the predictive performance of the induced model, e.g., its
ability to correctly classify new credit card applicants. Other
aspects of knowledge discovery, such as validity of
discovered patterns, simplicity of representation, and
identification of relevant features, are given only secondary
consideration by most existing algorithms. Consequently,
classification models induced from real-world data tend to
be overcomplex, statistically insignificant, and wasteful in
the number of used features. The information-theoretic
classification method presented in this paper is aimed at
solving these problems by using three guiding principles:
maximizing the mutual information between a set of
predictive attributes and the target (classification) attribute,
finding a minimal set of database attributes involved in the
induced model, and verifying the statistical significance of
the discovered patterns. The importance of these objectives
for the classification task is explained in the next sections.
1.1 Information Theory and Classification
The data classification process is aimed at reducing the
amount of uncertainty or gaining information about the
target (classification) attribute. In Shannon’s information
theory (see [4]), information is defined as that which
removes or reduces uncertainty. For a classification task,
more information means higher accuracy of a classification
model since the predicted class of new instances is more
likely to be identical to their actual class. A model that does
not increase the amount of information is useless and its
predictive accuracy is not expected to be better than just a
random guess. We also realize that more information is
needed to accurately predict a multivalued outcome (e.g.,
medical diagnosis) than to predict a binary outcome (e.g.,
customer credibility).
Information theory (see [4]) suggests a general model-
ing of conditional dependency between random vari-
ables. If nothing is known on the causes of a variable X,
its degree of uncertainty can be measured by the
unconditional entropy HðXÞ ¼ pðxÞ log2 pðxÞ (expected
value of log2½1=pðxÞ). The entropy reaches its maximum
value of log½domain size of X when X is uniformly
distributed in its domain, i.e., each value of X has the
same probability. Entropy is different from statistical
variance by its metric-free nature: It depends only on the
probability distribution of a random variable rather than
on its concrete values. Thus, in classification tasks, where
the metric of class labels is unimportant, minimizing the
entropy of the target attribute can be a criterion for
choosing the best hypothesis. Examples of this include
the use of information gain in ID3 [19] and C4.5 [21]
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004 203
. M. Last is with the Department of Information Systems Engineering, Ben-
Gurion University of the Negev, Beer-Sheva 84105, Israel.
E-mail: mlast@bgumail.bgu.ac.il.
. O. Maimon is with the Department of Industrial Engineering, Tel-Aviv
University, Tel-Aviv 69978, Israel. E-mail: maimon@eng.tau.ac.il.
Manuscript received 18 June 2001; revised 22 July 2002; accepted 17 Sept.
2002.
For information on obtaining reprints of this article, please send e-mail to:
tkde@computer.org, and reference IEEECS Log Number 114368.
1041-4347/04/$20.00  2004 IEEE Published by the IEEE Computer Society
algorithms for finding the best feature to split1 a node of
a decision tree.
According to the information theory, adding information
on attributes related to a random variable can decrease its
entropy. Moreover, it is shown mathematically in [4] that
additional information never increases the entropy. The
entropy of a random variable Y , given another random
variable X, (the conditional entropy) is given by HðY =XÞ ¼
pðx; yÞ log pðy=xÞ (expected value of log2½1=pðy=xÞ). A
symmetrical association between two random variables X
and Y (mutual information) is defined as a decrease in the
entropy of Y as a result of knowing X and vice versa,
namely,
IðX;Y Þ ¼
X
x;y
pðx; yÞ  log pðx=yÞ
pðxÞ
¼ HðY Þ HðY =XÞ
¼ HðXÞ HðX=Y Þ ¼ IðY ;XÞ;
ð1Þ
where pðxÞ is the unconditional probability of x; pðx=yÞ is the
conditional probability of x given y, and pðx; yÞ is the joint
probability of x and y. The intuition behind the definition of
mutual information is as follows: If Y and X are indepen-
dent, 8x; y : pðxÞ ¼ pðx=yÞ, resulting in IðX;Y Þ ¼ 0. In all
other cases, the ratio between conditional and unconditional
probabilities of x will be either below or above 1.00,
generating negative and positive terms, respectively. Each
nonzero term is weighted by the joint probability of the
corresponding value-pair. The above relationship between
entropy and mutual information is proven formally in [4].
The decrease in entropy of Y as a result of knowing n
variables ðX1; . . . ; XnÞ can be calculated incrementally by
using the following chain rule [4]:
IðX1; . . . ; Xn;Y Þ ¼ IðXi;Y =Xi1; . . . ; X1Þ: ð2Þ
The information-theoretic methodology of classification,
initially introduced by us in [16], is aimed at finding a
minimal set of predictive features that maximize a decrease
in the entropy of the classification attribute. The method has
already been applied to real-world tasks of knowledge
discovery in time-series databases [12] and manufacturing
data [14]. In this paper, we present, for the first time, a
detailed numeric example of the network construction
procedure, a new way of extracting rules from the network
structure, and a comprehensive comparison of our method
to other decision-tree algorithms.
1.2 Dimensionality Reduction and Feature
Selection
Minimizing the number of relevant attributes or features in
a classification model is important for several reasons, from
increasing the learning speed of a classification algorithm to
dealing with the “curse of dimensionality” problem in
parameter estimation. John et al. [11] distinguish between
two models of selecting a “good” set of features under some
objective function. The feature filter model assumes selecting
the features before applying an induction algorithm (by
using some evaluation measures), while the wrapper model
uses the prediction accuracy of the induction algorithm
itself to evaluate the features. The filter model is in line with
the definition of the KDD process in [8]: Selection of
relevant features is considered a preprocessing step of
knowledge discovery. The wrapper approach, on the other
hand, is aimed at optimizing the generalization perfor-
mance of a given data mining algorithm. An overview of
existing filter and wrapper methods for feature selection
can be found in [15].
The wrapper approach is usually associated with a
considerable computational effort since it requires the
rerunning of an induction algorithm multiple times. The
filter methods, on the other hand, are computationally
cheaper, but, as indicated by [15], there is a danger that the
features selected by a filter method will not allow a
classification algorithm to fully exploit its potential. Unlike
the filter and the wrapper approaches, the information-
theoretic method presented in this paper implements
automated feature selection “on the fly” as an integral part
of the learning process. Thus, a minimal subset of features is
found in a single run of the induction algorithm.
1.3 Statistical Significance of Classification Models
As indicated by [18], some learning algorithms tend to
create very complex models that do not generalize well
beyond the set of training examples. The problem of
overfitting the training set arises whenever the constructed
model incorporates some random patterns which are
unlikely to occur in the entire population. An ideal
induction algorithm should be able to find every valid
pattern presenting in data while filtering out all the random
factors and patterns.
The dilemma of increasing the complexity of a model
(e.g., by splitting a decision tree node) versus the danger of
overfitting a given sample is well-known in statistical
hypothesis testing (see [17]). A null hypothesis H0 (which is
usually less complex than the alternative hypothesis H1)
can only be rejected at a given significance level which
specifies how rare the training cases must be, based on the
assumption that H0 is true. In other words, the level of
significance represents the risk of a researcher in making a
decision to reject H0. One of the first decision-tree
algorithms, ID3 [19], has applied the chi-square statistic to
the null hypothesis about the irrelevance of a tested
attribute. However, most other methods of decision-tree
learning, like CART [3] and C4.5 [21], have adopted the
postpruning approach (grow a maximal tree and then
prune it) for the sake of exploring a larger set of potentially
valid patterns. The postpruning methods include cost-
complexity pruning [3] and pessimistic error pruning [21].
The MDL-based PUBLIC algorithm [24] attempts to save
part of the postpruning effort by implementing a “branch
and bound” strategy which does not expand nodes with
guaranteed high encoding cost.
The straightforward approach of the information-theo-
retic algorithm is to completely preprune the model by
ignoring statistically insignificant features and patterns.
Thus, the information-theoretic network is not grown
204 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
1. To “split” a node in decision-tree learning means to partition the set of
training samples associated with a node by creating a child node for each
value of the tested feature (see [18]).
beyond necessity, following the famous Occam’s razor
principle.2 An additional benefit of the prepruning ap-
proach is the ability to stop the model construction at any
time in the case of limited computation resources. As shown
later in this paper, statistical prepruning tends to produce
more compact models than existing postpruning techniques
without a substantial decrease in the predictive accuracy.
1.4 Paper Organization
Section 2 describes the structure of the information theoretic
connectionist network and presents a detailed numeric
example of the network construction procedure. We also
analyze the computational complexity of the proposed
method as a function of data dimensionality. In Section 3,
we compare the information-theoretic methodology to the
most common techniques of decision-tree construction and
evaluate the algorithm performance on a variety of standard
learning tasks. Section 4 concludes the paper with represent-
ing a number of issues for future research. More details of the
algorithm are given in the Appendix. A beta version of the
software is available at http://www.ise.bgu.ac.il/faculty/
mlast/ifn.htm.
2 METHOD DESCRIPTION
2.1 Information-Theoretic Network Structure
Gorin et al. [9], [10] have applied an information-theoretic
connectionist network, having a fixed number of internal
layers, to speech recognition tasks. Gorin et al. used two types
of networks: a single-layer network and a two-layer network.
A single-layer network has an input node for each input
variable and assumes that all variables are independent. The
two-layer network has a second (“hidden”) layer with a node
for each variable-pair. In this paper, we present an algorithm
for building a multilayer information-theoretic network (IN),
where the number of layers is determined automatically by
the network construction algorithm. A multilayer informa-
tion network has the following components:
1. jIj: total number of hidden layers (levels) in the
network. Each layer is uniquely associated with an
input (predicting) attribute by representing the
interaction of that attribute and the input attributes
of the previous layers. The first layer (layer 0)
includes only the root node and is not associated
with any input attribute. At each iteration of the
network construction procedure, the last hidden
layer is termed the final layer. The topology of the
network differs from the decision-tree structure
used by CART [3], ID3 [19], and C4.5 [21] in two
aspects: All nodes of a given layer are labeled by the
same input attribute and continuous input attributes
are discretized to the same intervals at all nodes of
the associated layer. In most decision-tree algo-
rithms, the choice of attributes and discretization
thresholds is done locally at each node. The
structure of the information-theoretic network is
motivated by our belief that many data sets can be
accurately represented by a compact model, based
on a “global” set of predictive features. This belief is
empirically validated in Section 3 of our paper.
2. Ll: a subset of nodes z in a hidden layer no. l. Each
node represents a conjunction of values of the first
l input attributes, which is similar to the definition of
an internal node in a standard decision tree. If a
hidden layer l is associated with a nominal input
attribute, each outgoing edge of a nonterminal node
corresponds to an attribute distinct value. For con-
tinuous features, the outgoing edges represent the
intervals obtained from the discretization process.
3. K: a subset of distinct target nodes Ct (the target
layer). Each target node is associated with a value
(class) t in the domain of the target attribute T . For
continuous target attributes, the target nodes repre-
sent disjoint intervals in the attribute range. A target
layer is missing in the standard decision-tree
structure.
4. ðz; tÞ: connection between a terminal (unsplit) node z
and a target node Ct. The information-theoretic
meaning of the connection weights is explained in
Section 2.5 below.
The connectionist nature of our system (each terminal
node is connected to every target node) resembles the
topological structure of multilayer neural networks (see
[18]), which also has input and output nodes and a variable
number of hidden layers. Consequently, we refer to our
system as a network and not as a tree. However, information
networks differ from neural networks in that the informa-
tion-theoretic weights are defined only for the connections to
the target layer, whereas internal connections are associated
with values or intervals of input attributes and do not have
any weights at all. A neural network has, in contrast, a
weight associated with every interlayer connection.
2.2 Illustrative Example
To demonstrate the construction procedure of an informa-
tion network presented in Section 2.3 below, we are using
the Credit Approval (“Australian”) data set from the UCI
Repository [2]. This is an encrypted form of a proprietary
database, containing data on 690 credit card applications
and their outcomes. The data were originally provided by a
large bank in Australia. Each case represents an application
for credit card facilities described by eight discrete and six
continuous attributes, with two decision classes (Accept/
Reject). In the UCI Repository, the original attribute names
have been changed to meaningless symbols (A1-A14) with
the purpose of protecting the confidentiality of the data.
However, the real names of the attributes are available at
the site of Rulequest Research (http://www.rulequest.
com/see5-examples.html). This data set has been used as
a benchmark with a wide range of learning algorithms
(starting from [20]).
The database attributes are shown in Table 1. The original
names of the attributes (provided by the Rulequest Research
site) are given in parentheses. The Domain column shows the
set or range of possible values for each attribute. In the Type
column, we make a distinction between discrete (nominal)
and continuously valued attributes. The last column (Use in
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 205
2. “Nunquam ponenda est pluralitas sin necesitate”: “Entities should not
be mutliplied beyond necessity” [18].
Network) specifies how each attribute is treated by the
information-theoretic algorithm.
2.3 Network Construction Procedure
2.3.1 Overview
The network construction algorithm starts with defining the
target layer (one node for each target value, or class) and the
“root” node representing an empty set of input attributes.
Unlike CART [3] and C4.5 [21], IN is built only in one
direction (top-down). After the construction process is
stopped, there is no bottom-up postpruning of the network
branches. The process of prepruning the network is
explained below.
A node is split if it provides a statistically significant
increase in the mutual information of the node and the target
attribute. Mutual information, or information gain, is
defined as a decrease in the conditional entropy of the
target attribute (see [4]). If the tested feature is nominal, the
splits correspond to the feature values. Splits on continuous
features represent thresholds, which maximize an increase
in mutual information. For each new layer, the algorithm
recomputes the best threshold splits of continuous attri-
butes and chooses an input attribute (either discrete or
continuous) which provides the maximum increase in
mutual information across all nodes of the final layer.
The nodes of a new hidden layer are defined for a
Cartesian product of split nodes of the final layer and the
values of the new input attribute. According to the chain
rule (see Section 1.1 above), the mutual information between
a set of input attributes and the target (defined as the
overall decrease in the conditional entropy) is equal to the
sum of drops in conditional entropy across all hidden
layers. If there is no candidate input attribute significantly
decreasing the conditional entropy of the target attribute,
the network construction stops.
The network construction algorithm is summarized in
Fig. 1.
2.3.2 Selecting Nominal Attributes
The Credit Approval data set (see Section 2.2 above) has
14 candidate input attributes (A1-A14). At the initial stage
of the algorithm, all 690 records of the training set belong to
the root node. For each nominal attribute, the algorithm
calculates the conditional mutual information of a candi-
date input attribute Ai and the target attribute T given a
node z by the following formula (based on [4]):
MIðAi;T=zÞ ¼
XMT1
t¼0
XMi1
j¼0
P ðCt;Vij; zÞ
 log
P ðV tij=zÞ
P ðVij=zÞ  P ðCt=zÞ
;
ð3Þ
where
. MT=Mi: number of distinct values of the target
attribute T/candidate input attribute i.
. P ðVij=z): an estimated conditional (a posteriori)
probability of a value j of the candidate input
attribute i given the node z (also called a relative
frequency estimator).
. P ðCt=zÞ: an estimated conditional (a posteriori)
probability of a value t of the target attribute T
given the node z.
. P ðV tij=zÞ: an estimated conditional (a posteriori)
probability of a value j of the candidate input
206 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
TABLE 1
Credit Approval Data Set: List of Attributes
attribute i and a value t of the target attribute T
given the node z.
. P ðCt;Vij; zÞ: an estimated joint probability of a value t
of the target attribute T , a value j of the candidate
input attribute i, and the node z.
The contingency table3 for a nominal attribute A8 (Other
Investments) and the target attribute (Class) at the root node
(z ¼ 0) is shown in Table 2. The data set has 306 records of
“bad” customers (Class = Reject), who do not have other
investments with the bank (Other Investments = No). Only
23 customers with the same characteristics turned out to be
“good” customers (Class = Accept). The customers with
other investments (Other Investments = Yes) represent an
opposite pattern: Most of them (284 out of 361) should be
accepted by the bank. The resulting conditional mutual
information for this attribute, based on the estimated values
of conditional and unconditional probabilities, is 0.426 bits.
The statistical significance of the estimated conditional
mutual information between a candidate input attribute Ai
and the target attribute T is evaluated by using the
likelihood-ratio statistic (based on [1]):
G2ðAi;T=zÞ ¼ 2  ðln2Þ  EðzÞ MIðAi;T=zÞ; ð4Þ
where EðzÞ is the number of records associated with the
node z.
The Likelihood-Ratio Test [23] is a general-purpose
method for testing the null hypothesis H0 that two discrete
random variables are statistically independent. For exam-
ple, if the customer credibility is independent of his/her
other investments in the bank, the proportion of credible
customers among those having other investments should
be equal to their proportion among those who do not. The
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 207
3. A contingency table represents a joint frequency distribution, or cross-
tabulation, of two discrete random variable [17].
Fig. 1. Network Construction Algorithm.
Likelihood-Ratio Test is directly related to the information
theory since the independence of two attributes implies
that their expected mutual information is zero. If H0 holds,
then the likelihood-ratio test statistic G2ðAi;T=zÞ is dis-
tributed as chi-square with ðNIiðzÞ  1Þ  ðNT ðzÞ  1Þ
degrees of freedom, where NIiðzÞ is the number of distinct
values of a candidate input attribute i at node z and NT ðzÞ
is the number of values (classes) of the target attribute T at
node z.
The default significance level (p-value), used by the
information-theoretic algorithm, is 0.1 percent. We have
found empirically that larger p-values tend to decrease the
generalization performance of the network for most real-
world data sets. Thus, under normal circumstances, the
user should keep the default level of p-value rather than
running the algorithm repeatedly for a set of p-values and
comparing the results. However, if a given data set is
known to contain mostly regular patterns rather than
random noise, the user can weaken the significance
requirement of the test, up to removing the significance
testing completely.
In the Credit data set, there are 690 records associated
with the root node. This implies that the likelihood-ratio
statistic of Other Investments is 2ln26900:426 ¼ 407. The
attribute Other Investments has two values (Yes/No) and the
target attribute (Class) has two values as well. Conse-
quently, the likelihood-ratio statistic calculated above has
ð2 1Þð2 1Þ ¼ 1 degree of freedom. According to the chi-
square distribution, the significance level of the obtained
value is much higher than 0.1 percent, which means that the
null hypotheses can be rejected and Other Investments is
considered as a candidate for the next input attribute.
Calculated values of conditional mutual information for
other candidate input attributes are shown in Table 3.
Discretization of continuous attributes is demonstrated in
the next section. As one can see, Other Investments happens
to be the best attribute at Layer 0 and it is selected as the
first input attribute in the network. The next layer (Layer 1)
is going to have two hidden nodes corresponding to
329 records of those customers who do not have other
investments (Node 1) and 361 records of those who have
them (Node 2).
2.3.3 Selecting Continuous Attributes
The conditional entropy of the target attribute can only be
calculated with respect to attributes taking a finite number of
values. The algorithm performs discretization of continuous
attributes “on-the-fly” by using an approach which is similar
to the information-theoretic heuristic of Fayyad and Irani [7]:
recursively finding a binary partition of an input attribute
that minimizes the conditional entropy of the target
attribute. However, the stopping criterion we are using is
different from [7]. Rather than searching for a minimum
description length (minimum number of bits for encoding the
training data), we make use of a standard statistical
likelihood-ratio test [23]. As indicated in Section 2.3.2 above,
the significance level of the test can be adjusted to the
noisiness of a given data set. The MDL-based stopping
criterion of [7] does not provide this flexibility. The search
for the best partition of a continuous attribute is dynamic: It is
performed each time a candidate input attribute is con-
sidered for selection. The dynamic discretization algorithm
is described in the Appendix.
One of continuous attributes considered at Layer 1 is A14
(Savings Account Balance). This attribute has 238 distinct
values ranging from 1 to 100,001. The dynamic discretization
algorithm tries to split the range of this attribute by each one
of its values across all nodes of the final hidden layer
(Nodes 1 and 2 in our case). This requires calculating
conditional mutual information from 2 238 ¼ 476 contin-
gency tables. The algorithm finds the threshold providing
the maximum significant conditional mutual information
and applies the same procedure recursively to each one of
resulting subintervals (again across all final nodes) as long as
208 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
TABLE 2
Contingency Table: Other Investments (Node 0)
TABLE 3
Conditional Mutual Information (Layer 0)
the increase in the conditional mutual information is
statistically significant. For the Balance attribute, the first
best threshold found by the algorithm is 401.
The contingency tables for the threshold of Balance = 401
at Nodes 1 and 2 are shown in Tables 4 and 5, respectively.
The total number of cases in each table is equal to the
number of records associated with the corresponding nodes
(329 and 361). The chi-square statistic G2 for Node 1 is 0.052
(using (13) in the Appendix). The confidence level of this
value (with one degree of freedom) is very low: about
18 percent only. Thus, we cannot reject the null hypothesis
and split the Balance attribute at this node. However, at
Node 2, we get G2 ¼ 56:7, which has a significance level
much higher than 0.1 percent. Consequently, the condi-
tional mutual information of 0.059 bits that we have at this
node can be considered statistically significant. Since the
final layer (Layer 1) has only two nodes and subsequent
partitioning of the Balance range has not caused a
statistically significant improvement in the mutual informa-
tion, the total conditional mutual information between
Balance and the target attribute given the final layer is also
equal to 0.059 bits. This number is higher than the
conditional mutual information associated with any other
attribute, which makes Balance the second selected attribute
in the network. The new layer (Layer 2) has two nodes
(nos. 3 and 4) associated with the two intervals of Balance
(below 401 and above 401). In the network, these nodes are
connected by edges to Node 2 in Layer 1. Node 1 becomes a
terminal node. The third and last input attribute (Bank
Account) has been selected by using a similar procedure.
2.3.4 Summary
The iterations of the network construction procedure
applied to the Credit Approval data set are summarized in
Table 6. The table shows the input attribute selected at each
step and the associated change in the conditional entropy of
the target attribute. The table also includes the increase in the
network size (number of split nodes) and the ratio between
the cumulative mutual information and the number of input
attributes. Only three attributes (out of 14 candidates) were
selected by the information-theoretic algorithm. As one can
see from the first row of Table 6, the first input attribute
(Other Investments) contributes more than 80 percent of the
overall mutual information, which is equal to 0.516 bits (see
the last row).
The resulting information-theoretic connectionist net-
work is shown in Fig. 2. Thick lines represent internal
connections (standing for values or intervals of input
attributes) and thin lines denote the connections between
the terminal nodes and the nodes of the target layer. Dotted
thin lines indicate the predicted target values, i.e., the
values having maximum probability at a given terminal
node. The classification performance of the networks
induced from this and other data sets is evaluated in
Section 3.
The subset of attributes selected by the information-
theoretic algorithm can be used as an input for any other
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 209
TABLE 4
Contingency Table: Balance (Node 1)
TABLE 5
Contingency Table: Balance (Node 2)
TABLE 6
Network Construction Procedure—Credit Approval
data mining method, i.e., IN can be implemented as a feature
filter method in the KDD process. The application of the
information-theoretic methodology to feature selection is
discussed by us in [13].
2.4 Time and Space Complexity
The computational complexity of the network construction
procedure depends on the types of candidate input
attributes presenting in the training data. In this section,
we compute the complexity bounds for “pure” data sets,
which include either discrete or continuous attributes only.
In the case of a “mixed” data set, the overall complexity can
be roughly estimated by the following expression:
Comp ¼ D
m
Compd þ
C
m
Compc; ð5Þ
where m is the total number of candidate input attributes
and D ðCÞ is the number of discrete (continuous) attributes,
respectively. Accordingly, Compd ðCompcÞ is the computa-
tional complexity in a purely discrete (continuous) data set.
The computational complexity bounds are calculated by
using the following notation:
. n: total number of records in a training data set.
. m: total number of candidate input attributes.
. p: portion of significant input attributes, selected by
the network construction procedure ðp  1Þ.
. L: maximum number of hidden nodes in a layer
(bounded by the number of distinct conjunctions of
input attribute values presenting in the training set).
. MC : maximum domain size of a candidate input
attribute. For continuous attributes, this is the
maximum number of thresholds considered by the
discretization procedure. In our algorithm, like in
the discretization algorithm of Fayyad and Irani [7],
the number of potential thresholds is equal to the
number of distinct attribute values, which is
bounded by the size of the training set ðnÞ.
. MT : domain size of the target attribute (number of
distinct classes).
The computational “bottleneck” of the algorithm is
estimating the conditional mutual information of a candi-
date input attribute and the target attribute, given every
hidden node. The calculation of the conditional mutual
information is performed at each hidden layer of the
information-theoretic network for all candidate input
attributes at that layer. All frequency estimators used in
the calculation of MIðAi;T=zÞ are recomputed at each
hidden node by a single pass over a subset of training
examples associated with that node (bounded by n). If a
candidate input attribute is discrete, the summation terms
of MIðAi;T=zÞ refer to a Cartesian product of values of a
candidate input attribute and the target attribute. The
number of hidden layers is equal to pm. This implies that,
for discrete attributes, the total number of calculations is
bounded by:
Compd ¼
Xpm
s¼0
L  ðnþMT MCÞ  ðm sÞ
 L  ðnþMT MCÞ m
2  p  ð2 pÞ
2
:
ð6Þ
The number of possible partitions of a continuous
attribute is bounded by MC . For every possible partition,
the summation terms MIðTh;T=S; zÞ are summed over all
nodes of the final layer for a Cartesian product of two
subintervals and the values of the target attribute. Conse-
quently, for continuous attributes, the total number of
calculations is bounded by:
Compc ¼
Xpm
s¼0
L Mc  ðnþ 2MT Þ  ðm sÞ
 L Mc  ðnþ 2MT Þ m
2  p  ð2 pÞ
2
:
ð7Þ
Roughly speaking, the additional computational effort
associated with dynamic discretization of m continuous
attributes is proportional to the product MCn m
2. Thus, the
time complexity of the network construction procedure is
linear in the number of records, linear in the number of
distinct attribute values, and quadratic in the number of
candidate input attributes. Moreover, it is reduced by the
factor of pð2 pÞ, which is based upon the portion p of
significant input attributes in a network.
Like most other algorithms for decision-tree construction
(e.g., see [21]), our method requires all the training
examples to reside in main memory (RAM). For each
network node, the program also has to keep in memory the
predicting discrete value (or a continuous threshold)
associated with every input attribute. The resulting space
complexity of the network construction procedure is
mðnþ pmLÞ. Scaling up the algorithm for very large data
sets is a subject of ongoing research.
2.5 Classification and Rule Extraction
Due to the disjunctive nature of the information-theoretic
multilayer network, each record in a database can be
associated with one and only one terminal node z. The
predicted value t of a target attribute T at a terminal node z
is found by the maximum a posteriori rule:
t ¼ argmax
t
fP ðCt=zÞg: ð8Þ
Terminal nodes represent conjunctions of values of input
attributes. A connection between a terminal node z and a
210 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
Fig. 2. Information-theoretic network: credit data set.
node of the target layer, associated with the value (class) Ct,
can be interpreted as a probabilistic rule of the form:
If terminal node = z, then the value of the target attribute T is t
with probability of P ðCt=zÞ.
In our previous work [16], we have extracted a probabil-
istic rule from every connection between a terminal node and
a node of the target layer. This has resulted in a large number
of rules having positive and negative information-theoretic
weights. In this paper, we are reducing the number of rules
extracted from an information network by associating a single
classification rule (If z, then t) with every terminal node. The
weight of that rule is calculated as the sum of information-
theoretic weights over all edges connecting the correspond-
ing terminal node to the nodes of the target layer:
wTz ¼
XMT1
t¼0
P ðCt; zÞ  log
P ðCt=zÞ
P ðCtÞ
¼ P ðzÞ 
XMT1
j¼0
P ðCt=zÞ  log
P ðCt=zÞ
P ðCtÞ
;
ð9Þ
where
. P ðCt; zÞ: an estimated joint probability of the target
value Ct and the node z.
. P ðCt=zÞ: an estimated conditional (a posteriori)
probability of the target value Ct given the node z.
. P ðCtÞ: an estimated unconditional (a priori) prob-
ability of the target value Ct.
. P ðzÞ: probability (relative frequency) of a node z.
The above expression is an extension of the average
information content of a rule, defined by Smyth and Good-
man [25] for binary-valued attributes, to a more general case
of multivalued input and target attributes. As indicated in
[25], a measure of this form (called J-measure) is useful for a
relative evaluation of rules induced from data since its
value is always nonnegative and it represents both the
simplicity (probability of node occurrence P ðzÞ) and good-
ness-of-fit (cross entropy) of a given rule.
Proposition 1. The sum of connection weights across all terminal
nodes is equal to the estimated mutual information between the
set of input attributes and the target attribute:
MIðT ; IÞ ¼
X
z2F
XMT1
t¼0
P ðCt; zÞ  log
P ðCt=zÞ
P ðCtÞ
; ð10Þ
where
. T : the target attribute,
. I: set of input attributes,
. z: hidden node in the information-theoretic network,
. F : subset of unsplit (terminal) nodes,
. P ðCt; zÞ: an estimated joint probability of the value Ct
and the node z,
. P ðCt=zÞ: an estimated conditional (a posteriori)
probability of the value Ct given the node z, and
. P ðCtÞ: an estimated unconditional (a priori) prob-
ability of the value Ct.
Proof. This proposition is directly derived from the
definition of mutual information between random vari-
ables X and Y [4]:
MIðX;Y Þ ¼
X
x2X
X
y2Y
pðx; yÞ  log pðy=xÞ
pðyÞ : ð11Þ
In the above expression, we have substituted Y with
the target attribute T and X with the set of input
attributes I. A node z 2 F represents a conjunction of
input attribute values. Since the information-theoretic
network represents a disjunction of these conjunctions,
each conjunction is associated with one and only one
node z 2 F . Consequently, the summation over the
terminal nodes covers all possible values of the input
attributes. This completes the proof. tu
The most informative rules can be found by sorting the
information-theoretic connection weights (wTz ) in decreasing
order. The four rules extracted from the network of the
Credit Approval Data Set (see Fig. 2) are presented in
Table 7. It seems like the rules 2-4 can be replaced with a
single classification rule: If Other investments is 1, then Class
is 1. This rule reduction will not affect the classification
accuracy of the network, but will ignore an important fact
that two input attributes Balance and Bank Account do affect
the credibility of a potential customer.
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 211
TABLE 7
Information-Theoretic Rules: Credit Data Set
3 EMPIRICAL EVALUATION
3.1 Overview
The performance of the information-theoretic algorithm
was evaluated on 10 publicly available data sets: Breast
Cancer, Chess Endgames, Credit Approval, Diabetes, Glass
Identification, Heart Disease, Iris Plants, Liver, Lung
Cancer, and Wine. All these data sets are posted at the
UCI Machine Learning Repository [2] and widely used by
the data mining community for evaluating learning algo-
rithms. The data sets selected by us here comprise a diverse
mixture of attribute types, ranging from purely continuous
to purely nominal attribute domains. A summary of
characteristics of these data sets appears in Table 8. The
algorithm performance (in terms of dimensionality reduc-
tion and predictive accuracy) is compared to two decision-
tree algorithms: ID3 (presented by Quinlan in [19]) and
C4.5, which is a state-of-the-art decision tree algorithm
introduced in [21] and improved in [22]. The classification
accuracy of C4.5 is based on the “fine-tuned” results
published in the literature, which represent the optimized
performance of this algorithm. Other results were obtained
with the default settings of all algorithms, including the
information-theoretic network.
3.2 Dimensionality Reduction
As indicated in Section 1.2 above, dimensionality reduc-
tion is an important objective of the knowledge discovery
process. Most real-world data sets contain some portion
of completely irrelevant attributes. Unlike the Naı̈ve
Bayes Classifier, which uses all attributes in a data set,
decision-tree algorithms tend to remove irrelevant attri-
butes from the final tree (see [21]). The network
construction algorithm, presented above, is also aimed
at minimizing the set of input attributes in an informa-
tion-theoretic network. Table 9 shows the initial number
of candidate input attributes in each data set, the number
of input attributes selected by the evaluated algorithms
(ID3, C4.5, and IN), and the reduction in data dimension-
ality (the portion of candidate input attributes that were
excluded from the model). The C4.5 trees were built
212 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
TABLE 8
Description of Data Sets
TABLE 9
Dimensionality Reduction—Summary Table
using Version 8 of the algorithm (described in [22]). For
ID3 and C4.5, we have counted all the attributes that
appear in at least one tree path. In the information-
theoretic network (IN), the number of input attributes is
equal to the number of internal layers. The training sets
included all records of each data set. Table 9 also
compares the complexity of the resulting models in terms
of the total number of nodes in a tree/network and the
run times of the algorithms on a Pentium III computer.
The results show that the models produced by the
information-theoretic algorithm are significantly smaller
than the decision trees built by ID3 and C4.5. Thus, C4.5
failed to remove more than 50 percent of the attributes in
eight data sets out of 10. On the other hand, the
information-theoretic network never included more than
50 percent of available attributes. The average difference
between the two methods is 32 percent of the number of
available attributes. This means that the information-
theoretic algorithm is a much more “aggressive” dimen-
sionality reducer than C4.5. ID3 tends to use fewer
attributes than C4.5, but its average number of selected
attributes (5.3) is still higher than the IN average (3.6).
Table 9 also shows that, in almost all cases, the
information-theoretic network produces a simpler model,
compared to ID3 and C4.5. IN has the minimal number of
nodes in nine data sets out of 10. This result is not
completely surprising since our algorithm uses fewer input
attributes and each node represents a conjunction of values
of input attributes. Due to the repetitive partitioning of the
training set (as opposed to the recursive approach of ID3
and C4.5), IN is slightly slower than other decision-tree
methods. As mentioned above, scaling-up the IN algorithm
and improving its computational efficiency is a subject of
ongoing research.
In the next section, we examine the trade off between the
dimensionality reduction and the predictive accuracy of the
information-theoretic network.
3.3 Predictive Accuracy
Here, we use a common approach to estimating predictive
accuracy, called k-fold cross-validation (see [18]). According
to this approach, the data set is randomly partitioned into k
disjoint subsets, with each subset being used once in a test
set and k 1 times in a training set. Following the common
practice of other researchers (e.g., see [15]), we have chosen
the value of k to be 10. Due to the high variance of cross-
validation runs, we have performed 10 runs of 10-fold
cross-validation, each based on a different random parti-
tioning of the data set.
Table 10 shows, for each data set, the estimated
predictive accuracy of the information-theoretic network
versus other decision-tree methods. The results of ID3 were
obtained with our own implementation of the algorithm
(based on [19]), while C4.5 results were taken from [5]. The
confidence intervals for the IN predictive accuracy have
been calculated at the 0.95 confidence level, using
t-distribution with n 1 degrees of freedom, where n is
the number of 10-fold cross-validation runs (10). An asterisk
(*) next to the upper bound of a confidence interval denotes
a statistically significant advantage of C4.5 over IN.
As one can see from Table 10, the predictive accuracy of
the information-theoretic algorithm tends to be only slightly
worse than the accuracy of C4.5. One exception is the Iris
data set, where the network has provided us with better
results than C4.5 along with reducing dimensionality by
75 percent. In other data sets, a small loss of accuracy (the
mean difference of less than 1 percent) is compensated by a
considerable reduction in the number of input attributes (on
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 213
TABLE 10
Predictive Accuracy—Comparison to Other Methods
average, the algorithm uses about 1/3 of the candidate
input attributes, which appear in each data set). ID3 does
not show any advantages at all since it has the lowest
average accuracy while using more input attributes than IN.
Though the choice of the best model (either the most
accurate or the simplest) depends on a specific application,
we believe that, in many cases, a small amount of accuracy
can be sacrificed for the sake of obtaining a much more
compact and interpretable model, like the one produced by
the information-theoretic algorithm.
4 CONCLUSION
In this paper, we have presented a novel algorithm for
building simple and reasonably accurate classification
models, termed information-theoretic networks. The under-
lying principles of our methodology include maximization
of mutual information, dimensionality reduction, and
statistical significance testing. The algorithm was evaluated
on a wide range of standard data sets containing contin-
uous, categorical, and binary-valued attributes. The related
issues to be studied further include: integrating the
information-theoretic network with other data mining
methods (e.g., by using the algorithm as a feature selector
only), utilizing prior knowledge in the network construc-
tion procedure, and applying the algorithm to nonrelational
(e.g., spatial) data.
APPENDIX
DYNAMIC DISCRETIZATION ALGORITHM:
PARTITION (DATA TABLE r, INFORMATION NETWORK,
ATTRIBUTE Ai, INTERVAL S, SIGNIFICANCE
LEVEL Sign)
Input: the set of n training instances, an information-
theoretic network, a continuous attribute Ai to be discre-
tized, the interval S to be partitioned (the first and the last
distinct values of Ai), and the minimum significance level
sign for splitting an interval (default: sign = 0.1 percent).
Output: the total number of discretization intervals for Ai
and the lower bound of each interval.
Step 1: Initialize to zero the degrees of freedom and the
estimated conditional mutual information of the candidate
input attribute and the target attribute given the final
hidden layer of nodes.
Step 2: Repeat for every distinct value included in the
interval S (except for the last value):
Step 2.1: Define the value as a partitioning threshold
(Th). All values below or equal to Th belong to the first
subinterval S1. Distinct values above Th belong to the
second subinterval S2.
Step 2.2: Repeat for every node z of the final hidden
layer:
Step 2.2.1: Calculate the estimated conditional mutual
information between the partition of the interval S at the
threshold Th and the target attribute T given the node z by
the following formula (based on [4]):
MIðTh;T=S; zÞ ¼
XMT1
t¼0
X2
y¼1
P ðSy;Ct; zÞ  log
P ðSy;Ct=S; zÞ
P ðSy=S; zÞ  P ðCt=S; z
;
ð12Þ
where
P ðSy=S; zÞ: an estimated conditional (a posteriori) prob-
ability of a subinterval Sy, given the interval S and the node z.
P ðCt=S; zÞ: an estimated conditional (a posteriori) prob-
ability of a value Ct of the target attribute T given the
interval S and the node z.
P ðSy;Ct=S; zÞ: an estimated joint probability of a value Ct
of the target attribute T and a subinterval Sy given the
interval S and the node z.
P ðSy;Ct; zÞ: an estimated joint probability of a value Ct of
the target attribute T , a subinterval Sy, and the node z.
Step 2.2.2: Calculate the likelihood-ratio test for the
partition of the interval S at the threshold Th and the target
attribute T given the node z by the following formula
(based on [23]):
G2ðTh;T=S; zÞ
¼ 2
XM11
t¼0
X2
y¼1
NijðSy; zÞ  ln
NtðSy; zÞ
P ðCt=S; zÞ  EðSy; zÞ
;
ð13Þ
where
NtðSy; zÞ: number of occurrences of a value Ct of the
target attribute T in subinterval Sy and the node z.
EðSy; zÞ: number of records in subinterval Sy and the
node z.
P ðCt=S; zÞ: an estimated conditional (a posteriori) prob-
ability of a value Ct of the target attribute T given the
interval S and the node z.
P ðCt=S; zÞ  EðSy; zÞ: an estimated number of occurrences
of a value Ct of the target attribute T in subinterval Sy and
the node z under the assumption that the conditional
probabilities of the target attribute values are identically
distributed given each subinterval.
Step 2.2.3: Calculate the degrees of freedom of the
likelihood-ratio statistic by:
DF ðTh;T=S; zÞ ¼ ðNI iðS; zÞ  1Þ  ðNT ðS; zÞ  1Þ
¼ ð2 1Þ  ðNT iðS; zÞ  1Þ
¼ NT iðS; zÞ  1;
ð14Þ
where
NIiðS; zÞ: number of subintervals of a candidate input
attribute i at node z (2).
NT ðS; zÞ: number of values of the target attribute in the
interval S at node z.
Step 2.2.4: If the likelihood-ratio statistic is significant,
mark the node as “split” by the threshold Th and update
the estimated conditional mutual information of the
candidate input attribute and the target attribute given the
threshold Th; else mark the node as “unsplit” by the
threshold Th.
Step 2.2.5: Go to next node.
Step 2.3: Go to next distinct value.
Step 3: Find the threshold Thmax maximizing the
estimated conditional mutual information between a parti-
tion of the candidate input attribute Ai and the target
214 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 16, NO. 2, FEBRUARY 2004
attribute T given the interval S and the set of input
attributes I by:
Thmax ¼ argmax
Th
MIðTh;T=I; SÞ ð15Þ
and update the estimated conditional mutual information
cond MIi between the candidate input attribute Ai and the
target attribute T .
Step 4: If the maximum estimated conditional mutual
information is greater than zero, then do:
Step 4.1: Repeat for every node z of the final hidden
layer: If the node z is split by the threshold Thmax, mark the
node as split by the candidate input attribute Ai.
Step 4.2: If the threshold Thmax is the first distinct value
in the interval S, mark Thmax as the lower bound of a new
discretization interval, else Partition (Data Table r, Net-
work, Attribute Ai, Interval S1).
Step 4.3: Partition (Data Table r, Network, Attribute Ai,
Interval S2)
Step 4.4: EndDo.
Else:
Step 5: Define a new discretization interval S and
increment the domain size of Ai (number of discretization
intervals).
REFERENCES
[1] F. Attneave, Applications of Information Theory to Psychology. Holt,
Rinehart, and Winston, 1959.
[2] C.L. Blake and C.J. Merz , UCI Repository of Machine Learning
Databases, http://www.ics.uci.edu/~mlearn/MLRepository.
html, 19 July 2002.
[3] L. Breiman, J.H. Friedman, R.A. Olshen, and P.J. Stone, Classifica-
tion and Regression Trees. Wadsworth, 1984.
[4] T.M. Cover and J.A. Thomas, Elements of Information Theory. Wiley,
1991.
[5] P. Domingos and M. Pazzani, “On the Optimality of the Simple
Bayesian Classifier under Zero-One Loss,” Machine Learning,
no. 29, pp. 103-130, 1997.
[6] P. Domingos, “Occam’s Two Razors: The Sharp and the Blunt,”
Proc. Fourth Int’l Conf. Knowledge Discovery and Data Mining,
pp. 37-43, 1998.
[7] U. Fayyad and K. Irani, “Multi-Interval Discretization of Con-
tinuous-Valued Attributes for Classification Learning,” Proc. 13th
Int’l Joint Conf. Artificial Intelligence, pp. 1022-1027, 1993.
[8] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth, “From Data
Mining to Knowledge Discovery: An Overview,” Advances in
Knowledge Discovery and Data Mining, U. Fayyad, G. Piatetsky-
Shapiro, P. Smyth, and R. Uthurusamy, eds., pp. 1-36, AAAI/MIT
Press, 1996.
[9] A.L. Gorin, S.E. Levinson, A.N. Gertner, and E. Goldman,
“Adaptive Acquisition of Language,” Computer Speech and
Language, vol. 5, no. 2, pp. 101-132, 1991.
[10] A.L. Gorin, S.E. Levinson, and A. Sankar, “An Experiment in
Spoken Language Acquisition,” IEEE Trans. Speech and Audio
Processing, vol. 2, no. 1, pp. 224-239, 1994.
[11] G.H. John, R. Kohavi, and K. Pfleger, “Irrelevant Features and the
Subset Selection Problem,” Proc. 11th Int’l Conf. Machine Learning,
pp. 121-129, 1994.
[12] M. Last, Y. Klein, and A. Kandel, “Knowledge Discovery in Time
Series Databases,” IEEE Trans. Systems, Man, and Cybernetics, Part
B, vol. 31, no. 1, pp. 160-169, Feb. 2001.
[13] M. Last, A. Kandel, and O. Maimon, “Information-Theoretic
Algorithm for Feature Selection,” Pattern Recognition Letters,
vol. 22, nos. 6-7, pp. 799-811, 2001.
[14] M. Last and A. Kandel, “Data Mining for Process and Quality
Control in the Semiconductor Industry,” Data Mining for Design
and Manufacturing: Methods and Applications, D. Braha, ed.,
pp. 207-234, Boston: Kluwer Academic, 2001.
[15] H. Liu and H. Motoda, Feature Selection for Knowledge Discovery and
Data Mining. Boston: Kluwer Academic, 1998.
[16] O. Maimon and M. Last, Knowledge Discovery and Data Mining, The
Info-Fuzzy Network (IFN) Methodology. Boston: Kluwer Academic,
2001.
[17] E.W. Minium, R.B. Clarke, and T. Coladarci, Elements of Statistical
Reasoning. New York: Wiley, 1999.
[18] T.M. Mitchell, Machine Learning. McGraw-Hill, 1997.
[19] J.R. Quinlan, “Induction of Decision Trees,” Machine Learning,
vol. 1, no. 1, pp. 81-106, 1986.
[20] J.R. Quinlan, “Simplifying Decision Trees,” Int’l J. Man-Machine
Studies, no. 27, pp. 221-234, 1987.
[21] J.R. Quinlan, C4.5: Programs for Machine Learning. Morgan
Kaufmann, 1993.
[22] J.R. Quinlan, “Improved Use of Continuous Attributes in C4.5,”
J. Artificial Intelligence Research, no. 4, pp. 77-90, 1996.
[23] C.R. Rao and H. Toutenburg, Linear Models: Least Squares and
Alternatives. Springer-Verlag, 1995.
[24] R. Rastogi and K. Shim, “PUBLIC: A Decision Tree Classifier that
Integrates Building and Pruning,” Proc. 24th Int’l Conf.Very Large
Databases (VLDB ’98), pp. 404-415, 1998.
[25] P. Smyth and R.M. Goodman, “An Information Theoretic
Approach to Rule Induction from Databases,” IEEE Trans.
Knowledge and Data Eng., vol. 4, no. 4, pp. 301-316, 1992.
Mark Last received the MSc (1990) and PhD
(2000) degrees in industrial engineering from Tel
Aviv University, Israel. He is currently a lecturer
in the Department of Information Systems
Engineering, Ben-Gurion University of the Ne-
gev, Israel. Prior to that, he was a visiting
assistant professor in the Department of Com-
puter Science and Engineering, University of
South Florida (1999-2001), a senior consultant
in industrial engineering and computing (1994-
1998), and the head of the Production Control Department at AVX Israel
(1989-1994). Dr. Last has published more than 60 papers and chapters
in journals, books, and conferences. He is a coauthor of the book
Knowledge Discovery and Data Mining—The Info-Fuzzy Network (IFN)
Methodology (Kluwer, 2000) with O. Maimon and a coeditor of the book
Data Mining and Computational Intelligence (Physica-Verlag, 2001) with
A. Kandel and H. Bunke. His current research interests include data
mining, pattern recognition, and software testing. He is a member of the
IEEE Computer Society.
Oded Maimon is a professor in the Industrial
Engineering Department, Tel Aviv University,
Israel, where he was the chairman until
recently. Prior to that, he was at MIT and
DEC. His research interests are in engineering
design and knowledge discovery from data,
with an emphasis on developing data mining
algorithms and analysis. He is a coauthor (with
M. Last) of the book Knowledge Discovery and
Data Mining—The Info-Fuzzy Network (IFN)
Methodology (Kluwer, 2000).
. For more information on this or any computing topic, please visit
our Digital Library at http://computer.org/publications/dlib.
LAST AND MAIMON: A COMPACT AND ACCURATE MODEL FOR CLASSIFICATION 215
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\Fast-Algorithms-for-Mining-Association-Rules\vldb94.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Fast Algorithms for Mining Association Rules
Rakesh Agrawal Ramakrishnan Srikant
IBM Almaden Research Center
650 Harry Road, San Jose, CA 95120
Abstract
We consider the problem of discovering association rules
between items in a large database of sales transactions.
We present two new algorithms for solving this problem
that are fundamentally dierent from the known algo-
rithms. Empirical evaluation shows that these algorithms
outperform the known algorithms by factors ranging from
three for small problems to more than an order of mag-
nitude for large problems. We also show how the best
features of the two proposed algorithms can be combined
into a hybrid algorithm, called AprioriHybrid. Scale-up
experiments show that AprioriHybrid scales linearly with
the number of transactions. AprioriHybrid also has ex-
cellent scale-up properties with respect to the transaction
size and the number of items in the database.
1 Introduction
Progress in bar-code technology has made it possi-
ble for retail organizations to collect and store mas-
sive amounts of sales data, referred to as the basket
data. A record in such data typically consists of the
transaction date and the items bought in the trans-
action. Successful organizations view such databases
as important pieces of the marketing infrastructure.
They are interested in instituting information-driven
marketing processes, managed by database technol-
ogy, that enable marketers to develop and implement
customized marketing programs and strategies [6].
The problem of mining association rules over basket
data was introduced in [4]. An example of such a
rule might be that 98% of customers that purchase
Visiting from the Department of Computer Science, Uni-
versity of Wisconsin, Madison.
Permission to copy without fee all or part of this material
is granted provided that the copies are not made or distributed
for direct commercial advantage, the VLDB copyright notice
and the title of the publication and its date appear, and notice
is given that copying is by permission of the Very Large Data
Base Endowment. To copy otherwise, or to republish, requires
a fee and/or special permission from the Endowment.
Proceedings of the 20th VLDB Conference
Santiago, Chile, 1994
tires and auto accessories also get automotive services
done. Finding all such rules is valuable for cross-
marketing and attached mailing applications. Other
applications include catalog design, add-on sales,
store layout, and customer segmentation based on
buying patterns. The databases involved in these
applications are very large. It is imperative, therefore,
to have fast algorithms for this task.
The following is a formal statement of the problem
[4]: Let I = fi1; i2; . . . ; img be a set of literals,
called items. Let D be a set of transactions, where
each transaction T is a set of items such that T 
I. Associated with each transaction is a unique
identier, called its TID. We say that a transaction
T contains X, a set of some items in I, if X  T .
An association rule is an implication of the form
X =) Y , where X  I, Y  I, and X \ Y = ;.
The rule X =) Y holds in the transaction set D with
condence c if c% of transactions in D that contain
X also contain Y . The rule X =) Y has support s
in the transaction set D if s% of transactions in D
contain X [Y . Our rules are somewhat more general
than in [4] in that we allow a consequent to have more
than one item.
Given a set of transactions D, the problem of min-
ing association rules is to generate all association rules
that have support and condence greater than the
user-specied minimum support (called minsup) and
minimum condence (called minconf ) respectively.
Our discussion is neutral with respect to the repre-
sentation of D. For example, D could be a data le,
a relational table, or the result of a relational expres-
sion.
An algorithm for nding all association rules,
henceforth referred to as the AIS algorithm, was pre-
sented in [4]. Another algorithm for this task, called
the SETM algorithm, has been proposed in [13]. In
this paper, we present two new algorithms, Apriori
and AprioriTid, that dier fundamentally from these
algorithms. We present experimental results showing
that the proposed algorithms always outperform the
earlier algorithms. The performance gap is shown to
increase with problem size, and ranges from a fac-
tor of three for small problems to more than an or-
der of magnitude for large problems. We then dis-
cuss how the best features of Apriori and Apriori-
Tid can be combined into a hybrid algorithm, called
AprioriHybrid. Experiments show that the Apriori-
Hybrid has excellent scale-up properties, opening up
the feasibility of mining association rules over very
large databases.
The problem of nding association rules falls within
the purview of database mining [3] [12], also called
knowledge discovery in databases [21]. Related, but
not directly applicable, work includes the induction
of classication rules [8] [11] [22], discovery of causal
rules [19], learning of logical denitions [18], tting
of functions to data [15], and clustering [9] [10]. The
closest work in the machine learning literature is the
KID3 algorithm presented in [20]. If used for nding
all association rules, this algorithmwill make as many
passes over the data as the number of combinations
of items in the antecedent, which is exponentially
large. Related work in the database literature is
the work on inferring functional dependencies from
data [16]. Functional dependencies are rules requiring
strict satisfaction. Consequently, having determined
a dependency X ! A, the algorithms in [16] consider
any other dependency of the form X + Y ! A
redundant and do not generate it. The association
rules we consider are probabilistic in nature. The
presence of a rule X ! A does not necessarily mean
that X + Y ! A also holds because the latter may
not have minimumsupport. Similarly, the presence of
rules X ! Y and Y ! Z does not necessarily mean
that X ! Z holds because the latter may not have
minimum condence.
There has been work on quantifying the \useful-
ness" or \interestingness" of a rule [20]. What is use-
ful or interesting is often application-dependent. The
need for a human in the loop and providing tools to
allow human guidance of the rule discovery process
has been articulated, for example, in [7] [14]. We do
not discuss these issues in this paper, except to point
out that these are necessary features of a rule discov-
ery system that may use our algorithms as the engine
of the discovery process.
1.1 Problem Decomposition and Paper
Organization
The problem of discovering all association rules can
be decomposed into two subproblems [4]:
1. Find all sets of items (itemsets) that have transac-
tion support above minimum support. The support
for an itemset is the number of transactions that
contain the itemset. Itemsets with minimum sup-
port are called large itemsets, and all others small
itemsets. In Section 2, we give new algorithms,
Apriori and AprioriTid, for solving this problem.
2. Use the large itemsets to generate the desired rules.
Here is a straightforward algorithm for this task.
For every large itemset l, nd all non-empty subsets
of l. For every such subset a, output a rule of
the form a =) (l   a) if the ratio of support(l)
to support(a) is at least minconf. We need to
consider all subsets of l to generate rules with
multiple consequents. Due to lack of space, we do
not discuss this subproblem further, but refer the
reader to [5] for a fast algorithm.
In Section 3, we show the relative performance
of the proposed Apriori and AprioriTid algorithms
against the AIS [4] and SETM [13] algorithms.
To make the paper self-contained, we include an
overview of the AIS and SETM algorithms in this
section. We also describe how the Apriori and
AprioriTid algorithms can be combined into a hybrid
algorithm, AprioriHybrid, and demonstrate the scale-
up properties of this algorithm. We conclude by
pointing out some related open problems in Section 4.
2 Discovering Large Itemsets
Algorithms for discovering large itemsets make mul-
tiple passes over the data. In the rst pass, we count
the support of individual items and determine which
of them are large, i.e. have minimumsupport. In each
subsequent pass, we start with a seed set of itemsets
found to be large in the previous pass. We use this
seed set for generating new potentially large itemsets,
called candidate itemsets, and count the actual sup-
port for these candidate itemsets during the pass over
the data. At the end of the pass, we determine which
of the candidate itemsets are actually large, and they
become the seed for the next pass. This process con-
tinues until no new large itemsets are found.
The Apriori and AprioriTid algorithms we propose
dier fundamentally from the AIS [4] and SETM [13]
algorithms in terms of which candidate itemsets are
counted in a pass and in the way that those candidates
are generated. In both the AIS and SETM algorithms,
candidate itemsets are generated on-the-y during the
pass as data is being read. Specically, after reading
a transaction, it is determined which of the itemsets
found large in the previous pass are present in the
transaction. New candidate itemsets are generated by
extending these large itemsets with other items in the
transaction. However, as we will see, the disadvantage
is that this results in unnecessarily generating and
counting too many candidate itemsets that turn out
to be small.
The Apriori and AprioriTid algorithms generate
the candidate itemsets to be counted in a pass by
using only the itemsets found large in the previous
pass { without considering the transactions in the
database. The basic intuition is that any subset
of a large itemset must be large. Therefore, the
candidate itemsets having k items can be generated
by joining large itemsets having k   1 items, and
deleting those that contain any subset that is not
large. This procedure results in generation of a much
smaller number of candidate itemsets.
The AprioriTid algorithm has the additional prop-
erty that the database is not used at all for count-
ing the support of candidate itemsets after the rst
pass. Rather, an encoding of the candidate itemsets
used in the previous pass is employed for this purpose.
In later passes, the size of this encoding can become
much smaller than the database, thus saving much
reading eort. We will explain these points in more
detail when we describe the algorithms.
Notation We assume that items in each transaction
are kept sorted in their lexicographic order. It is
straightforward to adapt these algorithms to the case
where the database D is kept normalized and each
database record is a <TID, item> pair, where TID is
the identier of the corresponding transaction.
We call the number of items in an itemset its size,
and call an itemset of size k a k-itemset. Items within
an itemset are kept in lexicographic order. We use
the notation c[1]  c[2]  . . .  c[k] to represent a k-
itemset c consisting of items c[1]; c[2]; . . .c[k], where
c[1] < c[2] < . . . < c[k]. If c = X  Y and Y
is an m-itemset, we also call Y an m-extension of
X. Associated with each itemset is a count eld to
store the support for this itemset. The count eld is
initialized to zero when the itemset is rst created.
We summarize in Table 1 the notation used in the
algorithms. The set Ck is used by AprioriTid and will
be further discussed when we describe this algorithm.
2.1 Algorithm Apriori
Figure 1 gives the Apriori algorithm. The rst pass
of the algorithm simply counts item occurrences to
determine the large 1-itemsets. A subsequent pass,
say pass k, consists of two phases. First, the large
itemsets Lk 1 found in the (k 1)th pass are used to
generate the candidate itemsets Ck, using the apriori-
gen function described in Section 2.1.1. Next, the
database is scanned and the support of candidates in
Ck is counted. For fast counting, we need to eciently
determine the candidates in Ck that are contained in a
Table 1: Notation
k-itemset An itemset having k items.
Set of large k-itemsets
Lk (those with minimum support).
Each member of this set has two elds:
i) itemset and ii) support count.
Set of candidate k-itemsets
Ck (potentially large itemsets).
Each member of this set has two elds:
i) itemset and ii) support count.
Set of candidate k-itemsets when the TIDs
Ck of the generating transactions are kept
associated with the candidates.
given transaction t. Section 2.1.2 describes the subset
function used for this purpose. See [5] for a discussion
of buer management.
1) L1 = flarge 1-itemsetsg;
2) for ( k = 2; Lk 1 6= ;; k++ ) do begin
3) Ck = apriori-gen(Lk 1 ); // New candidates
4) forall transactions t 2 D do begin
5) Ct = subset(Ck , t); // Candidates contained in t
6) forall candidates c 2 Ct do
7) c:count++;
8) end
9) Lk = fc 2 Ck j c:count  minsupg
10) end
11) Answer =
S
k
Lk;
Figure 1: Algorithm Apriori
2.1.1 Apriori Candidate Generation
The apriori-gen function takes as argument Lk 1,
the set of all large (k  1)-itemsets. It returns a
superset of the set of all large k-itemsets. The
function works as follows. 1 First, in the join step,
we join Lk 1 with Lk 1:
insert into Ck
select p.item1, p.item2, ..., p.itemk 1, q.itemk 1
from Lk 1 p, Lk 1 q
where p.item1 = q.item1, . . ., p.itemk 2 = q.itemk 2,
p.itemk 1 < q.itemk 1;
Next, in the prune step, we delete all itemsets c 2 Ck
such that some (k 1)-subset of c is not in Lk 1:
1Concurrent to our work, the following two-step candidate
generation procedure has been proposed in [17]:
C0
k
= fX [X 0jX;X 0 2 Lk 1; jX \X
0j = k   2g
Ck = fX 2 C
0
k
jX contains k members of Lk 1g
These two steps are similar to our join and prune steps
respectively. However, in general, step 1 would produce a
superset of the candidates produced by our join step.
forall itemsets c 2 Ck do
forall (k 1)-subsets s of c do
if (s 62 Lk 1) then
delete c from Ck;
Example Let L3 be ff1 2 3g, f1 2 4g, f1 3 4g, f1
3 5g, f2 3 4gg. After the join step, C4 will be ff1 2 3
4g, f1 3 4 5g g. The prune step will delete the itemset
f1 3 4 5g because the itemset f1 4 5g is not in L3.
We will then be left with only f1 2 3 4g in C4.
Contrast this candidate generation with the one
used in the AIS and SETM algorithms. In pass k
of these algorithms, a database transaction t is read
and it is determined which of the large itemsets in
Lk 1 are present in t. Each of these large itemsets
l is then extended with all those large items that
are present in t and occur later in the lexicographic
ordering than any of the items in l. Continuing with
the previous example, consider a transaction f1 2
3 4 5g. In the fourth pass, AIS and SETM will
generate two candidates, f1 2 3 4g and f1 2 3 5g,
by extending the large itemset f1 2 3g. Similarly, an
additional three candidate itemsets will be generated
by extending the other large itemsets in L3, leading
to a total of 5 candidates for consideration in the
fourth pass. Apriori, on the other hand, generates
and counts only one itemset, f1 3 4 5g, because it
concludes a priori that the other combinations cannot
possibly have minimum support.
Correctness We need to show that Ck  Lk.
Clearly, any subset of a large itemset must also
have minimum support. Hence, if we extended each
itemset in Lk 1 with all possible items and then
deleted all those whose (k  1)-subsets were not in
Lk 1, we would be left with a superset of the itemsets
in Lk.
The join is equivalent to extending Lk 1 with each
item in the database and then deleting those itemsets
for which the (k 1)-itemset obtained by deleting the
(k 1)th item is not in Lk 1. The condition p.itemk 1
< q.itemk 1 simply ensures that no duplicates are
generated. Thus, after the join step, Ck  Lk. By
similar reasoning, the prune step, where we delete
from Ck all itemsets whose (k 1)-subsets are not in
Lk 1, also does not delete any itemset that could be
in Lk.
Variation: Counting Candidates of Multiple
Sizes in One Pass Rather than counting only
candidates of size k in the kth pass, we can also
count the candidates C0
k+1
, where C0
k+1
is generated
from Ck, etc. Note that C0k+1  Ck+1 since Ck+1 is
generated from Lk. This variation can pay o in the
later passes when the cost of counting and keeping in
memory additional C0
k+1
  Ck+1 candidates becomes
less than the cost of scanning the database.
2.1.2 Subset Function
Candidate itemsets Ck are stored in a hash-tree. A
node of the hash-tree either contains a list of itemsets
(a leaf node) or a hash table (an interior node). In an
interior node, each bucket of the hash table points to
another node. The root of the hash-tree is dened to
be at depth 1. An interior node at depth d points to
nodes at depth d+1. Itemsets are stored in the leaves.
When we add an itemset c, we start from the root and
go down the tree until we reach a leaf. At an interior
node at depth d, we decide which branch to follow
by applying a hash function to the dth item of the
itemset. All nodes are initially created as leaf nodes.
When the number of itemsets in a leaf node exceeds
a specied threshold, the leaf node is converted to an
interior node.
Starting from the root node, the subset function
nds all the candidates contained in a transaction
t as follows. If we are at a leaf, we nd which of
the itemsets in the leaf are contained in t and add
references to them to the answer set. If we are at an
interior node and we have reached it by hashing the
item i, we hash on each item that comes after i in t
and recursively apply this procedure to the node in
the corresponding bucket. For the root node, we hash
on every item in t.
To see why the subset function returns the desired
set of references, consider what happens at the root
node. For any itemset c contained in transaction t, the
rst item of c must be in t. At the root, by hashing on
every item in t, we ensure that we only ignore itemsets
that start with an item not in t. Similar arguments
apply at lower depths. The only additional factor is
that, since the items in any itemset are ordered, if we
reach the current node by hashing the item i, we only
need to consider the items in t that occur after i.
2.2 Algorithm AprioriTid
The AprioriTid algorithm, shown in Figure 2, also
uses the apriori-gen function (given in Section 2.1.1)
to determine the candidate itemsets before the pass
begins. The interesting feature of this algorithm is
that the database D is not used for counting support
after the rst pass. Rather, the set Ck is used
for this purpose. Each member of the set Ck is
of the form < TID; fXkg >, where each Xk is a
potentially large k-itemset present in the transaction
with identier TID. For k = 1, C1 corresponds to
the database D, although conceptually each item i
is replaced by the itemset fig. For k > 1, Ck is
generated by the algorithm (step 10). The member
of Ck corresponding to transaction t is <t:T ID,
fc 2 Ckjc contained in tg>. If a transaction does
not contain any candidate k-itemset, then Ck will
not have an entry for this transaction. Thus, the
number of entries in Ck may be smaller than the
number of transactions in the database, especially for
large values of k. In addition, for large values of k,
each entry may be smaller than the corresponding
transaction because very few candidates may be
contained in the transaction. However, for small
values for k, each entry may be larger than the
corresponding transaction because an entry in Ck
includes all candidate k-itemsets contained in the
transaction.
In Section 2.2.1, we give the data structures used
to implement the algorithm. See [5] for a proof of
correctness and a discussion of buer management.
1) L1 = flarge 1-itemsetsg;
2) C1 = database D;
3) for ( k = 2; Lk 1 6= ;; k++ ) do begin
4) Ck = apriori-gen(Lk 1); // New candidates
5) Ck = ;;
6) forall entries t 2 Ck 1 do begin
7) // determine candidate itemsets in Ck contained
// in the transaction with identier t.TID
Ct = fc 2 Ck j (c  c[k]) 2 t:set-of-itemsets ^
(c  c[k 1]) 2 t.set-of-itemsetsg;
8) forall candidates c 2 Ct do
9) c:count++;
10) if (Ct 6= ;) then Ck += < t:TID; Ct >;
11) end
12) Lk = fc 2 Ck j c:count  minsupg
13) end
14) Answer =
S
k
Lk;
Figure 2: Algorithm AprioriTid
Example Consider the database in Figure 3 and
assume that minimum support is 2 transactions.
Calling apriori-gen with L1 at step 4 gives the
candidate itemsets C2. In steps 6 through 10, we
count the support of candidates in C2 by iterating over
the entries in C1 and generate C2. The rst entry in
C1 is f f1g f3g f4g g, corresponding to transaction
100. The Ct at step 7 corresponding to this entry t
is f f1 3g g, because f1 3g is a member of C2 and
both (f1 3g - f1g) and (f1 3g - f3g) are members of
t.set-of-itemsets.
Calling apriori-gen with L2 gives C3. Making a pass
over the data with C2 and C3 generates C3. Note that
there is no entry in C3 for the transactions with TIDs
100 and 400, since they do not contain any of the
itemsets in C3. The candidate f2 3 5g in C3 turns
out to be large and is the only member of L3. When
we generate C4 using L3, it turns out to be empty,
and we terminate.
Database
TID Items
100 1 3 4
200 2 3 5
300 1 2 3 5
400 2 5
C1
TID Set-of-Itemsets
100 f f1g, f3g, f4g g
200 f f2g, f3g, f5g g
300 f f1g, f2g, f3g, f5g g
400 f f2g, f5g g
L1
Itemset Support
f1g 2
f2g 3
f3g 3
f5g 3
C2
Itemset Support
f1 2g 1
f1 3g 2
f1 5g 1
f2 3g 2
f2 5g 3
f3 5g 2
C2
TID Set-of-Itemsets
100 f f1 3g g
200 f f2 3g, f2 5g, f3 5g g
300 f f1 2g, f1 3g, f1 5g,
f2 3g, f2 5g, f3 5g g
400 f f2 5g g
L2
Itemset Support
f1 3g 2
f2 3g 2
f2 5g 3
f3 5g 2
C3
Itemset Support
f2 3 5g 2
C3
TID Set-of-Itemsets
200 f f2 3 5g g
300 f f2 3 5g g
L3
Itemset Support
f2 3 5g 2
Figure 3: Example
2.2.1 Data Structures
We assign each candidate itemset a unique number,
called its ID. Each set of candidate itemsets Ck is kept
in an array indexed by the IDs of the itemsets in Ck.
A member of Ck is now of the form < TID; fIDg >.
Each Ck is stored in a sequential structure.
The apriori-gen function generates a candidate k-
itemset ck by joining two large (k 1)-itemsets. We
maintain two additional elds for each candidate
itemset: i) generators and ii) extensions. The
generators eld of a candidate itemset ck stores the
IDs of the two large (k   1)-itemsets whose join
generated ck. The extensions eld of an itemset
ck stores the IDs of all the (k+1)-candidates that
are extensions of ck. Thus, when a candidate ck is
generated by joining l1
k 1 and l
2
k 1, we save the IDs
of l1
k 1
and l2
k 1
in the generators eld for ck. At the
same time, the ID of ck is added to the extensions
eld of l1
k 1
.
We now describe how Step 7 of Figure 2 is
implemented using the above data structures. Recall
that the t.set-of-itemsets eld of an entry t in Ck 1
gives the IDs of all (k  1)-candidates contained in
transaction t.TID. For each such candidate ck 1 the
extensions eld gives Tk, the set of IDs of all the
candidate k-itemsets that are extensions of ck 1. For
each ck in Tk, the generators eld gives the IDs of
the two itemsets that generated ck. If these itemsets
are present in the entry for t.set-of-itemsets, we can
conclude that ck is present in transaction t.TID, and
add ck to Ct.
3 Performance
To assess the relative performance of the algorithms
for discovering large sets, we performed several
experiments on an IBM RS/6000 530H workstation
with a CPU clock rate of 33 MHz, 64 MB of main
memory, and running AIX 3.2. The data resided in
the AIX le system and was stored on a 2GB SCSI
3.5" drive, with measured sequential throughput of
about 2 MB/second.
We rst give an overview of the AIS [4] and SETM
[13] algorithms against which we compare the per-
formance of the Apriori and AprioriTid algorithms.
We then describe the synthetic datasets used in the
performance evaluation and show the performance re-
sults. Finally, we describe how the best performance
features of Apriori and AprioriTid can be combined
into an AprioriHybrid algorithm and demonstrate its
scale-up properties.
3.1 The AIS Algorithm
Candidate itemsets are generated and counted on-
the-y as the database is scanned. After reading a
transaction, it is determined which of the itemsets
that were found to be large in the previous pass are
contained in this transaction. New candidate itemsets
are generated by extending these large itemsets with
other items in the transaction. A large itemset l is
extended with only those items that are large and
occur later in the lexicographic ordering of items than
any of the items in l. The candidates generated
from a transaction are added to the set of candidate
itemsets maintained for the pass, or the counts of
the corresponding entries are increased if they were
created by an earlier transaction. See [4] for further
details of the AIS algorithm.
3.2 The SETM Algorithm
The SETM algorithm [13] was motivated by the desire
to use SQL to compute large itemsets. Like AIS,
the SETM algorithm also generates candidates on-
the-y based on transactions read from the database.
It thus generates and counts every candidate itemset
that the AIS algorithmgenerates. However, to use the
standard SQL join operation for candidate generation,
SETM separates candidate generation from counting.
It saves a copy of the candidate itemset together with
the TID of the generating transaction in a sequential
structure. At the end of the pass, the support count
of candidate itemsets is determined by sorting and
aggregating this sequential structure.
SETM remembers the TIDs of the generating
transactions with the candidate itemsets. To avoid
needing a subset operation, it uses this information
to determine the large itemsets contained in the
transaction read. Lk  Ck and is obtained by deleting
those candidates that do not have minimum support.
Assuming that the database is sorted in TID order,
SETM can easily nd the large itemsets contained in a
transaction in the next pass by sorting Lk on TID. In
fact, it needs to visit every member of Lk only once in
the TID order, and the candidate generation can be
performed using the relational merge-join operation
[13].
The disadvantage of this approach is mainly due
to the size of candidate sets Ck. For each candidate
itemset, the candidate set now has as many entries
as the number of transactions in which the candidate
itemset is present. Moreover, when we are ready to
count the support for candidate itemsets at the end
of the pass, Ck is in the wrong order and needs to be
sorted on itemsets. After counting and pruning out
small candidate itemsets that do not have minimum
support, the resulting set Lk needs another sort on
TID before it can be used for generating candidates
in the next pass.
3.3 Generation of Synthetic Data
We generated synthetic transactions to evaluate the
performance of the algorithms over a large range of
data characteristics. These transactions mimic the
transactions in the retailing environment. Our model
of the \real" world is that people tend to buy sets
of items together. Each such set is potentially a
maximal large itemset. An example of such a set
might be sheets, pillow case, comforter, and rues.
However, some people may buy only some of the
items from such a set. For instance, some people
might buy only sheets and pillow case, and some only
sheets. A transaction may contain more than one
large itemset. For example, a customer might place an
order for a dress and jacket when ordering sheets and
pillow cases, where the dress and jacket together form
another large itemset. Transaction sizes are typically
clustered around a mean and a few transactions have
many items. Typical sizes of large itemsets are also
clustered around a mean, with a few large itemsets
having a large number of items.
To create a dataset, our synthetic data generation
program takes the parameters shown in Table 2.
Table 2: Parameters
jDj Number of transactions
jT j Average size of the transactions
jIj Average size of the maximal potentially
large itemsets
jLj Number of maximal potentially large itemsets
N Number of items
We rst determine the size of the next transaction.
The size is picked from a Poisson distribution with
mean  equal to jT j. Note that if each item is chosen
with the same probability p, and there are N items,
the expected number of items in a transaction is given
by a binomial distribution with parameters N and p,
and is approximated by a Poisson distribution with
mean Np.
We then assign items to the transaction. Each
transaction is assigned a series of potentially large
itemsets. If the large itemset on hand does not t in
the transaction, the itemset is put in the transaction
anyway in half the cases, and the itemset is moved to
the next transaction the rest of the cases.
Large itemsets are chosen from a set T of such
itemsets. The number of itemsets in T is set to
jLj. There is an inverse relationship between jLj and
the average support for potentially large itemsets.
An itemset in T is generated by rst picking the
size of the itemset from a Poisson distribution with
mean  equal to jIj. Items in the rst itemset
are chosen randomly. To model the phenomenon
that large itemsets often have common items, some
fraction of items in subsequent itemsets are chosen
from the previous itemset generated. We use an
exponentially distributed random variable with mean
equal to the correlation level to decide this fraction
for each itemset. The remaining items are picked at
random. In the datasets used in the experiments,
the correlation level was set to 0.5. We ran some
experiments with the correlation level set to 0.25 and
0.75 but did not nd much dierence in the nature of
our performance results.
Each itemset in T has a weight associated with
it, which corresponds to the probability that this
itemset will be picked. This weight is picked from
an exponential distribution with unit mean, and is
then normalized so that the sum of the weights for all
the itemsets in T is 1. The next itemset to be put
in the transaction is chosen from T by tossing an jLj-
sided weighted coin, where the weight for a side is the
probability of picking the associated itemset.
To model the phenomenon that all the items in
a large itemset are not always bought together, we
assign each itemset in T a corruption level c. When
adding an itemset to a transaction, we keep dropping
an item from the itemset as long as a uniformly
distributed random number between 0 and 1 is less
than c. Thus for an itemset of size l, we will add l
items to the transaction 1  c of the time, l  1 items
c(1  c) of the time, l  2 items c2(1  c) of the time,
etc. The corruption level for an itemset is xed and
is obtained from a normal distribution with mean 0.5
and variance 0.1.
We generated datasets by setting N = 1000 and jLj
= 2000. We chose 3 values for jT j: 5, 10, and 20. We
also chose 3 values for jIj: 2, 4, and 6. The number of
transactions was to set to 100,000 because, as we will
see in Section 3.4, SETM could not be run for larger
values. However, for our scale-up experiments, we
generated datasets with up to 10 million transactions
(838MB for T20). Table 3 summarizes the dataset
parameter settings. For the same jT j and jDj values,
the size of datasets in megabytes were roughly equal
for the dierent values of jIj.
Table 3: Parameter settings
Name jT j jIj jDj Size in Megabytes
T5.I2.D100K 5 2 100K 2.4
T10.I2.D100K 10 2 100K 4.4
T10.I4.D100K 10 4 100K
T20.I2.D100K 20 2 100K 8.4
T20.I4.D100K 20 4 100K
T20.I6.D100K 20 6 100K
3.4 Relative Performance
Figure 4 shows the execution times for the six
synthetic datasets given in Table 3 for decreasing
values of minimumsupport. As the minimumsupport
decreases, the execution times of all the algorithms
increase because of increases in the total number of
candidate and large itemsets.
For SETM, we have only plotted the execution
times for the dataset T5.I2.D100K in Figure 4. The
execution times for SETM for the two datasets with
an average transaction size of 10 are given in Table 4.
We did not plot the execution times in Table 4
on the corresponding graphs because they are too
large compared to the execution times of the other
algorithms. For the three datasets with transaction
sizes of 20, SETM took too long to execute and
we aborted those runs as the trends were clear.
Clearly, Apriori beats SETM by more than an order
of magnitude for large datasets.
T5.I2.D100K T10.I2.D100K
0
10
20
30
40
50
60
70
80
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
SETM
AIS
AprioriTid
Apriori
0
20
40
60
80
100
120
140
160
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AIS
AprioriTid
Apriori
T10.I4.D100K T20.I2.D100K
0
50
100
150
200
250
300
350
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AIS
AprioriTid
Apriori
0
100
200
300
400
500
600
700
800
900
1000
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AIS
AprioriTid
Apriori
T20.I4.D100K T20.I6.D100K
0
200
400
600
800
1000
1200
1400
1600
1800
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AIS
AprioriTid
Apriori
0
500
1000
1500
2000
2500
3000
3500
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AIS
AprioriTid
Apriori
Figure 4: Execution times
Table 4: Execution times in seconds for SETM
Algorithm Minimum Support
2.0% 1.5% 1.0% 0.75% 0.5%
Dataset T10.I2.D100K
SETM 74 161 838 1262 1878
Apriori 4.4 5.3 11.0 14.5 15.3
Dataset T10.I4.D100K
SETM 41 91 659 929 1639
Apriori 3.8 4.8 11.2 17.4 19.3
Apriori beats AIS for all problem sizes, by factors
ranging from 2 for high minimum support to more
than an order of magnitude for low levels of support.
AIS always did considerably better than SETM. For
small problems, AprioriTid did about as well as
Apriori, but performance degraded to about twice as
slow for large problems.
3.5 Explanation of the Relative Performance
To explain these performance trends, we show in
Figure 5 the sizes of the large and candidate sets in
dierent passes for the T10.I4.D100K dataset for the
minimum support of 0.75%. Note that the Y-axis in
this graph has a log scale.
1
10
100
1000
10000
100000
1e+06
1e+07
1 2 3 4 5 6 7
N
u
m
b
e
r 
o
f 
It
e
m
se
ts
Pass Number
C-k-m (SETM)
C-k-m (AprioriTid)
C-k (AIS, SETM)
C-k (Apriori, AprioriTid)
L-k
Figure 5: Sizes of the large and candidate sets
(T10.I4.D100K, minsup = 0.75%)
The fundamental problem with the SETM algo-
rithm is the size of its Ck sets. Recall that the size of
the set Ck is given by
X
candidate itemsets c
support-count(c):
Thus, the sets Ck are roughly S times bigger than the
corresponding Ck sets, where S is the average support
count of the candidate itemsets. Unless the problem
size is very small, the Ck sets have to be written
to disk, and externally sorted twice, causing the
SETM algorithm to perform poorly.2 This explains
the jump in time for SETM in Table 4 when going
from 1.5% support to 1.0% support for datasets with
transaction size 10. The largest dataset in the scale-
up experiments for SETM in [13] was still small
enough that Ck could t in memory; hence they did
not encounter this jump in execution time. Note that
for the same minimum support, the support count for
candidate itemsets increases linearly with the number
of transactions. Thus, as we increase the number of
transactions for the same values of jT j and jIj, though
the size of Ck does not change, the size of Ck goes up
linearly. Thus, for datasets with more transactions,
the performance gap between SETM and the other
algorithms will become even larger.
The problem with AIS is that it generates too many
candidates that later turn out to be small, causing
it to waste too much eort. Apriori also counts too
many small sets in the second pass (recall that C2 is
really a cross-product of L1 with L1). However, this
wastage decreases dramatically from the third pass
onward. Note that for the example in Figure 5, after
pass 3, almost every candidate itemset counted by
Apriori turns out to be a large set.
AprioriTid also has the problem of SETM that Ck
tends to be large. However, the apriori candidate
generation used by AprioriTid generates signicantly
fewer candidates than the transaction-based candi-
date generation used by SETM. As a result, the Ck of
AprioriTid has fewer entries than that of SETM. Apri-
oriTid is also able to use a single word (ID) to store
a candidate rather than requiring as many words as
the number of items in the candidate.3 In addition,
unlike SETM, AprioriTid does not have to sort Ck.
Thus, AprioriTid does not suer as much as SETM
from maintaining Ck.
AprioriTid has the nice feature that it replaces a
pass over the original dataset by a pass over the set
Ck. Hence, AprioriTid is very eective in later passes
when the size of Ck becomes small compared to the
2The cost of external sorting in SETM can be reduced
somewhat as follows. Before writing out entries in Ck to
disk, we can sort them on itemsets using an internal sorting
procedure, and write them as sorted runs. These sorted runs
can then be merged to obtain support counts. However,
given the poor performance of SETM, we do not expect this
optimization to aect the algorithm choice.
3For SETM to use IDs, it would have to maintain two
additional in-memory data structures: a hash table to nd
out whether a candidate has been generated previously, and
a mapping from the IDs to candidates. However, this would
destroy the set-oriented nature of the algorithm. Also, once we
have the hash table which gives us the IDs of candidates, we
might as well count them at the same time and avoid the two
external sorts. We experimentedwith this variant of SETM and
found that, while it did better than SETM, it still performed
much worse than Apriori or AprioriTid.
size of the database. Thus, we nd that AprioriTid
beats Apriori when its Ck sets can t in memory and
the distribution of the large itemsets has a long tail.
When Ck doesn't t in memory, there is a jump in
the execution time for AprioriTid, such as when going
from 0.75% to 0.5% for datasets with transaction size
10 in Figure 4. In this region, Apriori starts beating
AprioriTid.
3.6 Algorithm AprioriHybrid
It is not necessary to use the same algorithm in all the
passes over data. Figure 6 shows the execution times
for Apriori and AprioriTid for dierent passes over the
dataset T10.I4.D100K. In the earlier passes, Apriori
does better than AprioriTid. However, AprioriTid
beats Apriori in later passes. We observed similar
relative behavior for the other datasets, the reason
for which is as follows. Apriori and AprioriTid
use the same candidate generation procedure and
therefore count the same itemsets. In the later
passes, the number of candidate itemsets reduces
(see the size of Ck for Apriori and AprioriTid in
Figure 5). However, Apriori still examines every
transaction in the database. On the other hand,
rather than scanning the database, AprioriTid scans
Ck for obtaining support counts, and the size of Ck
has become smaller than the size of the database.
When the Ck sets can t in memory, we do not even
incur the cost of writing them to disk.
0
2
4
6
8
10
12
14
1 2 3 4 5 6 7
T
im
e
 (
se
c)
Pass #
Apriori
AprioriTid
Figure 6: Per pass execution times of Apriori and
AprioriTid (T10.I4.D100K, minsup = 0.75%)
Based on these observations, we can design a
hybrid algorithm, which we call AprioriHybrid, that
uses Apriori in the initial passes and switches to
AprioriTid when it expects that the set Ck at the
end of the pass will t in memory. We use the
following heuristic to estimate if Ck would t in
memory in the next pass. At the end of the
current pass, we have the counts of the candidates
in Ck. From this, we estimate what the size of Ck
would have been if it had been generated. This
size, in words, is (
P
candidates c 2 Ck support(c) +
number of transactions). If Ck in this pass was small
enough to t in memory, and there were fewer large
candidates in the current pass than the previous pass,
we switch to AprioriTid. The latter condition is added
to avoid switching when Ck in the current pass ts in
memory but Ck in the next pass may not.
Switching from Apriori to AprioriTid does involve
a cost. Assume that we decide to switch from Apriori
to AprioriTid at the end of the kth pass. In the
(k+1)th pass, after nding the candidate itemsets
contained in a transaction, we will also have to add
their IDs to Ck+1 (see the description of AprioriTid
in Section 2.2). Thus there is an extra cost incurred
in this pass relative to just running Apriori. It is only
in the (k+2)th pass that we actually start running
AprioriTid. Thus, if there are no large (k+1)-itemsets,
or no (k+ 2)-candidates, we will incur the cost of
switching without getting any of the savings of using
AprioriTid.
Figure 7 shows the performance of AprioriHybrid
relative to Apriori and AprioriTid for three datasets.
AprioriHybrid performs better than Apriori in almost
all cases. For T10.I2.D100K with 1.5% support,
AprioriHybrid does a little worse than Apriori since
the pass in which the switch occurred was the
last pass; AprioriHybrid thus incurred the cost of
switching without realizing the benets. In general,
the advantage of AprioriHybrid over Apriori depends
on how the size of the Ck set decline in the later
passes. If Ck remains large until nearly the end and
then has an abrupt drop, we will not gain much by
using AprioriHybrid since we can use AprioriTid only
for a short period of time after the switch. This is
what happened with the T20.I6.D100K dataset. On
the other hand, if there is a gradual decline in the
size of Ck, AprioriTid can be used for a while after the
switch, and a signicant improvement can be obtained
in the execution time.
3.7 Scale-up Experiment
Figure 8 shows how AprioriHybrid scales up as the
number of transactions is increased from 100,000 to
10 million transactions. We used the combinations
(T5.I2), (T10.I4), and (T20.I6) for the average sizes
of transactions and itemsets respectively. All other
parameters were the same as for the data in Table 3.
The sizes of these datasets for 10 million transactions
were 239MB, 439MB and 838MB respectively. The
minimum support level was set to 0.75%. The
execution times are normalized with respect to the
times for the 100,000 transaction datasets in the rst
T10.I2.D100K
0
5
10
15
20
25
30
35
40
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AprioriTid
Apriori
AprioriHybrid
T10.I4.D100K
0
5
10
15
20
25
30
35
40
45
50
55
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AprioriTid
Apriori
AprioriHybrid
T20.I6.D100K
0
100
200
300
400
500
600
700
0.250.330.50.7511.52
T
im
e
 (
se
c)
Minimum Support
AprioriTid
Apriori
AprioriHybrid
Figure 7: Execution times: AprioriHybrid
graph and with respect to the 1 million transaction
dataset in the second. As shown, the execution times
scale quite linearly.
0
2
4
6
8
10
12
100 250 500 750 1000
R
e
la
tiv
e
 T
im
e
Number of Transactions (in ’000s)
T20.I6
T10.I4
T5.I2
0
2
4
6
8
10
12
14
1 2.5 5 7.5 10
R
e
la
tiv
e
 T
im
e
Number of Transactions (in Millions)
T20.I6
T10.I4
T5.I2
Figure 8: Number of transactions scale-up
Next, we examined how AprioriHybrid scaled up
with the number of items. We increased the num-
ber of items from 1000 to 10,000 for the three pa-
rameter settings T5.I2.D100K, T10.I4.D100K and
T20.I6.D100K. All other parameters were the same
as for the data in Table 3. We ran experiments for a
minimum support at 0.75%, and obtained the results
shown in Figure 9. The execution times decreased a
little since the average support for an item decreased
as we increased the number of items. This resulted
in fewer large itemsets and, hence, faster execution
times.
Finally, we investigated the scale-up as we increased
the average transaction size. The aim of this
experiment was to see how our data structures scaled
with the transaction size, independent of other factors
like the physical database size and the number of
large itemsets. We kept the physical size of the
0
5
10
15
20
25
30
35
40
45
1000 2500 5000 7500 10000
T
im
e
 (
se
c)
Number of Items
T20.I6
T10.I4
T5.I2
Figure 9: Number of items scale-up
database roughly constant by keeping the product
of the average transaction size and the number of
transactions constant. The number of transactions
ranged from 200,000 for the database with an average
transaction size of 5 to 20,000 for the database with
an average transaction size 50. Fixing the minimum
support as a percentage would have led to large
increases in the number of large itemsets as the
transaction size increased, since the probability of
a itemset being present in a transaction is roughly
proportional to the transaction size. We therefore
xed the minimum support level in terms of the
number of transactions. The results are shown in
Figure 10. The numbers in the key (e.g. 500) refer
to this minimum support. As shown, the execution
times increase with the transaction size, but only
gradually. The main reason for the increase was that
in spite of setting the minimum support in terms
of the number of transactions, the number of large
itemsets increased with increasing transaction length.
A secondary reason was that nding the candidates
present in a transaction took a little longer time.
4 Conclusions and Future Work
We presented two new algorithms, Apriori and Apri-
oriTid, for discovering all signicant association rules
between items in a large database of transactions.
We compared these algorithms to the previously
known algorithms, the AIS [4] and SETM [13] algo-
rithms. We presented experimental results, showing
that the proposed algorithms always outperform AIS
and SETM. The performance gap increased with the
problem size, and ranged from a factor of three for
small problems to more than an order of magnitude
for large problems.
We showed how the best features of the two pro-
0
5
10
15
20
25
30
5 10 20 30 40 50
T
im
e
 (
se
c)
Transaction Size
500
750
1000
Figure 10: Transaction size scale-up
posed algorithms can be combined into a hybrid al-
gorithm, called AprioriHybrid, which then becomes
the algorithm of choice for this problem. Scale-up ex-
periments showed that AprioriHybrid scales linearly
with the number of transactions. In addition, the ex-
ecution time decreases a little as the number of items
in the database increases. As the average transaction
size increases (while keeping the database size con-
stant), the execution time increases only gradually.
These experiments demonstrate the feasibility of us-
ing AprioriHybrid in real applications involving very
large databases.
The algorithms presented in this paper have been
implemented on several data repositories, including
the AIX le system, DB2/MVS, and DB2/6000.
We have also tested these algorithms against real
customer data, the details of which can be found in
[5]. In the future, we plan to extend this work along
the following dimensions:
 Multiple taxonomies (is-a hierarchies) over items
are often available. An example of such a
hierarchy is that a dish washer is a kitchen
appliance is a heavy electric appliance, etc. We
would like to be able to nd association rules that
use such hierarchies.
 We did not consider the quantities of the items
bought in a transaction, which are useful for some
applications. Finding such rules needs further
work.
The work reported in this paper has been done
in the context of the Quest project at the IBM Al-
maden Research Center. In Quest, we are exploring
the various aspects of the database mining problem.
Besides the problem of discovering association rules,
some other problems that we have looked into include
the enhancement of the database capability with clas-
sication queries [2] and similarity queries over time
sequences [1]. We believe that database mining is an
important new application area for databases, com-
bining commercial interest with intriguing research
questions.
Acknowledgment We wish to thank Mike Carey
for his insightful comments and suggestions.
References
[1] R. Agrawal, C. Faloutsos, and A. Swami. Ef-
cient similarity search in sequence databases.
In Proc. of the Fourth International Conference
on Foundations of Data Organization and Algo-
rithms, Chicago, October 1993.
[2] R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and
A. Swami. An interval classier for database
mining applications. In Proc. of the VLDB
Conference, pages 560{573, Vancouver, British
Columbia, Canada, 1992.
[3] R. Agrawal, T. Imielinski, and A. Swami.
Database mining: A performance perspective.
IEEE Transactions on Knowledge and Data En-
gineering, 5(6):914{925, December 1993. Special
Issue on Learning and Discovery in Knowledge-
Based Databases.
[4] R. Agrawal, T. Imielinski, and A. Swami. Mining
association rules between sets of items in large
databases. In Proc. of the ACM SIGMOD Con-
ference on Management of Data, Washington,
D.C., May 1993.
[5] R. Agrawal and R. Srikant. Fast algorithms for
mining association rules in large databases. Re-
search Report RJ 9839, IBM Almaden Research
Center, San Jose, California, June 1994.
[6] D. S. Associates. The new direct marketing.
Business One Irwin, Illinois, 1990.
[7] R. Brachman et al. Integrated support for data
archeology. In AAAI-93 Workshop on Knowledge
Discovery in Databases, July 1993.
[8] L. Breiman, J. H. Friedman, R. A. Olshen, and
C. J. Stone. Classication and Regression Trees.
Wadsworth, Belmont, 1984.
[9] P. Cheeseman et al. Autoclass: A bayesian
classication system. In 5th Int'l Conf. on
Machine Learning. Morgan Kaufman, June 1988.
[10] D. H. Fisher. Knowledge acquisition via incre-
mental conceptual clustering. Machine Learning,
2(2), 1987.
[11] J. Han, Y. Cai, and N. Cercone. Knowledge
discovery in databases: An attribute oriented
approach. In Proc. of the VLDB Conference,
pages 547{559, Vancouver, British Columbia,
Canada, 1992.
[12] M. Holsheimer and A. Siebes. Data mining: The
search for knowledge in databases. Technical
Report CS-R9406, CWI, Netherlands, 1994.
[13] M. Houtsma and A. Swami. Set-oriented mining
of association rules. Research Report RJ 9567,
IBM Almaden Research Center, San Jose, Cali-
fornia, October 1993.
[14] R. Krishnamurthy and T. Imielinski. Practi-
tioner problems in need of database research: Re-
search directions in knowledge discovery. SIG-
MOD RECORD, 20(3):76{78, September 1991.
[15] P. Langley, H. Simon, G. Bradshaw, and
J. Zytkow. Scientic Discovery: Computational
Explorations of the Creative Process. MIT Press,
1987.
[16] H. Mannila and K.-J. Raiha. Dependency
inference. In Proc. of the VLDB Conference,
pages 155{158, Brighton, England, 1987.
[17] H. Mannila, H. Toivonen, and A. I. Verkamo.
Ecient algorithms for discovering association
rules. In KDD-94: AAAI Workshop on Knowl-
edge Discovery in Databases, July 1994.
[18] S. Muggleton and C. Feng. Ecient induction
of logic programs. In S. Muggleton, editor,
Inductive Logic Programming. Academic Press,
1992.
[19] J. Pearl. Probabilistic reasoning in intelligent
systems: Networks of plausible inference, 1992.
[20] G. Piatestsky-Shapiro. Discovery, analy-
sis, and presentation of strong rules. In
G. Piatestsky-Shapiro, editor, Knowledge Dis-
covery in Databases. AAAI/MIT Press, 1991.
[21] G. Piatestsky-Shapiro, editor. Knowledge Dis-
covery in Databases. AAAI/MIT Press, 1991.
[22] J. R. Quinlan. C4.5: Programs for Machine
Learning. Morgan Kaufman, 1993.
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\Mining-Frequent-Patterns-without-Candidate-Generation\dami04_fptree.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining and Knowledge Discovery, 8, 53–87, 2004
c© 2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
Mining Frequent Patterns without Candidate
Generation: A Frequent-Pattern Tree
Approach∗
JIAWEI HAN hanj@cs.uiuc.edu
University of Illinois at Urbana-Champaign
JIAN PEI† jianpei@cse.buffalo.edu
State University of New York at Buffalo
YIWEN YIN yiweny@cs.sfu.ca
Simon Fraser University
RUNYING MAO runyingm@microsoft.com
Microsoft Corporation
Editor: Heikki Mannila
Received May 21, 2000; Revised April 21, 2001
Abstract. Mining frequent patterns in transaction databases, time-series databases, and many other kinds of
databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like
candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there
exist a large number of patterns and/or long patterns.
In this study, we propose a novel frequent-pattern tree (FP-tree) structure, which is an extended prefix-tree
structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-
based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth.
Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a condensed,
smaller data structure, FP-tree which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts
a pattern-fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a
partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for
mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance
study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns,
and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported
new frequent-pattern mining methods.
Keywords: frequent pattern mining, association mining, algorithm, performance improvements, data structure
∗The work was done at Simon Fraser University, Canada, and it was supported in part by the Natural Sciences
and Engineering Research Council of Canada, and the Networks of Centres of Excellence of Canada.
†To whom correspondence should be addressed.
54 HAN ET AL.
1. Introduction
Frequent-pattern mining plays an essential role in mining associations (Agrawal et al.,
1993, 1996; Agrawal and Srikant, 1994; Mannila et al., 1994), correlations (Brin et al.,
1997), causality (Silverstein et al., 1998), sequential patterns (Agrawal and Srikant, 1995),
episodes (Mannila et al., 1997), multi-dimensional patterns (Lent et al., 1997; Kamber
et al., 1997), max-patterns (Bayardo, 1998), partial periodicity (Han et al., 1999), emerging
patterns (Dong and Li, 1999), and many other important data mining tasks.
Most of the previous studies, such as Agrawal and Srikant (1994), Mannila et al. (1994),
Agrawal et al. (1996), Savasere et al. (1995), Park et al. (1995), Lent et al. (1997), Sarawagi
et al. (1998), Srikant et al. (1997), Ng et al. (1998) and Grahne et al. (2000), adopt an
Apriori-like approach, which is based on the anti-monotone Apriori heuristic (Agrawal and
Srikant, 1994): if any length k pattern is not frequent in the database, its length (k + 1)
super-pattern can never be frequent. The essential idea is to iteratively generate the set of
candidate patterns of length (k +1) from the set of frequent-patterns of length k (for k ≥ 1),
and check their corresponding occurrence frequencies in the database.
The Apriori heuristic achieves good performance gained by (possibly significantly) re-
ducing the size of candidate sets. However, in situations with a large number of frequent
patterns, long patterns, or quite low minimum support thresholds, an Apriori-like algorithm
may suffer from the following two nontrivial costs:
– It is costly to handle a huge number of candidate sets. For example, if there are 104
frequent 1-itemsets, the Apriori algorithm will need to generate more than 107 length-2
candidates and accumulate and test their occurrence frequencies. Moreover, to discover
a frequent pattern of size 100, such as {a1, . . . , a100}, it must generate 2100 − 2 ≈ 1030
candidates in total. This is the inherent cost of candidate generation, no matter what
implementation technique is applied.
– It is tedious to repeatedly scan the database and check a large set of candidates by pattern
matching, which is especially true for mining long patterns.
Can one develop a method that may avoid candidate generation-and-test and utilize some
novel data structures to reduce the cost in frequent-pattern mining? This is the motivation
of this study.
In this paper, we develop and integrate the following three techniques in order to solve
this problem.
First, a novel, compact data structure, called frequent-pattern tree, or FP-tree in short,
is constructed, which is an extended prefix-tree structure storing crucial, quantitative infor-
mation about frequent patterns. To ensure that the tree structure is compact and informative,
only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in
such a way that more frequently occurring nodes will have better chances of node sharing
than less frequently occurring ones. Our experiments show that such a tree is compact,
and it is sometimes orders of magnitude smaller than the original database. Subsequent
frequent-pattern mining will only need to work on the FP-tree instead of the whole data set.
Second, an FP-tree-based pattern-fragment growth mining method is developed, which
starts from a frequent length-1 pattern (as an initial suffix pattern), examines only its
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 55
conditional-pattern base (a “sub-database” which consists of the set of frequent items co-
occurring with the suffix pattern), constructs its (conditional) FP-tree, and performs mining
recursively with such a tree. The pattern growth is achieved via concatenation of the suffix
pattern with the new ones generated from a conditional FP-tree. Since the frequent itemset
in any transaction is always encoded in the corresponding path of the frequent-pattern trees,
pattern growth ensures the completeness of the result. In this context, our method is not
Apriori-like restricted generation-and-test but restricted test only. The major operations of
mining are count accumulation and prefix path count adjustment, which are usually much
less costly than candidate generation and pattern matching operations performed in most
Apriori-like algorithms.
Third, the search technique employed in mining is a partitioning-based, divide-and-
conquer method rather than Apriori-like level-wise generation of the combinations of fre-
quent itemsets. This dramatically reduces the size of conditional-pattern base generated at
the subsequent level of search as well as the size of its corresponding conditional FP-tree.
Moreover, it transforms the problem of finding long frequent patterns to looking for shorter
ones and then concatenating the suffix. It employs the least frequent items as suffix, which
offers good selectivity. All these techniques contribute to substantial reduction of search
costs.
A performance study has been conducted to compare the performance of FP-growth with
two representative frequent-pattern mining methods, Apriori (Agrawal and Srikant, 1994)
and TreeProjection (Agarwal et al., 2001). Our study shows that FP-growth is about an
order of magnitude faster than Apriori, especially when the data set is dense (containing
many patterns) and/or when the frequent patterns are long; also, FP-growth outperforms
the TreeProjection algorithm. Moreover, our FP-tree-based mining method has been im-
plemented in the DBMiner system and tested in large transaction databases in industrial
applications.
Although FP-growth was first proposed briefly in Han et al. (2000), this paper makes
additional progress as follows.
– The properties of FP-tree are thoroughly studied. Also, we point out the fact that, although
it is often compact, FP-tree may not always be minimal.
– Some optimizations are proposed to speed up FP-growth, for example, in Section 3.2,
a technique to handle single path FP-tree has been further developed for performance
improvements.
– A database projection method has been developed in Section 4 to cope with the situation
when an FP-tree cannot be held in main memory—the case that may happen in a very
large database.
– Extensive experimental results have been reported. We examine the size of FP-tree as
well as the turning point of FP-growth on data projection to building FP-tree. We also
test the fully integrated FP-growth method on large datasets which cannot fit in main
memory.
The remainder of the paper is organized as follows. Section 2 introduces the FP-tree
structure and its construction method. Section 3 develops an FP-tree-based frequent-pattern
mining algorithm, FP-growth. Section 4 explores techniques for scaling FP-growth in large
56 HAN ET AL.
databases. Section 5 presents our performance study. Section 6 discusses the issues on
further improvements of the method. Section 7 summarizes our study and points out some
future research issues.
2. Frequent-pattern tree: Design and construction
Let I = {a1, a2, . . . , am} be a set of items, and a transaction database DB = 〈T1, T2, . . . ,
Tn〉, where Ti (i ∈ [1 . . . n]) is a transaction which contains a set of items in I . The support1
(or occurrence frequency) of a pattern A, where A is a set of items, is the number of
transactions containing A in DB. A pattern A is frequent if A’s support is no less than a
predefined minimum support threshold, ξ .
Given a transaction database DB and a minimum support threshold ξ , the problem of
finding the complete set of frequent patterns is called the frequent-pattern mining problem.
2.1. Frequent-pattern tree
To design a compact data structure for efficient frequent-pattern mining, let’s first examine
an example.
Example 1. Let the transaction database, DB, be the first two columns of Table 1, and the
minimum support threshold be 3 (i.e., ξ = 3).
A compact data structure can be designed based on the following observations:
1. Since only the frequent items will play a role in the frequent-pattern mining, it is necessary
to perform one scan of transaction database DB to identify the set of frequent items (with
frequency count obtained as a by-product).
2. If the set of frequent items of each transaction can be stored in some compact structure,
it may be possible to avoid repeatedly scanning the original transaction database.
3. If multiple transactions share a set of frequent items, it may be possible to merge the
shared sets with the number of occurrences registered as count. It is easy to check whether
two sets are identical if the frequent items in all of the transactions are listed according
to a fixed order.
Table 1. A transaction database as running example.
TID Items bought (Ordered) frequent items
100 f, a, c, d, g, i, m, p f, c, a, m, p
200 a, b, c, f, l, m, o f, c, a, b, m
300 b, f, h, j, o f, b
400 b, c, k, s, p c, b, p
500 a, f, c, e, l, p, m, n f, c, a, m, p
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 57
4. If two transactions share a common prefix, according to some sorted order of frequent
items, the shared parts can be merged using one prefix structure as long as the count is
registered properly. If the frequent items are sorted in their frequency descending order,
there are better chances that more prefix strings can be shared.
With the above observations, one may construct a frequent-pattern tree as follows.
First, a scan of DB derives a list of frequent items, 〈( f :4), (c:4), (a:3), (b:3), (m:3), (p:3)〉
(the number after “:” indicates the support), in which items are ordered in frequency-
descending order. This ordering is important since each path of a tree will follow this order.
For convenience of later discussions, the frequent items in each transaction are listed in this
ordering in the rightmost column of Table 1.
Second, the root of a tree is created and labeled with “null”. The FP-tree is constructed
as follows by scanning the transaction database DB the second time.
1. The scan of the first transaction leads to the construction of the first branch of the tree:
〈( f :1), (c:1), (a:1), (m:1), (p:1)〉. Notice that the frequent items in the transaction are
listed according to the order in the list of frequent items.
2. For the second transaction, since its (ordered) frequent item list 〈 f, c, a, b, m〉 shares a
common prefix 〈 f, c, a〉 with the existing path 〈 f, c, a, m, p〉, the count of each node
along the prefix is incremented by 1, and one new node (b:1) is created and linked as a
child of (a:2) and another new node (m:1) is created and linked as the child of (b:1).
3. For the third transaction, since its frequent item list 〈 f, b〉 shares only the node 〈 f 〉 with
the f -prefix subtree, f ’s count is incremented by 1, and a new node (b:1) is created and
linked as a child of ( f :3).
4. The scan of the fourth transaction leads to the construction of the second branch of the
tree, 〈(c:1), (b:1), (p:1)〉.
5. For the last transaction, since its frequent item list 〈 f, c, a, m, p〉 is identical to the first
one, the path is shared with the count of each node along the path incremented by 1.
To facilitate tree traversal, an item header table is built in which each item points to its
first occurrence in the tree via a node-link. Nodes with the same item-name are linked in
sequence via such node-links. After scanning all the transactions, the tree, together with the
associated node-links, are shown infigure 1.
Based on this example, a frequent-pattern tree can be designed as follows.
Definition 1 (FP-tree). A frequent-pattern tree (or FP-tree in short) is a tree structure
defined below.
1. It consists of one root labeled as “null”, a set of item-prefix subtrees as the children of
the root, and a frequent-item-header table.
2. Each node in the item-prefix subtree consists of three fields: item-name, count, and
node-link, where item-name registers which item this node represents, count registers
the number of transactions represented by the portion of the path reaching this node, and
58 HAN ET AL.
Figure 1. The FP-tree in Example 1.
node-link links to the next node in the FP-tree carrying the same item-name, or null if
there is none.
3. Each entry in the frequent-item-header table consists of two fields, (1) item-name and
(2) head of node-link (a pointer pointing to the first node in the FP-tree carrying the
item-name).
Based on this definition, we have the following FP-tree construction algorithm.
Algorithm 1 (FP-tree construction).
Input: A transaction database DB and a minimum support threshold ξ .
Output: FP-tree, the frequent-pattern tree of DB.
Method: The FP-tree is constructed as follows.
1. Scan the transaction database DB once. Collect F , the set of frequent items, and the
support of each frequent item. Sort F in support-descending order as FList, the list of
frequent items.
2. Create the root of an FP-tree, T , and label it as “null”. For each transaction Trans in DB
do the following.
Select the frequent items in Trans and sort them according to the order of FList. Let the
sorted frequent-item list in Trans be [p | P], where p is the first element and P is the
remaining list. Call insert tree([p | P], T ).
The function insert tree([p | P], T ) is performed as follows. If T has a child N such
that N.item-name = p.item-name, then increment N ’s count by 1; else create a new node
N , with its count initialized to 1, its parent link linked to T , and its node-link linked to
the nodes with the same item-name via the node-link structure. If P is nonempty, call
insert tree(P, N ) recursively.
Analysis. The FP-tree construction takes exactly two scans of the transaction database: The
first scan collects the set of frequent items, and the second scan constructs the FP-tree. The
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 59
cost of inserting a transaction Trans into the FP-tree is O(|freq(Trans)|), where freq(Trans)
is the set of frequent items in Trans. We will show that the FP-tree contains the complete
information for frequent-pattern mining.
2.2. Completeness and compactness of FP-tree
There are several important properties of FP-tree that can be derived from the FP-tree
construction process.
Given a transaction database DB and a support threshold ξ . Let F be the frequent items in
DB. For each transaction T , freq(T ) is the set of frequent items in T , i.e., freq(T ) = T ∩ F ,
and is called the frequent item projection of transaction T . According to the Apriori
principle, the set of frequent item projections of transactions in the database is sufficient
for mining the complete set of frequent patterns, because an infrequent item plays no role
in frequent patterns.
Lemma 2.1. Given a transaction database DB and a support threshold ξ, the complete
set of frequent item projections of transactions in the database can be derived from DB’s
FP-tree.
Rationale. Based on the FP-tree construction process, for each transaction in the DB, its
frequent item projection is mapped to one path in the FP-tree.
For a path a1a2 . . . ak from the root to a node in the FP-tree, let cak be the count at the
node labeled ak and c′ak be the sum of counts of children nodes of ak . Then, according to
the construction of the FP-tree, the path registers frequent item projections of cak − c′ak
transactions.
Therefore, the FP-tree registers the complete set of frequent item projections without
duplication.
Based on this lemma, after an FP-tree for DB is constructed, it contains the complete
information for mining frequent patterns from the transaction database. Thereafter, only the
FP-tree is needed in the remaining mining process, regardless of the number and length of
the frequent patterns.
Lemma 2.2. Given a transaction database DB and a support threshold ξ . Without con-
sidering the (null) root, the size of an FP-tree is bounded by
∑
T ∈DB |freq(T )|, and the
height of the tree is bounded by maxT ∈DB{|freq(T )|}, where freq(T ) is the frequent item
projection of transaction T .
Rationale. Based on the FP-tree construction process, for any transaction T in DB, there
exists a path in the FP-tree starting from the corresponding item prefix subtree so that the set
of nodes in the path is exactly the same set of frequent items in T . The root is the only extra
node that is not created by frequent-item insertion, and each node contains one node-link
and one count. Thus we have the bound of the size of the tree stated in the Lemma.
The height of any p-prefix subtree is the maximum number of frequent items in any
transaction with p appearing at the head of its frequent item list. Therefore, the height of
60 HAN ET AL.
the tree is bounded by the maximal number of frequent items in any transaction in the
database, if we do not consider the additional level added by the root.
Lemma 2.2 shows an important benefit of FP-tree: the size of an FP-tree is bounded by the
size of its corresponding database because each transaction will contribute at most one path
to the FP-tree, with the length equal to the number of frequent items in that transaction. Since
there are often a lot of sharings of frequent items among transactions, the size of the tree is
usually much smaller than its original database. Unlike the Apriori-like method which may
generate an exponential number of candidates in the worst case, under no circumstances,
may an FP-tree with an exponential number of nodes be generated.
FP-tree is a highly compact structure which stores the information for frequent-pattern
mining. Since a single path “a1 → a2 → · · · → an” in the a1-prefix subtree registers all
the transactions whose maximal frequent set is in the form of “a1 → a2 → · · · → ak” for
any 1 ≤ k ≤ n, the size of the FP-tree is substantially smaller than the size of the database
and that of the candidate sets generated in the association rule mining.
The items in the frequent item set are ordered in the support-descending order: More
frequently occurring items are more likely to be shared and thus they are arranged closer
to the top of the FP-tree. This ordering enhances the compactness of the FP-tree structure.
However, this does not mean that the tree so constructed always achieves the maximal com-
pactness. With the knowledge of particular data characteristics, it is sometimes possible
to achieve even better compression than the frequency-descending ordering. Consider the
following example. Let the set of transactions be: {adef , bdef , cdef , a, a, a, b, b, b, c, c, c},
and the minimum support threshold be 3. The frequent item set associated with sup-
port count becomes {a:4, b:4, c:4, d:3, e:3, f :3}. Following the item frequency ordering
a → b → c → d → e → f , the FP-tree constructed will contain 12 nodes, as shown in
figure 2(a). However, following another item ordering f → d → e → a → b → c, it will
contain only 9 nodes, as shown in figure 2(b).
The compactness of FP-tree is also verified by our experiments. Sometimes a rather small
FP-tree is resulted from a quite large database. For example, for the database Connect-4 used
in MaxMiner (Bayardo, 1998), which contains 67,557 transactions with 43 items in each
transaction, when the support threshold is 50% (which is used in the MaxMiner experiments
Figure 2. FP-tree constructed based on frequency descending ordering may not always be minimal.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 61
(Bayardo, 1998)), the total number of occurrences of frequent items is 2,219,609, whereas
the total number of nodes in the FP-tree is 13,449 which represents a reduction ratio of
165.04, while it still holds hundreds of thousands of frequent patterns! (Notice that for
databases with mostly short transactions, the reduction ratio is not that high.) Therefore,
it is not surprising some gigabyte transaction database containing many long patterns may
even generate an FP-tree that fits in main memory. Nevertheless, one cannot assume that
an FP-tree can always fit in main memory no matter how large a database is. Methods for
highly scalable FP-growth mining will be discussed in Section 5.
3. Mining frequent patterns using FP-tree
Construction of a compact FP-tree ensures that subsequent mining can be performed with
a rather compact data structure. However, this does not automatically guarantee that it will
be highly efficient since one may still encounter the combinatorial problem of candidate
generation if one simply uses this FP-tree to generate and check all the candidate patterns.
In this section, we study how to explore the compact information stored in an FP-tree,
develop the principles of frequent-pattern growth by examination of our running exam-
ple, explore how to perform further optimization when there exists a single prefix path in
an FP-tree, and propose a frequent-pattern growth algorithm, FP-growth, for mining the
complete set of frequent patterns using FP-tree.
3.1. Principles of frequent-pattern growth for FP-tree mining
In this subsection, we examine some interesting properties of the FP-tree structure which
will facilitate frequent-pattern mining.
Property 3.1 (Node-link property). For any frequent item ai , all the possible patterns
containing only frequent items and ai can be obtained by following ai ’s node-links, starting
from ai ’s head in the FP-tree header.
This property is directly from the FP-tree construction process, and it facilitates the access
of all the frequent-pattern information related to ai by traversing the FP-tree once following
ai ’s node-links.
To facilitate the understanding of other properties of FP-tree related to mining, we first
go through an example which performs mining on the constructed FP-tree (figure 1) in
Example 1.
Example 2. Let us examine the mining process based on the constructed FP-tree shown
in figure 1. Based on Property 3.1, all the patterns containing frequent items that a node ai
participates can be collected by starting at ai ’s node-link head and following its node-links.
We examine the mining process by starting from the bottom of the node-link header table.
For node p, its immediate frequent pattern is (p:3), and it has two paths in the FP-tree:
〈 f :4, c:3, a:3, m:2, p:2〉 and 〈c:1, b:1, p:1〉. The first path indicates that string
“( f, c, a, m, p)” appears twice in the database. Notice the path also indicates that string
62 HAN ET AL.
〈 f, c, a〉 appears three times and 〈 f 〉 itself appears even four times. However, they only
appear twice together with p. Thus, to study which string appear together with p, only p’s
prefix path 〈 f :2, c:2, a:2, m:2〉 (or simply, 〈 f cam:2〉) counts. Similarly, the second path
indicates string “(c, b, p)” appears once in the set of transactions in DB, or p’s prefix path
is 〈cb:1〉. These two prefix paths of p, “{( f cam:2), (cb:1)}”, form p’s subpattern-base,
which is called p’s conditional pattern base (i.e., the subpattern-base under the condition of
p’s existence). Construction of an FP-tree on this conditional pattern-base (which is called
p’s conditional FP-tree) leads to only one branch (c:3). Hence, only one frequent pattern
(cp:3) is derived. (Notice that a pattern is an itemset and is denoted by a string here.) The
search for frequent patterns associated with p terminates.
For node m, its immediate frequent pattern is (m:3), and it has two paths, 〈 f :4, c:3,
a:3, m:2〉 and 〈 f :4, c:3, a:3, b:1, m:1〉. Notice p appears together with m as well, however,
there is no need to include p here in the analysis since any frequent patterns involving p
has been analyzed in the previous examination of p. Similar to the above analysis, m’s
conditional pattern-base is {(fca:2), (fcab:1)}. Constructing an FP-tree on it, we derive m’s
conditional FP-tree, 〈 f :3, c:3, a:3〉, a single frequent pattern path, as shown in figure 3.
This conditional FP-tree is then mined recursively by calling mine(〈 f :3, c:3, a:3〉 | m).
Figure 3 shows that “mine(〈 f :3, c:3, a:3〉 | m)” involves mining three items (a), (c), ( f )
in sequence. The first derives a frequent pattern (am:3), a conditional pattern-base {(fc:3)},
and then a call “mine(〈 f :3, c:3〉 | am)”; the second derives a frequent pattern (cm:3), a
conditional pattern-base {( f :3)}, and then a call “mine(〈 f :3〉 | cm)”; and the third derives
only a frequent pattern (fm:3). Further recursive call of “mine(〈 f :3, c:3〉 | am)” derives two
patterns (cam:3) and (fam:3), and a conditional pattern-base {( f :3)}, which then leads
to a call “mine(〈 f :3〉 | cam)”, that derives the longest pattern (fcam:3). Similarly, the call
of “mine(〈 f :3〉 | cm)” derives one pattern (fcm:3). Therefore, the set of frequent patterns
involving m is {(m:3), (am:3), (cm:3), ( f m:3), (cam:3), (fam:3), (fcam:3), (fcm:3)}. This
indicates that a single path FP-tree can be mined by outputting all the combinations of the
items in the path.
Similarly, node b derives (b:3) and it has three paths: 〈 f :4, c:3, a:3, b:1〉, 〈 f :4, b:1〉, and
〈c:1, b:1〉. Since b’s conditional pattern-base {(fca:1), ( f :1), (c:1)} generates no frequent
item, the mining for b terminates. Node a derives one frequent pattern {(a:3)} and one
subpattern base {( f c:3)}, a single-path conditional FP-tree. Thus, its set of frequent patterns
Figure 3. Mining FP-tree | m, a conditional FP-tree for item m.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 63
Table 2. Mining frequent patterns by creating conditional (sub)pattern-bases.
Item Conditional pattern-base Conditional FP-tree
p {( f cam:2), (cb:1)} {(c:3)}|p
m {( f ca:2), (fcab:1)} {( f :3, c:3, a:3)}|m
b {( f ca:1), ( f :1), (c:1)} ∅
a {( f c:3)} {( f :3, c:3)}|a
c {( f :3)} {( f :3)}|c
f ∅ ∅
can be generated by taking their combinations. Concatenating them with (a:3), we have
{( f a:3), (ca:3), (fca:3)}. Node c derives (c:4) and one subpattern-base {( f :3)}, and the
set of frequent patterns associated with (c:3) is {(fc:3)}. Node f derives only ( f :4) but no
conditional pattern-base.
The conditional pattern-bases and the conditional FP-trees generated are summarized in
Table 2.
The correctness and completeness of the process in Example 2 should be justified.
This is accomplished by first introducing a few important properties related to the mining
process.
Property 3.2 (Prefix path property). To calculate the frequent patterns with suffix ai , only
the prefix subpathes of nodes labeled ai in the FP-tree need to be accumulated, and the
frequency count of every node in the prefix path should carry the same count as that in the
corresponding node ai in the path.
Rationale. Let the nodes along the path P be labeled as a1, . . . , an in such an order that
a1 is the root of the prefix subtree, an is the leaf of the subtree in P , and ai (1 ≤ i ≤ n) is
the node being referenced. Based on the process of FP-tree construction presented in Algo-
rithm 1, for each prefix node ak (1 ≤ k < i), the prefix subpath of the node ai in P occurs
together with ak exactly ai .count times. Thus every such prefix node should carry the same
count as node ai . Notice that a postfix node am (for i < m ≤ n) along the same path also
co-occurs with node ai . However, the patterns with am will be generated when examining
the suffix node am , enclosing them here will lead to redundant generation of the patterns that
would have been generated for am . Therefore, we only need to examine the prefix subpath
of ai in P .
For example, in Example 2, node m is involved in a path 〈 f :4, c:3, a:3, m:2, p:2〉, to
calculate the frequent patterns for node m in this path, only the prefix subpath of node m,
which is 〈 f :4, c:3, a:3〉, need to be extracted, and the frequency count of every node in the
prefix path should carry the same count as node m. That is, the node counts in the prefix
path should be adjusted to 〈 f :2, c:2, a:2〉.
64 HAN ET AL.
Based on this property, the prefix subpath of node ai in a path P can be copied and
transformed into a count-adjusted prefix subpath by adjusting the frequency count of every
node in the prefix subpath to the same as the count of node ai . The prefix path so transformed
is called the transformed prefix path of ai for path P .
Notice that the set of transformed prefix paths of ai forms a small database of patterns
which co-occur with ai . Such a database of patterns occurring with ai is called ai ’s con-
ditional pattern-base, and is denoted as “pattern base | ai ”. Then one can compute all
the frequent patterns associated with ai in this ai -conditional pattern-base by creating a
small FP-tree, called ai ’s conditional FP-tree and denoted as “FP-tree | ai ”. Subsequent
mining can be performed on this small conditional FP-tree. The processes of construction of
conditional pattern-bases and conditional FP-trees have been demonstrated in Example 2.
This process is performed recursively, and the frequent patterns can be obtained by a
pattern-growth method, based on the following lemmas and corollary.
Lemma 3.1 (Fragment growth). Let α be an itemset in DB, B be α’s conditional pattern-
base, and β be an itemset in B. Then the support of α ∪β in DB is equivalent to the support
of β in B.
Rationale. According to the definition of conditional pattern-base, each (sub)transaction
in B occurs under the condition of the occurrence of α in the original transaction database
DB. If an itemset β appears in B ψ times, it appears with α in DB ψ times as well. Moreover,
since all such items are collected in the conditional pattern-base of α, α ∪ β occurs exactly
ψ times in DB as well. Thus we have the lemma.
From this lemma, we can directly derive an important corollary.
Corollary 3.1 (Pattern growth). Let α be a frequent itemset in DB, B be α’s conditional
pattern-base, and β be an itemset in B. Then α ∪ β is frequent in DB if and only if β is
frequent in B.
Based on Corollary 3.1, mining can be performed by first identifying the set of frequent
1-itemsets in DB, and then for each such frequent 1-itemset, constructing its conditional
pattern-bases, and mining its set of frequent 1-itemsets in the conditional pattern-base, and
so on. This indicates that the process of mining frequent patterns can be viewed as first
mining frequent 1-itemset and then progressively growing each such itemset by mining
its conditional pattern-base, which can in turn be done similarly. By doing so, a frequent
k-itemset mining problem is successfully transformed into a sequence of k frequent 1-
itemset mining problems via a set of conditional pattern-bases. Since mining is done by
pattern growth, there is no need to generate any candidate sets in the entire mining process.
Notice also in the construction of a new FP-tree from a conditional pattern-base obtained
during the mining of an FP-tree, the items in the frequent itemset should be ordered in the
frequency descending order of node occurrence of each item instead of its support (which
represents item occurrence). This is because each node in an FP-tree may represent many
occurrences of an item but such a node represents a single unit (i.e., the itemset whose
elements always occur together) in the construction of an item-associated FP-tree.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 65
3.2. Frequent-pattern growth with single prefix path of FP-tree
The frequent-pattern growth method described above works for all kinds of FP-trees. How-
ever, further optimization can be explored on a special kind of FP-tree, called single prefix-
path FP-tree, and such an optimization is especially useful at mining long frequent patterns.
A single prefix-path FP-tree is an FP-tree that consists of only a single path or a single
prefix path stretching from the root to the first branching node of the tree, where a branching
node is a node containing more than one child.
Let us examine an example.
Example 3. Figure 4(a) is a single prefix-path FP-tree that consists of one prefix path,
〈(a:10) → (b:8) → (c:7)〉, stretching from the root of the tree to the first branching node (c:7).
Although it can be mined using the frequent-pattern growth method described above, a better
method is to split the tree into two fragments: the single prefix-path, 〈(a:10) → (b:8) →
(c:7)〉, as shown in figure 4(b), and the multipath part, with the root replaced by a pseudo-
root R, as shown in figure 4(c). These two parts can be mined separately and then combined
together.
Let us examine the two separate mining processes. All the frequent patterns associated
with the first part, the single prefix-path P = 〈(a:10) → (b:8) → (c:7)〉, can be mined by
enumeration of all the combinations of the subpaths of P with the support set to the minimum
support of the items contained in the subpath. This is because each such subpath is distinct
and occurs the same number of times as the minimum occurrence frequency among the
items in the subpath which is equal to the support of the last item in the subpath. Thus, path
P generates the following set of frequent patterns, freq pattern set(P) = {(a:10), (b:8),
(c:7), (ab:8), (ac:7), (bc:7), (abc:7)}.
Let Q be the second FP-tree (figure 4(c)), the multipath part rooted with R. Q can be
mined as follows.
First, R is treated as a null root, and Q forms a multipath FP-tree, which can be mined
using a typical frequent-pattern growth method. The mining result is: freq pattern set(Q)
= {(d:4), (e:3), ( f :3), (d f :3)}.
Figure 4. Mining an FP-tree with a single prefix path.
66 HAN ET AL.
Second, for each frequent itemset in Q, R can be viewed as a conditional frequent
pattern-base, and each itemset in Q with each pattern generated from R may form a dis-
tinct frequent pattern. For example, for (d:4) in freq pattern set(Q), P can be viewed as
its conditional pattern-base, and a pattern generated from P , such as (a:10), will generate
with it a new frequent itemset, (ad:4), since a appears together with d at most four times.
Thus, for (d:4) the set of frequent patterns generated will be (d:4) × freq pattern set(P) =
{(ad:4), (bd:4), (cd:4), (abd:4), (acd:4), (bcd:4), (abcd:4)}, where X × Y means that ev-
ery pattern in X is combined with every one in Y to form a “cross-product-like” larger
itemset with the support being the minimum support between the two patterns. Thus,
the complete set of frequent patterns generated by combining the results of P and Q
will be freq pattern set(Q) × freq pattern set(P), with the support being the support of
the itemset in Q (which is always no more than the support of the itemset
from P).
In summary, the set of frequent patterns generated from such a single prefix path consists
of three distinct sets: (1) freq pattern set(P), the set of frequent patterns generated from the
single prefix-path, P; (2) freq pattern set(Q), the set of frequent patterns generated from
the multipath part of the FP-tree, Q; and (3) freq pattern set(Q) × freq pattern set(P), the
set of frequent patterns involving both parts.
We first show if an FP-tree consists of a single path P , one can generate the set of frequent
patterns according to the following lemma.
Lemma 3.2 (Pattern generation for an FP-tree consisting of single path). Suppose an
FP-tree T consists of a single path P. The complete set of the frequent patterns of T can
be generated by enumeration of all the combinations of the subpaths of P with the support
being the minimum support of the items contained in the subpath.
Rationale. Let the single path P of the FP-tree be 〈a1:s1 → a2:s2 → · · · → ak :sk〉. Since
the FP-tree contains a single path P , the support frequency si of each item ai (for 1 ≤ i ≤ k)
is the frequency of ai co-occurring with its prefix string. Thus, any combination of the items
in the path, such as 〈ai , . . . , a j 〉 (for 1 ≤ i, j ≤ k), is a frequent pattern, with their co-
occurrence frequency being the minimum support among those items. Since every item in
each path P is unique, there is no redundant pattern to be generated with such a combi-
national generation. Moreover, no frequent patterns can be generated outside the FP-tree.
Therefore, we have the lemma.
We then show if an FP-tree consists of a single prefix-path, the set of frequent patterns
can be generated by splitting the tree into two according to the following lemma.
Lemma 3.3 (Pattern generation for an FP-tree consisting of single prefix path). Suppose
an FP-tree T, similar to the tree in figure 4(a), consists of (1) a single prefix path P, similar
to the tree P in figure 4(b), and (2) the multipath part, Q, which can be viewed as an
independent FP-tree with a pseudo-root R, similar to the tree Q in figure 4(c).
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 67
The complete set of the frequent patterns of T consists of the following three portions:
1. The set of frequent patterns generated from P by enumeration of all the combinations
of the items along path P, with the support being the minimum support among all the
items that the pattern contains.
2. The set of frequent patterns generated from Q by taking root R as “null.”
3. The set of frequent patterns combining P and Q formed by taken the cross-product
of the frequent patterns generated from P and Q, denoted as freq pattern set(P) ×
freq pattern set(Q), that is, each frequent itemset is the union of one frequent itemset
from P and one from Q and its support is the minimum one between the supports of the
two itemsets.
Rationale. Based on the FP-tree construction rules, each node ai in the single prefix path
of the FP-tree appears only once in the tree. The single prefix-path of the FP-tree forms a
new FP-tree P , and the multipath part forms another FP-tree Q. They do not share nodes
representing the same item. Thus, the two FP-trees can be mined separately.
First, we show that each pattern generated from one of the three portions by following
the pattern generation rules is distinct and frequent. According to Lemma 3.2, each pattern
generated from P , the FP-tree formed by the single prefix-path, is distinct and frequent.
The set of frequent patterns generated from Q by taking root R as “null” is also distinct
and frequent since such patterns exist without combining any items in their conditional
databases (which are in the items in P . The set of frequent patterns generated by combining
P and Q, that is, taking the cross-product of the frequent patterns generated from P and
Q, with the support being the minimum one between the supports of the two itemsets, is
also distinct and frequent. This is because each frequent pattern generated by P can be
considered as a frequent pattern in the conditional pattern-base of a frequent item in Q,
and whose support should be the minimum one between the two supports since this is the
frequency that both patterns appear together.
Second, we show that no patterns can be generated out of this three portions. Since
according to Lemma 3.1, the FP-tree T without being split into two FP-trees P and Q gen-
erates the complete set of frequent patterns by pattern growth. Since each pattern generated
from T will be generated from either the portion P or Q or their combination, the method
generates the complete set of frequent patterns.
3.3. The frequent-pattern growth algorithm
Based on the above lemmas and properties, we have the following algorithm for mining
frequent patterns using FP-tree.
Algorithm 2 (FP-growth: Mining frequent patterns with FP-tree by pattern fragment
growth).
Input: A database DB, represented by FP-tree constructed according to Algorithm 1, and
a minimum support threshold ξ .
Output: The complete set of frequent patterns.
68 HAN ET AL.
Method: call FP-growth(FP-tree, null).
Procedure FP-growth(Tree, α)
{
(1) if Tree contains a single prefix path // Mining single prefix-path FP-tree
(2) then {
(3) let P be the single prefix-path part of Tree;
(4) let Q be the multipath part with the top branching node replaced by a null root;
(5) for each combination (denoted as β) of the nodes in the path P do
(6) generate pattern β ∪ α with support = minimum support of nodes in β;
(7) let freq pattern set(P) be the set of patterns so generated; }
(8) else let Q be Tree;
(9) for each item ai in Q do { // Mining multipath FP-tree
(10) generate pattern β = ai ∪ α with support = ai .support;
(11) construct β’s conditional pattern-base and then β’s conditional FP-tree Treeβ ;
(12) if Treeβ = ∅
(13) then call FP-growth(Treeβ, β);
(14) let freq pattern set(Q) be the set of patterns so generated; }
(15) return(freq pattern set(P) ∪ freq pattern set(Q) ∪ (freq pattern set(P)
× freq pattern set(Q)))
}
Analysis. With the properties and lemmas in Sections 2 and 3, we show that the algorithm
correctly finds the complete set of frequent itemsets in transaction database DB.
As shown in Lemma 2.1, FP-tree of DB contains the complete information of DB in
relevance to frequent pattern mining under the support threshold ξ .
If an FP-tree contains a single prefix-path, according to Lemma 3.3, the generation of the
complete set of frequent patterns can be partitioned into three portions: the single prefix-path
portion P , the multipath portion Q, and their combinations. Hence we have lines (1)-(4) and
line (15) of the procedure. According to Lemma 3.2, the generated patterns for the single
prefix path are the enumerations of the subpaths of the prefix path, with the support being the
minimum support of the nodes in the subpath. Thus we have lines (5)-(7) of the procedure.
After that, one can treat the multipath portion or the FP-tree that does not contain the single
prefix-path as portion Q (lines (4) and (8)) and construct conditional pattern-base and mine
its conditional FP-tree for each frequent itemset ai . The correctness and completeness of
the prefix path transformation are shown in Property 3.2. Thus the conditional pattern-bases
store the complete information for frequent pattern mining for Q. According to Lemmas 3.1
and its corollary, the patterns successively grown from the conditional FP-trees are the set
of sound and complete frequent patterns. Especially, according to the fragment growth
property, the support of the combined fragments takes the support of the frequent itemsets
generated in the conditional pattern-base. Therefore, we have lines (9)-(14) of the procedure.
Line (15) sums up the complete result according to Lemma 3.3.
Let’s now examine the efficiency of the algorithm. The FP-growth mining process scans
the FP-tree of DB once and generates a small pattern-base Bai for each frequent item ai ,
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 69
each consisting of the set of transformed prefix paths of ai . Frequent pattern mining is then
recursively performed on the small pattern-base Bai by constructing a conditional FP-tree
for Bai . As reasoned in the analysis of Algorithm 1, an FP-tree is usually much smaller than
the size of DB. Similarly, since the conditional FP-tree, “FP-tree | ai ”, is constructed on the
pattern-base Bai , it should be usually much smaller and never bigger than Bai . Moreover, a
pattern-base Bai is usually much smaller than its original FP-tree, because it consists of the
transformed prefix paths related to only one of the frequent items, ai . Thus, each subsequent
mining process works on a set of usually much smaller pattern-bases and conditional FP-
trees. Moreover, the mining operations consist of mainly prefix count adjustment, counting
local frequent items, and pattern fragment concatenation. This is much less costly than
generation and test of a very large number of candidate patterns. Thus the algorithm is
efficient.
From the algorithm and its reasoning, one can see that the FP-growth mining process is
a divide-and-conquer process, and the scale of shrinking is usually quite dramatic. If the
shrinking factor is around 20-100 for constructing an FP-tree from a database, it is expected
to be another hundreds of times reduction for constructing each conditional FP-tree from
its already quite small conditional frequent pattern-base.
Notice that even in the case that a database may generate an exponential number of
frequent patterns, the size of the FP-tree is usually quite small and will never grow ex-
ponentially. For example, for a frequent pattern of length 100, “a1, . . . , a100”, the FP-tree
construction results in only one path of length 100 for it, possibly “〈a1, → · · · →a100〉” (if
the items are ordered in the list of frequent items as a1, . . . , a100). The FP-growth algorithm
will still generate about 1030 frequent patterns (if time permits!!), such as “a1, a2, . . ., a1a2,
. . ., a1a2a3, . . ., a1 . . . a100.” However, the FP-tree contains only one frequent pattern path of
100 nodes, and according to Lemma 3.2, there is even no need to construct any conditional
FP-tree in order to find all the patterns.
4. Scaling FP-tree-based FP-growth by database projection
FP-growth proposed in the last section is essentially a main memory-based frequent pat-
tern mining method. However, when the database is large, or when the minimum support
threshold is quite low, it is unrealistic to assume that the FP-tree of a database can fit in
main memory. A disk-based method should be worked out to ensure that mining is highly
scalable. In this section, a method is developed to first partition the database into a set of pro-
jected databases, and then for each projected database, construct and mine its corresponding
FP-tree.
Let us revisit the mining problem in Example 1.
Example 4. Suppose the FP-tree in figure 1 cannot be held in main memory. Instead of
constructing a global FP-tree, one can project the transaction database into a set of frequent
item-related projected databases as follows.
Starting at the tail of the frequent item list, p, the set of transactions that contain item
p can be collected into p-projected database. Infrequent items and item p itself can be
removed from them because the infrequent items are not useful in frequent pattern mining,
70 HAN ET AL.
Table 3. Projected databases and their FP-trees.
Item Projected database Conditional FP-tree
p {fcam, cb, fcam} {(c:3)}|p
m {fca, fcab, fca} {( f :3, c:3, a:3)}|m
b {fca, f, c} ∅
a {fc, fc, fc} {( f :3, c:3)}|a
c { f, f, f } {( f :3)}|c
f ∅ ∅
and item p is by default associated with each projected transaction. Thus, the p-projected
database becomes {fcam, cb, fcam}. This is very similar to the the p-conditional pattern-
base shown in Table 2 except fcam and fcam are expressed as (fcam:2) in Table 2. After
that, the p-conditional FP-tree can be built on the p-projected database based on the FP-tree
construction algorithm.
Similarly, the set of transactions containing item m can be projected into m-projected
database. Notice that besides infrequent items and item m, item p is also excluded from the
set of projected items because item p and its association with m have been considered in the
p-projected database. For the same reason, the b-projected database is formed by collecting
transactions containing item b, but infrequent items and items f , m and b are excluded. This
process continues for deriving a-projected database, c-projected database, and so on. The
complete set of item-projected databases derived from the transaction database are listed in
Table 3, together with their corresponding conditional FP-trees. One can easily see that the
two processes, construction of the global FP-tree and projection of the database into a set
of projected databases, derive identical conditional FP-trees.
As shown in Section 2, a conditional FP-tree is usually orders of magnitude smaller than
the global FP-tree. Thus, construction of a conditional FP-tree from each projected database
and then mining on it will dramatically reduce the size of FP-trees to be handled. What
about that a conditional FP-tree of a projected database still cannot fit in main memory?
One can further project the projected database, and the process can go on recursively until
the conditional FP-tree fits in main memory.
Let us define the concept of projected database formally.
Definition 2 (Projected database).
– Let ai be a frequent item in a transaction database, DB. The ai -projected database for ai
is derived from DB by collecting all the transactions containing ai and removing from
them (1) infrequent items, (2) all frequent items after ai in the list of frequent items, and
(3) ai itself.
– Let a j be a frequent item in α-projected database. Then the a jα-projected database is
derived from theα-projected database by collecting all entries containing a j and removing
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 71
from them (1) infrequent items, (2) all frequent items after a j in the list of frequent items,
and (3) a j itself.
According to the rules of construction of FP-tree and that of construction of projected
database, the ai -projected database is derived by projecting the same set of items in the
transactions containing ai into the projected database as those collected in the construction
of the ai -subtree in the FP-tree. Thus, the two methods derive the same sets of conditional
FP-trees.
There are two methods for database projection: parallel projection and partition projec-
tion.
Parallel projection is implemented as follows: Scan the database to be projected once,
where the database could be either a transaction database or an α-projected database. For
each transaction T in the database, for each frequent item ai in T , project T to the ai -
projected database based on the transaction projection rule, specified in the definition of
projected database. Since a transaction is projected in parallel to all the projected databases
in one scan, it is called parallel projection. The set of projected databases shown in Table 3
of Example 4 demonstrates the result of parallel projection. This process is illustrated in
figure 5(a).
Parallel projection facilitates parallel processing because all the projected databases are
available for mining at the end of the scan, and these projected databases can be mined
in parallel. However, since each transaction in the database is projected to multiple pro-
jected databases, if a database contains many long transactions with multiple frequent
items, the total size of the projected databases could be multiple times of the original one.
Let each transaction contains on average l frequent items. A transaction is then projected
to l − 1 projected database. The total size of the projected data from this transaction is
1 + 2 + · · · + (l − 1) = l(l−1)2 . This implies that the total size of the single item-projected
databases is about l−12 times of that of the original database.
To avoid such an overhead, we propose a partition projection method. Partition projection
is implemented as follows. When scanning the database (original or α-projected) to be
projected, a transaction T is projected to the ai -projected database only if ai is a frequent
item in T and there is no any other item after ai in the list of frequent items appearing
Figure 5. Parallel projection vs. partition projection.
72 HAN ET AL.
in the transaction. Since a transaction is projected to only one projected database at the
database scan, after the scan, the database is partitioned by projection into a set of projected
databases, and hence it is called partition projection.
The projected databases are mined in the reversed order of the list of frequent items. That
is, the projected database of the least frequent item is mined first, and so on. Each time when
a projected database is being processed, to ensure the remaining projected databases obtain
the complete information, each transaction in it is projected to the a j -projected database,
where a j is the item in the transaction such that there is no any other item after a j in the
list of frequent items appearing in the transaction. The partition projection process for the
database in Example 4 is illustrated in figure 5(b).
The advantage of partition projection is that the total size of the projected databases at
each level is smaller than the original database, and it usually takes less memory and I/Os to
complete the partition projection. However, the processing order of the projected databases
becomes important, and one has to process these projected databases in a sequential manner.
Also, during the processing of each projected database, one needs to project the processed
transactions to their corresponding projected databases, which may take some I/O as well.
Nevertheless, due to its low memory requirement, partition projection is still a promising
method in frequent pattern mining.
Example 5. Let us examine how the database in Example 4 can be projected by partition
projection.
First, by one scan of the transaction database, each transaction is projected to only one
projected database. The first transaction, facdgimp, is projected to the p-projected database
since p is the last frequent item in the list of frequent items. Thus, fcam (i.e., with infrequent
items removed) is inserted into the p-projected database. Similarly, transaction abcflmo is
projected to the m-projected database as fcab, bfhjo to the b-projected database as f ,
bcksp to the p-projected database as cb, and finally, afcelpmn to the p-projected database
as fcam. After this phrase, the entries in every projected databases are shown in Table 4.
With this projection, the original database can be replaced by the set of single-item
projected databases, and the total size of them is smaller than that of the original database.
Second, the p-projected database is first processed (i.e., construction of p-conditional
FP-tree), where p is the last item in the list of frequent items. During the processing of the
p-projected database, each transaction is projected to the corresponding projected database
Table 4. Single-item projected databases by partition projection.
Item Projected databases
p {fcam, cb, fcam}
m {fcab}
b { f }
a ∅
c ∅
f ∅
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 73
according to the same partition projection rule. For example, fcam is projected to the m-
projected database as fca, cb is projected to the b-projected database as c, and so on. The
process continues until every single-item projected database is completely processed.
5. Experimental evaluation and performance study
In this section, we present a performance comparison of FP-growth with the classical
frequent pattern mining algorithm Apriori, and an alternative database projection-based al-
gorithm, TreeProjection. We first give a concise introduction and analysis to TreeProjection,
and then report our experimental results.
5.1. A comparative analysis of FP-growth and TreeProjection methods
The TreeProjection algorithm proposed by Agarwal et al. (2001) constructs a lexicographical
tree and projects a large database into a set of reduced, item-based sub-databases based
on the frequent patterns mined so far. Since it applies a tree construction method and
performs mining recursively on progressively smaller databases, it shares some similarities
with FP-growth. However, the two methods have some fundamental differences in tree
construction and mining methodologies, and will lead to notable differences on efficiency
and scalability. We will explain such similarities and differences by working through the
following example.
Example 6. For the same transaction database presented in Example 1, we construct the
lexicographic tree according to the method described in Agarwal et al. (2001). The result
tree is shown in figure 6, and the construction process is presented as follows.
By scanning the transaction database once, all frequent 1-itemsets are identified. As
recommended in Agarwal et al. (2001), the frequency ascending order is chosen as the
Figure 6. A lexicographical tree built for the same transactional database DB.
74 HAN ET AL.
ordering of the items. So, the order is p-m-b-a-c-f , which is exactly the reverse order of
what is used in the FP-tree construction. The top level of the lexicographic tree is constructed,
i.e. the root and the nodes labeled by length-1 patterns. At this stage, the root node labeled
“null” and all the nodes which store frequent 1-itemsets are generated. All the transactions
in the database are projected to the root node, i.e., all the infrequent items are removed.
Each node in the lexicographical tree contains two pieces of information: (i) the pattern
that node represents, and (ii) the set of items that may generate longer patterns by adding
them to the pattern. The latter piece information is recorded as active extensions and active
items.
Then, a matrix at the root node is created, as shown below. The matrix computes the
frequencies of length-2 patterns, thus all pairs of frequent items are included in the matrix.
The items in pairs are arranged in the ordering. The matrix is built by adding counts from
every transaction, i.e., computing frequent 2-itemsets based on transactions stored in the
root node.
p m b a c f
p
m 2
b 1 1
a 2 3 1
c 3 3 2 3
f 2 3 2 3 3
At the same time of building the matrix, transactions in the root are projected to level-1
nodes as follows. Let t = a1a2 . . . an be a transaction with all items listed in ordering. t is
projected to node ai (1 ≤ i < n − 1) as t ′ai = ai+1ai+2 . . . an .
From the matrix, all the frequent 2-itemsets are found as: {pc, ma, mc, mf , ac, af , cf }.
The nodes in lexicographic tree for them are generated. At this stage, the only nodes
for 1-itemsets which are active are those for m and a, because only they contain enough
descendants to potentially generate longer frequent itemsets. All nodes up to and including
level-1 except for these two nodes are pruned.
In the same way, the lexicographic tree is grown level by level. From the matrix at node
m, nodes labeled mac, ma f, and mcf are added, and only ma is active in all the nodes for
frequent 2-itemsets. It is easy to see that the lexicographic tree in total contains 19 nodes.
The number of nodes in a lexicographic tree is exactly that of the frequent itemsets.
TreeProjection proposes an efficient way to enumerate frequent patterns. The efficiency of
TreeProjection can be explained by two main factors: (1) the transaction projection limits
the support counting in a relatively small space, and only related portions of transactions
are considered; and (2) the lexicographical tree facilitates the management and counting of
candidates and provides the flexibility of picking efficient strategy during the tree genera-
tion phase as well as transaction projection phase. Agarwal et al. (2001) reports that their
algorithm is up to one order of magnitude faster than other recent techniques in literature.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 75
However, in comparison with the FP-growth method, TreeProjection suffers from some
problems related to efficiency, and scalability. We analyze them as follows.
First, TreeProjection may encounter difficulties at computing matrices when the database
is huge, when there are a lot of transactions containing many frequent items, and/or when the
support threshold is very low. This is because in such cases there often exist a large number
of frequent items. The size of the matrices at high level nodes in the lexicographical tree can
be huge, as shown in our introduction section. The study in TreeProjection (Agarwal et al.,
2001) has developed some smart memory caching methods to overcome this problem.
However, it could be wise not to generate such huge matrices at all instead of finding
some smart caching techniques to reduce the cost. Moreover, even if the matrix can be
cached efficiently, its computation still involves some nontrivial overhead. To compute a
matrix at node P with n projected transactions, the cost is O(
∑n
i=1
|Ti |2
2 ), where |Ti | is
the length of the transaction. If the number of transaction is large and the length of each
transaction is long, the computation is costly. The FP-growth method will never need to
build up matrices and compute 2-itemset frequency since it avoids the generation of any
candidate k-itemsets for any k by applying a pattern growth method. Pattern growth can be
viewed as successive computation of frequent 1-itemset (of the database and conditional
pattern bases) and assembling them into longer patterns. Since computing frequent 1-
itemsets is much less expensive than computing frequent 2-itemsets, the cost is substantially
reduced.
Second, since one transaction may contain many frequent itemsets, one transaction in
TreeProjection may be projected many times to many different nodes in the lexicographical
tree. When there are many long transactions containing numerous frequent items, transaction
projection becomes a nontrivial cost of TreeProjection. The FP-growth method constructs
FP-tree which is a highly compact form of transaction database. Thus both the size and the
cost of computation of conditional pattern bases, which corresponds roughly to the compact
form of projected transaction databases, are substantially reduced.
Third, TreeProjection creates one node in its lexicographical tree for each frequent item-
set. At the first glance, this seems to be highly compact since FP-tree does not ensure that
each frequent node will be mapped to only one node in the tree. However, each branch of the
FP-tree may store many “hidden” frequent patterns due to the potential generation of many
combinations using its prefix paths. Notice that the total number of frequent k-itemsets can
be very large in a large database or when the database has quite long frequent itemsets.
For example, for a frequent itemset (a1, a2, . . . , a100), the number of frequent itemsets at
the 50th-level of the lexicographic tree will be ( 10050 ) = 100!50!×50! ≈ 1.0 × 1029. For the same
frequent itemset, FP-tree and FP-growth will only need one path of 100 nodes.
In summary, FP-growth mines frequent itemsets by (1) constructing highly compact
FP-trees which share numerous “projected” transactions and hide (or carry) numerous
frequent patterns, and (2) applying progressive pattern growth of frequent 1-itemsets which
avoids the generation of any potential combinations of candidate itemsets implicitly or
explicitly, whereas TreeProjection must generate candidate 2-itemsets for each projected
database. Therefore, FP-growth is more efficient and more scalable than TreeProjection,
especially when the number of frequent itemsets becomes really large. These observations
and analyses are well supported by our experiments reported in this section.
76 HAN ET AL.
5.2. Environments of experiments
All the experiments are performed on a 266-MHz Pentium PC machine with 128 megabytes
main memory, running on Microsoft Windows/NT. All the programs are written in Mi-
crosoft/Visual C++6.0. Notice that we do not directly compare our absolute number of
runtime with those in some published reports running on the RISC workstations because
different machine architectures may differ greatly on the absolute runtime for the same
algorithms. Instead, we implement their algorithms to the best of our knowledge based on
the published reports on the same machine and compare in the same running environment.
Please also note that run time used here means the total execution time, that is, the pe-
riod between input and output, instead of CPU time measured in the experiments in some
literature. We feel that run time is a more comprehensive measure since it takes the total
running time consumed as the measure of cost, whereas CPU time considers only the cost
of the CPU resource. Also, all reports on the runtime of FP-growth include the time of
constructing FP-trees from the original databases.
The experiments are pursued on both synthetic and real data sets. The synthetic data
sets which we used for our experiments were generated using the procedure described in
Agrawal and Srikant (1994). We refer readers to it for more details on the generation of
data sets.
We report experimental results on two synthetic data sets. The first one is T10.I4.D100K
with 1K items. In this data set, the average transaction size and average maximal potentially
frequent itemset size are set to 10 and 4, respectively, while the number of transactions in
the dataset is set to 100 K. It is a sparse dataset. The frequent itemsets are short and not
numerous.
The second synthetic data set we used is T25.I20.D100K with 10 K items. The average
transaction size and average maximal potentially frequent itemset size are set to 25 and 20,
respectively. There exist exponentially numerous frequent itemsets in this data set when
the support threshold goes down. There are also pretty long frequent itemsets as well as
a large number of short frequent itemsets in it. It contains abundant mixtures of short and
long frequent itemsets.
To test the capability of FP-growth on dense datasets with long patterns, we use the
real data set Connect-4, compiled from the Connect-4 game state information. The data set
is from the UC-Irvine Machine Learning Database Repository (http://www.ics.uci.edu/∼
mlearn/MLRepository.html). It contains 67, 557 transactions, while each transaction is with
43 items. It is a dense dataset with a lot of long frequent itemsets.
5.3. Compactness of FP-tree
To test the compactness of FP-trees, we compare the sizes of the following structures.
– Alphabetical FP-tree. It includes the space of all the links. However, in such an FP-tree,
the alphabetical order of items are used instead of frequency descending order.
– Ordered FP-tree. Again, the size covers that of all links. In such an FP-tree, the items are
sorted according to frequency descending order.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 77
Figure 7. Compactness of FP-tree over data set Connect-4.
– Transaction database. Each item in a transaction is stored as an integer. It is simply the
sum of occurrences of items in transactions.
– Frequent transaction database. That is the sub-database extracted from the original one
by removing all infrequent items.
In real dataset Connect-4, FP-tree achieves good compactness. As seen from the result
shown in figure 7, the size of ordered FP-tree is always smaller than the size of the transaction
database and the frequent transaction database. In a dense database, the size of the database
and that of its frequent database are close. The size of the alphabetical FP-tree is smaller than
that of the two databases in most cases but is slightly larger (about 1.5 to 2.5 times larger)
than the size of the ordered FP-tree. It indicates that the frequency-descending ordering of
the items benefits data compression in this case.
In dataset T25.I20.D100k, which contains abundant mixture of long and short frequent
patterns, FP-tree is compact most of the time. The result is shown in figure 8. Only when
Figure 8. Compactness of FP-tree over data set T25.I20.D100k.
78 HAN ET AL.
Figure 9. Compactness of FP-tree over data set T10.I4.D100k.
the support threshold lower than 2.5%, is the size of FP-tree larger than that of frequent
database. Moreover, as long as the support threshold is over 1.5%, the FP-tree is smaller
than the transaction database. The difference of sizes of ordered FP-tree and alphabetical
FP-tree is quite small in this dataset. It is about 2%.
In sparse dataset T10.I4.D100k, FP-tree achieves good compactness when the support
threshold is over 3.5%. Again, the difference of ordered FP-tree and alphabetical FP-tree is
trivial. The result is shown in figure 9.
The above experiments lead to the following conclusions.
– FP-tree achieves good compactness most of the time. Especially in dense datasets, it
can compress the database many times. Clearly, there is some overhead for pointers and
counters. However, the gain of sharing among frequent projections of transactions is
substantially more than the overhead and thus makes FP-tree space more efficient in
many cases.
– When support is very low, FP-tree becomes bushy. In such cases, the degree of sharing
in branches of FP-tree becomes low. The overhead of links makes the size of FP-tree
large. Therefore, instead of building FP-tree, we should construct projected databases.
That is the reason why we build FP-tree for transaction database/projected database only
when it passes certain density threshold. From the experiments, one can see that such a
threshold is pretty low, and easy to touch. Therefore, even for very large and/or sparse
database, after one or a few rounds of database projection, FP-tree can be used for all the
remaining mining tasks.
In the following experiments, we employed an implementation of FP-growth that inte-
grates both database projection and FP-tree mining. The density threshold is set to 3%, and
items are listed in frequency descending order.
5.4. Scalability study
The runtime of Apriori, TreeProjection, and FP-growth on synthetic data set T10.I4.D100K
as the support threshold decreases from 0.15% to 0.01% is shown in figure 10.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 79
Figure 10. Scalability with threshold over sparse data set.
FP-growth is faster than both Apriori and TreeProjection. TreeProjection is faster and
more scalable than Apriori. Since the dataset is sparse, as the support threshold is high,
the frequent itemsets are short and the set of such itemsets is not large, the advantages of
FP-growth and TreeProjection over Apriori are not so impressive. However, as the support
threshold goes down, the gap becomes wider. FP-growth can finish the computation for
support threshold 0.01% within the time for Apriori over 0.05%. TreeProjection is also
scalable, but is slower than FP-growth.
The advantages of FP-growth over Apriori becomes obvious when the dataset contains
an abundant number of mixtures of short and long frequent patterns. Figure 11 shows the
experimental results of scalability with threshold over dataset T25.I20.D100k. FP-growth
can mine with support threshold as low as 0.05%, with which Apriori cannot work out
within reasonable time. TreeProjection is also scalable and faster than Apriori, but is slower
than FP-growth.
Figure 11. Scalability with threshold over dataset with abundant mixtures of short and long frequent patterns.
80 HAN ET AL.
Figure 12. Scalability with threshold over Connect-4.
The advantage of FP-growth is dramatic in datasets with long patterns, which is challeng-
ing to the algorithms that mine the complete set of frequent patterns. The result on mining
the real dataset Connect-4 is shown in figure 12. To the best of our knowledge, this is the
first algorithm that handles such dense real dataset in performance study. From the figure,
one can see that FP-growth is scalable even when there are many long patterns. Without
candidate generation, FP-growth enumerates long patterns efficiently. In such datasets, nei-
ther Apriori nor TreeProjection are comparable to the performance of FP-growth. To deal
with long patterns, Apriori has to generate a tremendous number of candidates, that is very
costly. The main costs in TreeProjection are matrix computation and transaction projection.
In a database with a large number of frequent items, the matrices become quite large, and
the computation cost jumps up substantially. In contrast, the height of FP-tree is limited by
the maximal length of the transactions, and many transactions share the prefix paths of an
FP-tree. This explains why FP-growth has distinct advantages when the support threshold
is low and when the number of transactions is large.
To test the scalability of FP-growth against the number of transactions, a set of synthetic
datasets are generated using the same parameters of T10.I4 and T25.I20, and the number
of transactions ranges from 100 k to 1 M. FP-growth is tested over them using the same
support threshold in percentage. The result is in figure 13, which shows the linear increase
of runtime with the number of transactions. Please note that unlike the way reported in some
literature, we do not replicate transactions in real data sets to test the scalability. This is
because no matter how many times the transactions are replicated, FP-growth builds up an
FP-tree with the size identical to that of the original (nonreplicated) one, and the scaling-up
of such databases becomes trivial.
6. Discussions
The frequent-pattern growth method introduced here represents an interesting approach for
scalable frequent-pattern mining. In this section, we discuss some additional issues related
to its implementation, usage, and extension.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 81
Figure 13. Scalability of FP-growth with number of transactions.
6.1. Materialization and maintenance of FP-trees
Although we have studied the dynamic generation of FP-trees, it is possible to materialize
and incrementally update an FP-tree. We examine the related issues here.
6.1.1. Construction of disk-resident FP-trees. When the database grows very large, it
is unrealistic to construct a main memory-based FP-tree. Database projection has been
introduced in Section 3.4 as an effective approach. An interesting alternative is to construct
a disk-resident FP-tree.
The B+-tree structure, popularly used in relational database systems, can be used to index
FP-tree as well. Since there are many operations localized to single paths or individual
item prefix sub-trees, such as pattern matching for node insertion, creation of transformed
prefix paths for each node ai , etc., it is important to cluster FP-tree nodes according to the
tree/subtree structure. That is, one should (1) store each item prefix sub-tree on the same
page, if possible, or at least on a sequence of continuous pages on disk; (2) store each
subtree on the same page, and put the shared prefix path as the header information of the
page; and (3) cluster the node-links belonging to the same paged nodes together, etc. This
also facilitates a breadth-first search fashion for mining all the patterns starting from all the
nodes in the header in parallel.
To reduce the I/O costs by following node-links, mining should be performed in a group
accessing mode, that is, when accessing nodes following node-links, one should exhaust
the node traversal tasks in main memory before fetching the nodes on disks.
Notice that one may also construct node-link-free FP-trees. In this case, when traversing
a tree path, one should project the prefix subpaths of all the nodes into the corresponding
conditional pattern bases. This is feasible if both FP-tree and one page of each of its one-
level conditional pattern bases can fit in memory. Otherwise, additional I/Os will be needed
to swap in and out the conditional pattern bases.
6.1.2. Materialization of an FP-tree for frequent-pattern mining. Although an FP-tree
is rather compact, its construction needs two scans of a transaction database, which may
82 HAN ET AL.
represent a nontrivial overhead. It could be beneficial to materialize an FP-tree for regular
frequent pattern mining.
One difficulty for FP-tree materialization is how to select a good minimum support thresh-
old ξ in materialization since ξ is usually query-dependent. To overcome this difficulty, one
may use a low ξ that may usually satisfy most of the mining queries in the FP-tree con-
struction. For example, if we notice that 98% queries have ξ ≥ 20, we may choose ξ = 20
as the FP-tree materialization threshold: that is, only 2% of queries may need to construct a
new FP-tree. Since an FP-tree is organized in the way that less frequently occurring items
are located at the deeper paths of the tree, it is easy to select only the upper portions of the
FP-tree (or drop the low portions which do not satisfy the support threshold) when mining
the queries with higher thresholds. Actually, one can directly work on the materialized
FP-tree by starting at an appropriate header entry since one just need to get the prefix paths
no matter how low support the original FP-tree is.
6.1.3. Incremental update of an FP-tree. Another issue related to FP-tree materialization
is how to incrementally update an FP-tree, such as when adding daily new transactions into
a database containing records accumulated for months.
If the materialized FP-tree takes 1 as its minimum support (i.e., it is just a compact version
of the original database), the update will not cause any problem since adding new records is
equivalent to scanning additional transactions in the FP-tree construction. However, a full
FP-tree may be an undesirably large. Thus setting 1 as its minimum support may not be a
good solution.
In the general case, we can register the occurrence frequency of every items in F1 and
track them in updates. This is not too costly but it benefits the incremental updates of an
FP-tree as follows. Suppose an FP-tree was constructed based on a validity support threshold
(called “watermark”) ψ = 0.1% in a DB with 108 transactions. Suppose an additional 106
transactions are added in. The frequency of each item is updated. If the highest relative
frequency among the originally infrequent items (i.e., not in the FP-tree) goes up to, say
12%, the watermark will need to go up accordingly to ψ > 0.12% to exclude such item(s).
However, with more transactions added in, the watermark may even drop since an item’s
relative support frequency may drop with more transactions added in. Only when the FP-tree
watermark is raised to some undesirable level, the reconstruction of the FP-tree for the new
DB becomes necessary.
6.2. Extensions of frequent-pattern growth method in data mining
The philosophy of database compression and partition-based frequent-pattern mining can
be extended to constraint-based mining and mining other kinds of frequent patterns, such
as max-patterns, sequential patterns.
6.2.1. FP-tree mining with constraints. Constraint-based frequent-pattern mining repre-
sents an important direction towards user-controlled data mining. Constraint-based asso-
ciation mining using the Apriori-like mining methodology has been studied extensively
(Srikant et al., 1997; Ng et al., 1998). With the introduction of FP-growth method, one
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 83
may wonder whether constraint-based mining may benefit with FP-tree-like structures. A
thorough study of this issue, such as classification of various kinds of constraints and devel-
opment of methods of FP-tree-based mining with sophisticated constraints, such as those
in Ng et al. (1998), should be the task of another research paper.2 Here we only show
how to apply FP-tree structure to mining frequent patterns by incorporation of constraints
associated with a set of items.
Suppose one may just like to derive frequent patterns only associated with a particular
set of items, S, such as mining the set of frequent patterns containing c or m in Example 1.
Instead of mining frequent patterns for all the frequent items, one may explore the FP-tree-
based mining as follows. With the same FP-tree, the FP-growth mining method may just
need to be modified minorly. The only additional care is when computing a transformed
prefix path for an item m, one also needs to look down the path to include the items, such
as p, which are not in S. Our previous computation for the whole database will not need
to consider m’s pairing with p since it would have been checked when examining node p.
However, since p is not in S now, such a pair would have been missed if m’s computation
did not look down the path to include p.
6.2.2. FP-tree mining of other frequent patterns. FP-tree-based mining method can be
extended to mining many other kinds of interesting frequent patterns. We examine a few
such examples.
The first example is on mining frequent closed itemsets. Since frequent pattern mining
often generates a very large number of frequent itemsets, it hinders the effectiveness of
mining since users have to sift through a large number of mined rules to find useful ones.
An interesting alternative method proposed recently by Pasquier et al. (1999) is to mine
frequent closed itemsets, where an itemset α is a closed itemset if there exists no proper
superset of α that has the same support as α in the database. Mining frequent closed itemsets
has the same power as mining the complete set of frequent itemsets, but it may substantially
reduce redundant rules to be generated and increase the effectiveness of mining. A study at
mining closed items using an Apriori-like philosophy but adopting a vertical data format,
i.e., viewing database as “(item id: a set of transactions)” instead of “(transaction id: a set
of items),” has been studied in Zaki and Hsiao (2002). The FP-tree-based frequent-pattern
growth method can be extended and further optimized for mining such closed itemsets,
which has been reported in our subsequent study, as a new closed pattern mining algorithm,
called CLOSET (Pei et al., 2000).
The second example is on mining sequential patterns. A sequential patterns is a frequent
pattern in an event sequence database where a sequence is a set of events happening at
different times. Most of the previously developed sequential pattern mining methods, such
as Agrawal and Srikant (1995), Srikant and Agrawal (1996) and Mannila et al. (1997), follow
the methodology of Apriori since the Apriori-based method may substantially reduce the
number of combinations to be examined. However, Apriori still encounters problems when a
sequence database is large and/or when sequential patterns to be mined are numerous and/or
long. Our frequent-pattern growth method can be extended to mining sequential patterns
using the ideas of projection of sequence database and growth of subsequence fragments
to confine search space. An efficient sequential pattern method, called PrefixSpan (Pei
84 HAN ET AL.
et al., 2001), has been developed in this direction and our performance study has shown a
substantial performance improvement over the Apriori-based GSP algorithm (Srikant and
Agrawal, 1996).
7. Conclusions
We have proposed a novel data structure, frequent pattern tree (FP-tree), for storing com-
pressed, crucial information about frequent patterns, and developed a pattern growth method,
FP-growth, for efficient mining of frequent patterns in large databases.
There are several advantages of FP-growth over other approaches: (1) It constructs a
highly compact FP-tree, which is usually substantially smaller than the original database
and thus saves the costly database scans in the subsequent mining processes. (2) It applies
a pattern growth method which avoids costly candidate generation and test by successively
concatenating frequent 1-itemset found in the (conditional) FP-trees. This ensures that it
never generates any combinations of new candidate sets which are not in the database
because the itemset in any transaction is always encoded in the corresponding path of
the FP-trees. In this context, mining is not Apriori-like (restricted) generation-and-test but
frequent pattern (fragment) growth only. The major operations of mining are count accumu-
lation and prefix path count adjustment, which are usually much less costly than candidate
generation and pattern matching operations performed in most Apriori-like algorithms.
(3) It applies a partitioning-based divide-and-conquer method which dramatically reduces
the size of the subsequent conditional pattern bases and conditional FP-tree. Several other
optimization techniques, including direct pattern generation for single tree-path and em-
ploying the least frequent events as suffix, also contribute to the efficiency of the method.
We have implemented the FP-growth method, studied its performance in comparison with
several influential frequent pattern mining algorithms in large databases. Our performance
study shows that the method mines both short and long patterns efficiently in large databases,
outperforming the current candidate pattern generation-based algorithms. The FP-growth
method has also been implemented in the DBMiner system and been tested in large industrial
databases, such as a retail chain database, with satisfactory performance.
Since our first publication of FP-growth method for mining frequent patterns without
candidate generation (Han et al., 2000), there have been many subsequent studies on im-
provements of performance of frequent patterns based on the pattern-growth philosophy, as
well as extension of the scope of the method to cover other kinds of pattern mining tasks.
The pattern-growth framework has been extended towards (1) mining closed itemsets as
proposed in the CLOSET algorithm (Pei et al., 2000), (2) mining sequential patterns as pro-
posed in the PrefixSpan algorithm (Pei et al., 2001), and (3) pushing tough constraints deep
into frequent pattern mining processes (Pei et al., 2001). Moreover, a notable effort is the
proposal of the H-mine algorithm (Pei et al., 2001) for mining frequent patterns efficiently
in sparse data sets. FP-growth, though efficient at mining dense data sets, may incur un-
necessary overhead due to its recursive construction of FP-trees. Following the philosophy
of frequent pattern growth, but not constructing FP-trees, the H-mine algorithm constructs
another data structure, called H-struct, and mines directly on the H-struct without recursive
generation of numerous conditional FP-trees. The experiments reported in Pei et al. (2001)
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 85
shows that H-mine outperforms FP-growth when database is sparse. A suggested approach
is to integrate the two algorithms and dynamically select the FP-tree-based and H-struct-
based algorithms based on the characteristics of current data distribution. Recently, some
studies also show that various FP-tree mining strategies (such as bottom-up vs. top-down
methods) may lead to different efficiency over data sets of different data distributions.
There are still many interesting research issues related to the extensions of pattern-growth
approach, such as mining structured patterns by further development of the frequent pattern-
growth approach, mining approximate or fault-tolerant patterns in noisy environments,
frequent-pattern-based clustering and classification, and so on.
Acknowledgments
We would like to express our thanks to anonymous reviewers of our conference and journal
submissions on this theme. Their constructive comments have improved the quality of this
work.
Notes
1. Notice that support is defined here as absolute occurrence frequency, not the relative one as in some literature.
2. One such study has been performed by us in Pei et al. (2001).
References
Agarwal, R., Aggarwal, C., and Prasad, V.V.V. 2001. A tree projection algorithm for generation of frequent
itemsets. Journal of Parallel and Distributed Computing, 61:350–371,
Agrawal, R., Imielinski, T., and Swami, A. 1993. Mining association rules between sets of items in large databases.
In Proc. 1993 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD’93), Washington, DC, pp. 207–216.
Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., and Verkamo, A.I. 1996. Fast discovery of association rules.
In Advances in Knowledge Discovery and Data Mining, U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.
Uthurusamy (Eds.), AAAI/MIT Press, pp. 307–328.
Agrawal, R. and Srikant, R. 1994. Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. Very
Large Data Bases (VLDB’94), Santiago, Chile, pp. 487–499.
Agrawal, R. and Srikant, R. 1995. Mining sequential patterns. In Proc. 1995 Int. Conf. Data Engineering (ICDE’95),
Taipei, Taiwan, pp. 3–14.
Bayardo, R.J. 1998. Efficiently mining long patterns from databases. In Proc. 1998 ACM-SIGMOD Int. Conf.
Management of Data (SIGMOD’98), Seattle, WA, pp. 85–93.
Brin, S., Motwani, R., and Silverstein, C. 1997. Beyond market basket: Generalizing association rules to cor-
relations. In Proc. 1997 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD’97), Tucson, Arizona,
pp. 265–276.
Dong, G. and Li, J. 1999. Efficient mining of emerging patterns: Discovering trends and differences. In Proc. 1999
Int. Conf. Knowledge Discovery and Data Mining (KDD’99), San Diego, CA, pp. 43–52.
Grahne, G., Lakshmanan, L., and Wang, X. 2000. Efficient mining of constrained correlated sets. In Proc. 2000
Int. Conf. Data Engineering (ICDE’00), San Diego, CA, pp. 512–521.
Han, J., Dong, G., and Yin, Y. 1999. Efficient mining of partial periodic patterns in time series database. In Proc.
1999 Int. Conf. Data Engineering (ICDE’99), Sydney, Australia, pp. 106–115.
Han, J., Pei, J., and Yin, Y. 2000. Mining frequent patterns without candidate generation. In Proc. 2000 ACM-
SIGMOD Int. Conf. Management of Data (SIGMOD’00), Dallas, TX, pp. 1–12.
86 HAN ET AL.
Kamber, M., Han, J., and Chiang, J.Y. 1997. Metarule-guided mining of multi-dimensional association rules using
data cubes. In Proc. 1997 Int. Conf. Knowledge Discovery and Data Mining (KDD’97), Newport Beach, CA,
pp. 207–210.
Lent, B., Swami, A., and Widom, J. 1997. Clustering association rules. In Proc. 1997 Int. Conf. Data Engineering
(ICDE’97), Birmingham, England, pp. 220–231.
Mannila, H., Toivonen, H., and Verkamo, A.I. 1994. Efficient algorithms for discovering association rules. In Proc.
AAAI’94 Workshop Knowledge Discovery in Databases (KDD’94), Seattle, WA, pp. 181–192.
Mannila, H., Toivonen, H., and Verkamo, A.I. 1997. Discovery of frequent episodes in event sequences. Data
Mining and Knowledge Discovery, 1:259–289.
Ng, R., Lakshmanan, L.V.S., Han, J., and Pang, A. 1998. Exploratory mining and pruning optimizations of
constrained associations rules. In Proc. 1998 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD’98),
Seattle, WA, pp. 13–24.
Pasquier, N., Bastide, Y., Taouil, R., and Lakhal, L. 1999. Discovering frequent closed itemsets for association
rules. In Proc. 7th Int. Conf. Database Theory (ICDT’99), Jerusalem, Israel, pp. 398–416.
Park, J.S., Chen, M.S., and Yu, P.S. 1995. An effective hash-based algorithm for mining association rules. In Proc.
1995 ACM-SIGMOD Int. Conf. Management of Data (SIGMOD’95), San Jose, CA, pp. 175–186.
Pei, J., Han, J., and Lakshmanan, L.V.S. 2001. Mining frequent itemsets with convertible constraints. In Proc.
2001 Int. Conf. Data Engineering (ICDE’01), Heidelberg, Germany, pp. 433–332.
Pei, J., Han, J., Lu, H., Nishio, S., Tang, S., and Yang, D. 2001. H-Mine: Hyper-structure mining of frequent
patterns in large databases. In Proc. 2001 Int. Conf. Data Mining (ICDM’01), San Jose, CA, pp. 441–448.
Pei, J., Han, J., and Mao, R. 2000. CLOSET: An efficient algorithm for mining frequent closed itemsets. In
Proc. 2000 ACM-SIGMOD Int. Workshop Data Mining and Knowledge Discovery (DMKD’00), Dallas, TX,
pp. 11–20.
Pei, J., Han, J., Mortazavi-Asl, B., Pinto, H., Chen, Q., Dayal, U., and Hsu, M.-C. 2001. PrefixSpan: Mining
sequential patterns efficiently by prefix-projected pattern growth. In Proc. 2001 Int. Conf. Data Engineering
(ICDE’01), Heidelberg, Germany, pp. 215–224.
Srikant, R. and Agrawal, R. 1996. Mining sequential patterns: Generalizations and performance improvements.
In Proc. 5th Int. Conf. Extending Database Technology (EDBT’96), Avignon, France, pp. 3–17.
Silverstein, C., Brin, S., Motwani, R., and Ullman, J. 1998. Scalable techniques for mining causal structures. In
Proc. 1998 Int. Conf. Very Large Data Bases (VLDB’98), New York, NY, pp. 594–605.
Savasere, A., Omiecinski, E., and Navathe, S. 1995. An efficient algorithm for mining association rules in large
databases. In Proc. 1995 Int. Conf. Very Large Data Bases (VLDB’95), Zurich, Switzerland, pp. 432–443.
Sarawagi, S., Thomas, S., and Agrawal, R. 1998. Integrating association rule mining with relational database
systems: Alternatives and implications. In Proc. 1998 ACM-SIGMOD Int. Conf. Management of Data (SIG-
MOD’98), Seattle, WA, pp. 343–354.
Srikant, R., Vu, Q., and Agrawal, R. 1997. Mining association rules with item constraints. In Proc. 1997 Int. Conf.
Knowledge Discovery and Data Mining (KDD’97), Newport Beach, CA, pp. 67–73.
Zaki, M.J. and Hsiao, C.J. 2002. CHARM: An efficient algorithm for closed itemset mining. In Proc. 2002 SIAM
Int. Conf. Data Mining, Arlington, VA, pp. 457–473.
Jiawei Han is a Professor in the Department of Computer Science at the University of Illinois at Urbana-
Champaign. Previously, he was an Endowed University Professor at Simon Fraser University, Canada. He has
been working on research into data mining, data warehousing, spatial and multimedia databases, deductive and
object-oriented databases, and bio-medical databases, with over 250 journal and conference publications. He has
chaired or served in over 90 program committees of international conferences and workshops. He also served
or is serving on the editorial boards for Data Mining and Knowledge Discovery: An International Journal, IEEE
Transactions on Knowledge and Data Engineering, and Journal of Intelligent Information Systems. He is currently
serving on the Board of Directors for the Executive Committee of ACM Special Interest Group on Knowledge
Discovery and Data Mining (SIGKDD) and chairman of ACM SIGKDD Curriculum Committee. Jiawei has re-
ceived IBM Faculty Awards, the Outstanding Contribution Award at the 2002 International Conference on Data
Mining, and an ACM Service Award.
MINING FREQUENT PATTERNS WITHOUT CANDIDATE GENERATION 87
Jian Pei received the B.Eng. and the M.Eng. degrees, both in Computer Science, from Shanghai Jiao Tong
University, China, in 1991 and 1993, respectively, and the Ph.D. degree in Computing Science from Simon Fraser
University, Canada, in 2002. He was a Ph.D. candidate in Peking University in 1997–1999. He is currently an
Assistant Professor of Computer Science and Engineering, the State University of New York at Buffalo, USA.
His research interests include data mining, data warehousing, online analytical processing, database systems, and
bio-informatics. His current research is supported in part by the National Science Foundation (NSF). He has
published over 40 research papers in refereed journals, conferences, and workshops. He has served in the program
committees of over 30 international conferences and workshops. He has been a reviewer for some leading academic
journals. He is a member of the ACM, the ACM SIGMOD, the ACM SIGKDD and the IEEE Computer Society.
Yiwen Yin received his M.Sc. degree in Computing Science at Simon Fraser University in 2001 and has been
working as a software engineering in B.C., Canada.
Runying Mao received her B.Eng degree from Zhejiang University, China, in 1997, and her M.Sc. degree from
Simon Fraser University, Canada, in 2001, both in Computer Science. She is currently working in the SQL Server
Group of Microsoft Corp. She also worked in China Telecom from 1997 to 1999.
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\Scalable-Parallel-Data-Mining-for-Association-Rules\p277-han.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Scalable Parallel Data Mining for Association Rules *
Abstract
Eui-Hong (Sam) Han George Karypis Vipin Kumar
Department of Computer Science Department of Computer Science Department of Computer Science
University of Minnesota University of Minnesota University of Minnesota
Minneapolis, MN 55455 Minneapolis, MN 55455 Minneapolis, MN 55455
han@ce.umn.edu karypis@cs.umn.edu kumarfke.umn.edu
One of the important problems in data mining is dBcover-
ing association rules from databases of transactions where
each transaction consists of a set of iterns. The most time
consuming operation in this discovery process is the com-
putation of the frequency of the occurrences of interesting
subset of items (called candidates) in the database of trans-
actions. To prune the exponentially large space of candi-
dates, most existing algorithms, consider only those candi-
dates that have a user defined minimum support. Even with
the pruning, the task of finding all association rules requires
a lot of computation power and time. Parallel computers
offer a potentiaJ solution to the computation requirement
of this task, provided efficient and scalable parallel algo-
rithms can be designed. In this paper, we present two new
parallel algorithms for mining association rules. The Intel-
ltgent Data Distribution algorithm efficiently uses aggregate
memory of the parallel computer by employing intelligent
candidate psrtit ioning scheme and uses efficient communi-
cation mechanism to move data among the processors. The
Hybrid Distribution algorithm further improves upon the In-
teUigent Data Distribution algorithm by dynamically parti-
tioning the candidate set to maintain good load balance.
The experimental results on a Cray T3D parallel computer
show that the Hybrid Distribution algorithm scales linearly
and exploits the aggregate memory better and can generate
more association rules with a single scan of database per
pass.
1 Introduction
One of the important problems in data mining [SAD+ 93] is
discovering association Am from databases of transactions,
“This work was supported by NSF grant ASC-9634719, Army
Research Office contract DA/DAAH04-95-l-0538, Cray Research
lncFellowship, and IBM partnership award, the content of which
does not necessarily reflect the policy of the government, and no
official endorsement should be inferred. Access to computing fa-
cilities was provided by AHPCRC, Minnesota Supercomputer In-
stitute, Cray Research. I-nc., and NSF grant CDA-9414015. See
http: //www.cs.umn.edu/han/papers.html#DataMiningPapers for an
extended version of this paper and other related papers.
Permission to make digital/hard copy of part or all this work for
personal or claearoom uae ia granted without fee provided that
copies are not made or distributed for profit or commercial advan-
tage, the copyright notice, the title of the publication and ita date
appear, and notice ia givan that copying ia by permission of ACM,
Inc. To copy otherwisa, to republish, to peat on servers, or to
redistribute to Iiats, requiree prior epacific parmiasion and/or a fee.
SIGMOD ’97 AZ,USA
@ 1997 ACM 0-89791-91 1-4197/0005 ...$3.50
277
where each transaction contaius a set of items. The most
time consuming operation in this discovery process is the
computation of-the frequencies of the occurrence of subsets
of items, also called candidates, in the database of transac-
tions. Since usually such transaction-based databases con-
tain extremely large amounts of data aud large number
of distinct items, the total number of candidates ia pro-
hibitively large. Hence, current association rule discovery
techniques [AS94, HS95, SON95, SA95] try to prune the
search space by requiring a minimum level of support for
candidates under consideration. Support is a measure baaed
on the number of occurrences of the candidates in database
transactions. Apriori [AS94] is a recent state-of-the-art al-
gorithm that aggressively prunes the set of potential can-
didates of size k by looking at the precise support for can-
didates of size k - 1. In the ktk iteration, this algorithm
computes the occurrences of potential candidates of size k
in each of the transactions. To do this task efficiently, the
algorithm maintains all potential candidates of size k in a
hash tree. This algorithm does not require the transactions
to stay in main memory, but requires the hash trees to stay
in main memory.
Even with the Klghly effective pruning method of Apri-
ori, the task of finding all association rules requires a lot of
computation power that is available only in parallel com-
puters. Furthermore, the size of the main memory in the
aerial computer puts an upper limit on the size of the candi-
date sets that can be considered in any iteration (aud thus
a lower bound on the minimum level of support imposed
on candidates under consideration). Parallel computers also
offer increased memory to solve this problem.
Two parallel algorithms, Count Distribution and Data
Distribution were proposed in [AS96]. The Count Distribu-
tion algorithm haa shown to scale linearly and have excellent
speedup and sizeup behavior with respect to the number of
transactions [AS96]. However, this algorithm works only
when the entire hash tree in each pass of the algorithm fits
into the main memory of single processor of the parallel com-
puters. Hence, the fJount Distribution algorithm, like its se-
quential counterpart Apriori, is unscalable with respect to
increasing candidate size. The Data Distribution algorithm
addresses the memory problem of the Count Distribution
algorithm by partitioning the candidate set and assigning
a partition to each processor in the system. However, this
algorithm results in high communication overhead due to
data movement and redundant computation [AS96].
In this paper, we present two parallel algorithms for min-
ing association rules. We iirst present Intelligent Data Dis-
tribution algorithm that improves upon the Data Distribu-
tzon algorithm such that the communication overhead and
redundant computation is minimized. The Hybn”d Distribu-
tion algorithm further improves upon the Intelligent Data
Distribution algorithm by dynamically grouping processors
and partitioning the candidate set accordingly to maintain
good load balance. The experimental results on a Cray
T3D parallel computer show that the Hybrid Distribution
algorithm scales linearly and exploits the aggregate memory
better and can generate more association rules with a single
scan of database per pass. An extended version of this paper
that also contains the analysis of the performance of these
schemes is available in [HKK97].
The rest of this paper is organized as follows. Section 2
provides an overview of the serial algorithm for mining as-
sociation rules. Section 3 describes existing and proposed
parallel algorithms. Experimental results are shown in Sec-
tion 4. Section 5 contains conclusions.
2 Basic Concepts
Let T be the set of transactions where each transaction is
a subset of the item-set I. Let C be a subset of 1, then we
define the support count of C with respect to T to be:
u(c) = I{t[t E Z’, c g t}!.
An association rule is an expression of the form X ~ Y,
where X ~ 1 and Y ~ I. The supports of the rule X ~ Y
is defined as u (X U Y)/lTl, and the confidence a is defined
as a(X U Y)/a(X). For example, consider a rule {1 2} +
{3}, i.e. items 1 and 2 implies 3. The support of this rule is
the frequency of the item-set {1 2 3} in the transactions. For
example, a support of 0.05 means that 570 of the transac-
tions contain {1 2 3}. The confidence of this rule is defined
as the ratio of the frequencies of {1 2 3} and {1 2}. For
example, if 107o of the transactions contain {1 2}, then the
confidence of the rule is 0.05/0.10 = 0.5. A rule that has a
very high confidence (i.e., that is close to 1.0) is often very
important, because it provides an accurate prediction on the
association of the items in the rule. The support of a rule
is also important, since it indicates how frequent the rule is
in the transactions. Rules that have very small support are
often uninteresting, since they do not describe significantly
large populations. This is one of the reasons why most algo-
rithms disregard any rules that do not satisfy the minimum
support condition specified by the user. This filtering due
to the minimum required support is also critical in reduc-
ing the number of derived association rules to a manageable
size.
The task of discovering an association rule is to find all
rules X ~ Y, where s is at least a given minimum sup-
port threshold and a is at least a given minimum confidence
threshold. The association rule discovery is composed of
two steps. The first step is to discover all the frequent
item-sets (candidate sets that has more support than the
minimum support threshold specified) and the second step
is to generate association rules that have higher confidence
than the minimum confidence threshold from these frequent
item-sets.
A number of algorithms have been developed for discov-
ering association rules [AIS93, AS94, HS95]. Our parallel
algorithms are based on the Apriori algorithm [AS94] that
has smaller computational complexity compared to other al-
gorithms. In the rest of this section, we briefly describe the
Apriori algorithm. The reader should refer to [AS94] for
further details.
1. F’1 = { frequent l-item-sets} ;
2. for ( k = 2; ~h_l # ~; k++ ) do begin
3. ck = apriori-gen(~k_~ )
4. for all transactions t E T
5. subset (C~, t)
6. Fh = {c C ck I C.count ~ minsup}
7. end
8. Answer = U Fk
Figure 1: Apriori Algorithm
The Apriori algorithm consists of a number of passes.
During pa& k, the-algorithm finds the set of frequen~ item-
sets Fh of length k that satisfy the minimum support re-
quirement. The algorithm terminates when Fh is empty.
The high level structures of the Aptioti algorithm are given
in Figure 1. Initially FI contains all the items (i.e., item set
of size one) that satisfy the minimum support requirement.
Then for k = 2,3,4,..., the algorithm generates ck of can-
didates item-sets of length k using ~k-1. This is done in
the function apriori.gen, which generates ck by performing
a join operation on the item-sets of &1. Once the crm-
didate item-sets are found, their frequencies me computed
by counting how many transactions contain these candidate
item-sets. Finally, Fk is generated by pruning ck to elim-
inate item-sets with frequencies smaller than the minimum
support. The union of the frequent item-sets, U Fk, is the
frequent item-sets from which we generate association rules.
Computing the counts of the candidate item-sets is the
most computationally expensive step of the algorithm. One
naive way to compute these counts is to scan each trans-
action and see if it contains. any of the caddate item-sets
as its subset by performing a string-matching against each
candidate item-set. A faster way of performing this opera-
tion is to use a candidate hash tree in which the candidate
item-sets are hashed [AS94]. Figure 2 shows one example
of the candidate hash tree with candidates of length 3. The
internal nodes of the haah tree have hash tables that contain
links to child nodes. The leaf nodes contain the candidate
item-sets. When each candidate item-set is generated, the
items in the set are stored in sorted order. Each candidate
item-set is inserted into the hash tree by h~hing each item
at the internal nodes in sequence and following the links in
the hash table. Once the leaf is reached, the candidate item-
set is inserted at the leaf if the total number of candidate
item-sets are less than the maximum allowed. If the total
number of candidate item-sets at the leaf exceeds the maxi-
mum allowed and there are more items to be hashed in the
candidate item-set, the leaf node is converted into an inter-
nal node and child nodes are created for the new internal
node. The candidate item-sets are distributed to the child
nodes according to the haah values of the items. For exam-
ple, the candkiate item set {1 2 4} is inserted by hashing
item 1 at the root to reach the left child node of the root,
hashing item 2 at that node to reach the middle child node,
hashing item 3 to reach the left child node which is a leaf
node.
The subset function traverses the hash tree from the root
with every item in a transaction as a possible starting item
of a candidate. In the next level of the tree, all the items
of the transaction following the starting item are hashed.
278
Hash FU,,CLIOII
‘+-u-----..-
Transaction 2+=
w ,“’’””” ,... -”
---- , /’-. ,-. .,/ ,, ‘“a~. / ,,‘./’ . / .’
Candidate Hash Tree I
689
Figure 2: Subset operation on the root of a candidate hash
tree.
/ ‘\ / ,“
This is done recursively until a leaf is reached. At this time,
all the candidates at the leaf are checked against the trans-
action and their counts are updated accordingly. Figure 2
shows the subset operation at the first level of the tree with
transaction {1 2 3 5 6}. The item 1 is h~hed to the left
child node of the root and the following transaction {2 35
6} is applied recursively to the left child node. The item 2
is hashed to the middle child node of the root and the whole
transaction is checked against two candidate item-sets in the
middle child node. Then item 3 is hashed to the right child
node of the root and the following transaction {5 6} is ap-
plied recursively to the right child node. Figure 3 shows the
subset operation on the left child node of the root. Here
the items 2 and 5 are hashed to the middle child node and
the following transactions {3 5 6} and {6} respectively are
applied recursively to the middle child node. The item 3 is
hashed to the right child node and the remaining transaction
{5 6} is applied recursively to the right child node.
The bulk of the computation is spent in finding the fre-
quent item-sets and the amount of time required to find the
rules horn these frequent item-sets is relatively small. For
this reason, parallel association algorithms focus on how to
parallelize the first step. The parallel implementation of the
second step is straightforward and is discussed in [AS96].
3 Parallel Algorithms
In this section, we will focus on the parallelization of the
first task that finds all frequent item-sets. We first dkcuss
two parallel algorithms proposed in [AS96] to help motivate
our parallel formulations. In all our discussions, we assume
that the transactions are evenly distributed among the pro-
cessors.
3.1 Count Distribution Algorithm
In the Count Distribution ( CD) algorithm proposed in [AS96],
each processor computes how many times all the candidates
appear in the locally stored transactions. This is done by
building the entire hash tree that corresponds to all the can-
didates and then performing a single pass over the locally
stored transact ions to collect the counts. The global counts
of the candidates are computed by summing these individ-
ual counts using a global reduction operation ~GGK94].
This algorithm is illustrated in Figure 4. Note that since
each mocessor needs to build a hash tree for all the candl-
C“’’’’:H”’T791$K!
dates; these hash trees are identical at each processor. Thus,
12+ m-------
excluding the global reduction, each processor in the CDal-
gorithm executes the serial Aprioti algorithm on the locally
stored transactions.
13+
15+
~-
B-----‘#&
EH2124 125457 458
Figure 3: Subset operation on
root of a candidate hash tree.
c1159
the left most subtree of the
Thw algorithm has been shown to scale linearly with the
number of transactions [AS96]. This is because each pro-
cessor can compute the counts independently of the other
processors and needs to communicate with the other pro-
cessors only once at the end of the computation step. How-
ever, this algorithm works well only when the hash trees cau
fit into the main memory of each processor. If the number
of candidates is large, then the hash tree does not fit into
the main memory. In this case, this algorithm has to par-
tition the haah tree and compute the counts by scanning
the database multiple times, once for each partition of the
hash tree. Note that the number of candidates increases if
either the number of distinct items in the database increases
or if the minimum support level of the association rules de
creases. Thus the CD algorithm is effective for small number
of distinct items and a high minimum support level.
279
Proc o ProcI Proc 2 Proc 3
(
Data
(,Candidate HashTreeIA.B) I 2 I
I ,—,—, I
yk.
(
Data
NIP
f
mt D
‘. “..
‘. -------
‘..
I ( CamiidateHashTree
\ s[A,B} I[A,C] 2[A,D} 3M (B,C] I[B,E} 2(C,D) 2(D,E] 5
.-7 %
-
---
-- ---
(
---
/ ~ CandidateHashTme
~
( (A,B] 3(A,C) 3(A,D) 4
M (B,C} 2
(B,E] 4
(C,D] 3
(D,E} 1
,7 Y.
. -.. ...”
--- - -----
Data
NJP
(nt’ DCandidateHashTret
\
~
(A,B] 2
(A,C] I
(A,D] 3
M (B,C] 5
[B,E] 2
(C,D] 3
(D,E) 2
,= +
/“ /
----
.’
..”
. . . GlobslReduction /-’. ..- ~---------- ----------- ---------------- -----
N: number of dataitems
M: sire of candidateset
P: numberof processors
Figure 4: Count Distribution (CD) Algorithm
3.2 Data Distribution Algorithm
The Data Distribution (LID) algorithm [AS96] addresses the
memory problem of the CD algorithm by partitioning the
candidate item-sets among the processors. This partition-
ing is done in a round robin fashion. Each processor is
responsible for computing the counts of its locally stored
subset of the candidate item-sets for all the transactions in
the database. In order to do that, each processor needs to
scan the portions of the transactions assigned to the other
processors as well as its locally stored portion of the trans-
actions. In the DD algorithm, this is done by having each
processor receive the portions of the transactions stored in
the other processors according to the following fashion. Each
processor allocates P buffers (each one page long and one
for each processor). At processor Pi, the ith buffer is used
to store transactions from the locally stored database and
the remaining buffers are used to store transactions from
the other processors, such that buffer j storea transactions
from processor Pj. Now each processor Pi checks the P
buffers to see which one contains data. Let k be this buffer
(ties are broken in favor of buffers of other processors and
ties among buffers of other processors are broken arbitrar-
ily). The processor processes the transactions in this buffer
and updates the counts of its own candidate subset. If this
buffer corresponds to the buffer that stores local transactions
(i.e., k = i), then it is sent to all the other processors asyn-
chronously and a new page is read from the local database.
If this buffer corresponds to a buffer that stores transactions
from another processor (i.e., k # i), then it is cleared and
an asynchronous receive request is issued to processor pk.
This continues until every processor has processed all the
transactions. Having computed the counts of its candidate
item-sets, each processor finds the frequent item-sets from
its candidate item-set and these frequent item-sets are sent
to every other processor using an all-to-all broadcast opera-
tion [KGGK94]. Figure 5 shows the high level operations of
the algorithm. Note that each processor has a different set
of candidates in the candidate hash tree.
This algorithm exploits the total available memory bet-
ter than CD, as it partitions the candidate set among pro-
cessors. As the number of processors increases, the number
of candidates that the algorithm can handle also increases
However, as reported in [AS96], the performance of this al-
gorithm is significantly worse than the CD algorithm. The
run time of this algorithm is 10 to 20 times more than that
of the CD algorithm on 16 processors [AS96]. The problem
lies with the communication pattern of the algorithm and
the redundant work that is performed in processing all the
transactions.
The communication pattern of this algorithm causes two
problems. First, during each pass of the algorithm each
processor sends to all the other processors the portion of
the database that resides locally. In particular, each pro-
cessor reads the locally stored portion of the database one
page at a time and sends it to all the other processors by
issuing P — 1 send operations. Similarly, each processor is-
sues a receive operation from each other processor in order
to receive these pages. If the interconnection network of the
underlying parallel computer is fully connected (i.e., there is
a direct link between all pairs of processors) and each pro-
cessor can receive data on all incoming links simultaneously,
then this communication pattern will lead to a very good
performance. In particular, if (l(N/P) is the size of the
database assigned locally to each processor, the amount of
time spent in the communication will be O(IV), However, on
280
Data
<---
Broadc
ProcO
LccalData RemoIeData
,----,
II
1,
t,
1,
{,
1,
1,
II
II,-----
()
Count Col
CandidateHashTtte
H
{LB) 2
M@ {B,C] 3
(C,E) 3
Data--- >
Oadce
Pm 2 Proc3
LccalData
D
w
(
count
RemoteData
~_---l
{,
II
II
1,
1,
II
1,
1,
II
------
)
Cnu (’count
RemoteData
~.---,
1!
1,
1,
18
II
1,
1,
II
II
,-----
)
Cou
LccrdData
w
[
RetnoreData
~----1
1,
II
1,
1,
1,
1,
1,
II
II
,-----
r)
count cm
CandidateHsshTree
❑
[A,E] 1
M? [C,D] 1
{u] 1
k,: ,/ k ,f k
\ / ‘. .,. ‘.,
/.,’. ‘. ..’,. ~.’ ‘. .’/’‘---------------- ‘.
‘.
------- -’/’,,-. /
‘.. All-to-allBrosdcsst .0’
N mrmta ofdataitems . . ------- ---
---~ ---
M sireofrarrdidales t
-------------- --------------
P:numberof pwcssors
Figure 5: Data Distribution (DD) Algorithm
sIi realistic parallel computers, the processors are connected
via a sparser networks (such as 2D, 3D or hypercube) and a
processor can receive data from (or send data to) only one
other processor at a time. On such machines, this communi-
cation pattern will take significantly more than O(PJ) time
because of contention.
Second, if we look at the size of the candidate sets as a
function of the number of passes of the algorithm, we see
that in the first few passes, the size of the candidate sets
increases and after that it decreases. In particulru, during
the last several passes of the algorithm, there are only a
small number of items in the candidate sets. However, each
processor in the DD algorithm still sends the locally stored
portions of the database to all the other processors. Thus,
even tbough the computation decreases, the amount of com-
munication remains the same.
The redundant work is introduced due to the fact that
every processor has to process every single transaction in
the database. Although, the number of candidates stored at
each processor has been reduced by a factor of P, the amount
of computation performed for each transaction has not been
proportionally reduced. In CD (see Figure 4), only ZV/P
transactions go through each hash tree of M candidates,
whereas in DD (see Figure 5), IV transactions have to go
through each haah tree of M/P candidates. If the amount of
work required for each transaction to be checked against the
hash tree of M/P candidates is l/P of that of the hash tree
of M candidates, then there is no extra work. However, for
this to be true in the DD algorithm, the average depth of the
haah tree has to be reduced by P and the average number
of candidates in the leaf nodes has to be alao reduced by
P. This does not happen in the hash tree scheme discussed
in Section 2. To see this, consider a hash tree with single
Data
*--*
Brndwt
t
candidate at the leaf node and with branching factor of B.
By reducing the number of candidates by P, the depth of
the hash tree will decrease by only logB P. With B > P
(which would be the most likely), the logB P <1. On the
other hand, when the hash tree is completely expanded to
the depth k and the number of candidates at the leaf is
greater than P, the number of candidates at the leaf goes
down by P, but the depth of the tree does not change. In
most of real cases, the hash tree will be in between these two
extreme cases. However, in general, the amount of work per
transaction will not go down by P of the original hash tree
with M candldatea.
3.3 Intelligent Data Distribution Algorithm
We developed the Intelligent Data Dwtribution (IDD) al-
gorithm that solves the problems of the DD algorithm dis-
cussed in Section 3.2.
The locally stored portions of the datab- can be sent
to all the other processors by using the ring-based all-to-
all broadcast described in ~GGK94]. This operation does
not suffer from the contention problems and it takes O(N)
time on any parallel architecture that cart be embedded in
a ring. Figure 6 shows the pseudo code for this data move-
ment operation. In our algorithm, the processors form a
logical ring and each processor determines its right and left
neighboring procmsors. Each processor has one send btier
(SBuf) and one receive butYer (RBuf). Initially, the SBuf
is filled with one block of local data. Then each processor
initiates an synchronous send operation to the right neigh-
boring processor with SBuf and au asynchronous receive op-
eration to the left neighboring processor with RBuf. While
these asynchronous operations are proceeding, each proces-
281
while (!done) {
FillBuffer(fd, SBuf);
for (k = O; k < P-1; ++k) {
J* send/receive data in non-blocking pipeline ‘/
MPIJrecv(RBuf, left);
MPIJsend(SBuf, right);
/* process transactions in SBuf and update hash tree “/
Subset(HTree, SBuf);
MPI-Waitallo;
/“ swap two buffers “/
tmp = SBuf;
SBuf = RBuC
RBuf = tmp;
}
/“ process transactions in SBuf and update hash tree “/
Subset(HTree, SBuf);
}
Figure 6: Pseudo Code for Data Movements
sor processes the transactions in SBuf and collects the counts
of the candidates assigned to the processor. After this op-
eration, each processor waits until these asynchronous op-
erations complete. Then the roles of SBuf and RBuf are
switched and the above operations continue for P – 1 times.
Compared to DD, where all the processors send data to all
other processors, we perform only a point-to-point commu-
nication between neighbora, thus eliminating any communi-
cation contention.
Recall that in the DD algorithm, the communication
overhead of data movements dominates the computation
work in the later passes of the process. In IDD, we solve
this problem by switching to the CD algorithm when the to-
tal number of candidates falls below a threshold. Note that
swittilng to the CD algorithm does not cause any commu-
nication or computation overhead since each processor can
independently determine when to switch provided that the
threshold parameter is globafly known. A good choice of
the parameter is the maximum number of candidates that
single processor can have in the main memory.
In order to eliminate the redundant work due to the par-
titioning of the candidate item-sets, we must find a fast way
to check whether a given transaction can potentially con-
tain any of the candidates stored at each processor. This
cannot be done by partitioning ck in a round-robin fashion.
However, if we partition Ck among processors in such a way
that each processor gets item-sets that begin only with a
subset of all possible items, then we can check the items of
a transaction against this subset to determine if the hash
tree contains candidates starting with these items. We tra-
verse the haah tree with only the items in the transaction
that belong to this subset. Thus, we solve the redundant
work problem of DD by the intelligent partitioning of ck.
Figure 7 shows the high level picture of the afgorithm.
In this example, Processor O has all the candidates start-
ing with items A and C, Processor 1 has all the candidates
starting with B and E, and so on. Each processor keeps
the first items of the candidates it has in a bit-map. In the
Apriori algorithm, at the root level of hash tree, every item
in a transaction is h~hed and checked against the hash tree.
However, in our algorithm, at the root level, each processor
filters every item of the transaction by checking against the
bit-map to see if the processor contains candidates start-
ing with that item of the transaction. If the processor does
not contain the candidates starting with that item, the pro-
cessing steps involved with that item as the first item in
the candidate can be skipped. This reduces the amount of
transaction data that has to go through the hash tree; thus,
reducing the computation. For example, let {A B C D E
F G H} be a transaction that processor O is processing in
the subset function discussed in Section 2. At the top level
of the function, processor O will only proceed with items A
and C(i.e., A+ BCDEFG Hand C+ DE FG H).
When the page containing this transaction is shifted to pro-
cessor 1, this processor will only process items starting with
Band E(i.e., B+ CD EFG Hand E+ FGH). For
each transaction in the database, our approach reduces the
amount of work performed by each processor by a factor of
P; thus, eliminates any redundant work. Note that both the
judicious partitioning of the hash tree (indirectly caused by
the partitioning of candidate item-set) and the filtering step
are required to eliminate this redundant work.
The intelligent partitioning of the candidate set used in
IDD requires our algorithm to have a good load bafancing.
One of the criteria of a good partitioning involved here is
to have an equal number of candidates in all the proces-
sors. This gives about the same size hash tree in all the
processors and thus provides good load balancing among
processors. Note that in the DD algorithm, this was accom-
plished by distributing candidates in a round robin fashion.
Another criteria is to have each processor have a mixed bag
of the candidates. This will help to prevent load imbalance
due to the skew in data. For instance, consider a database
with 100 distinct items numbered from 1 to 100 and that
the database transactions have more data items numbered
with 1 to 50. If we partition the candidates between two
processors and assign all the candidates starting with iterns
1 to 50 to processor PO and candidates starting with items
51 to 100 to processor PI, then there would be more work
for processor Po.
To achieve a load-balanced distribution of the candidate
item-sets, we use a partitioning algorithm that is baaed on
bin-packing [PS82]. For each item in the database, we com-
pute the number of candidate item-sets starting with this
particular item. We then use a bin-packing algorithm to
partition these items in P buckets such that the numbers
of the candidate item-sets starting with these items in each
bucket are equal. To remove any data skew, our bin-packing
algorithm randomly selects the item to be assigned next in
a bin. Figure 7 shows the partitioned candidate hash tree
and its corresponding bitmaps in each processor. We were
able to achieve less than 570 of load imbalance with the bin
packing method described here.
3.4 Hybrid Algorithm
The IDD afgorithm exploits the total system memory while
minimizing the communication overhead involved. The av-
erage number of candidates assigned to ed processor is
M/P, where M is the number of total candidates. As more
processors are used, the number of candidates assigned to
each processor decreaws. This has two implications. First,
with fewer number of candidates per processor, it is much
more difficult to balance the work. Second, the smaller num-
ber of candidates gives a smaller hash tree and less compu-
tation work per data in SBuf of Figure 6. Eventually the
amount of computation may be less than the communica-
tion involved, and this reduces overall efficiency. This will
be au even more serious problem in a system that cannot
282
Dala----
Shifl
Pro-cO
LocalData RemoteData
NIP
[
~.---,
1,
!,
1,
1,
1,
1,
Id
1,
1,
,----4
‘fOunm
GafiJ(A,B] 2
M/T’ {A,C] 3
(C,E] 3
Data---+
shift
Prw I
LocalData Remo[eDali
~-.-.l
1,
1,
II
1,
1,
1,1,
1,
1!
,-----
P
(--+
BitMap
B,E
lab--+
;hifl r-W
Dun/
~----1
1,
II
1,
1, Data
l,- ---!
1, ShitiII
1,
II
______
~oun
QI
BitMap
D
P::T9
Can&lateHashTree
❑
[D,E] 2
M/P {B:D] 5 MiP [D,F) 3
{E,F} 1 [D,G] 4
~ ~ .-..- -..-.. ~. -. -------------‘.. --------
Prrc 3
Lwal Da!a Renw Data
,----,
1,
1,
1,
1,
1,
1 I
1,
!,
f,
,-----
Y“w
\ m(F,G) 3 JM/P (G,f] 4(G,J] 2
--- ,.’---
,,’. .
‘.. ..”
All-to-allBroadcast ------ ---
N numkxofdataitems
--- --
------- -----
------ -----------------
M sizeofcandidateset
P:nmrdxrof processm
Figure 7: Intelligent Data Distribution (IDD) Algorithm
perform asynchronous communication.
The H@rid DzstributiorL (HD) algorithm addresses the
above problem by combining the CD and the IDD algo-
rithms in the following way. Consider a P-processor system
in which the processors are split into G equal size groups,
each containing P/G processors. In the HD algorithm, we
execute the CD algorithm as if there were only P/G proces-
sors. That is, we partition the transactions of the database
into P/G parts each of size N/( P/G), and assign the task
of computing the counts of the candidate set C~ for each
subset of the transactions to each one of these groups of
processors. Within each group, these counts are computed
using the lDD algorithm. That is, the transactions and the
candidate set ck are partitioned among the processors of
each group, so that each processor gets roughly [C~l/G can-
didate item-sets ad N/F’ transactions. Now, each group
of processors computes the counts using the IDD algorithm,
and the overall counts are computing by performing a re-
duction operation among the P/G groups of processors.
The HD algorithm can be better visualized if we think of
the processors as being arranged in a two dimensional grid
of G rows and P/G columns. The transactions are parti-
tioned equally among the P processors, and the candidate
set ck is partitioned among the processors of each column
of this grid. This partitioning of Ck is the same for each
column of processors, that is, the processors along each row
of the grid get the same subset of Ck. Now, the IDD algo-
rithm is executed independently afong each column of the
grid, and the total counts of each subset of C,$ is obtained
by performing a reduction operation along the rows of this
processor grid. Figure 8 illustrates the HD algorithm for a
3 x 4 grid of processors.
The HD algorithm determines the configuration of the
Data--- +
shift
processor grid dynamically. In particular, the HD algorithm
partitions the candidate set into a big enough section and
assign a group of processors to each partition. The same
parameter that was used to determine whether to switch to
CD algorithm can be used to decide the size of the parti-
tion in this algorithm. For example, let this parameter be
C. If the total number of candidates M is less than C, it
switches to CD algorithm. Otherwise find out the number
of processor groups G = [M/Cl and form a logical G x P/G
processor mesh configuration. In the example of Figure 8,
the HD afgorithm executes the CD algorithm as if there were
only 4 processors, where the 4 processors correspond to the
4 processor columns. That is, the database transactions are
partitioned in 4 parts, and each one of these 4 hypothet-
ical processors computes the local counts of all the candi-
date item-sets. Then the global counts can be computed by
performing the global reduction operation discussed in Sec-
tion 3.1. However, since each one of these hypothetical pro-
cessors is made up of 3 processors, the computation of local
counts of the candidate item-sets in a hypothetical processor
corresponds to the computation of the counts of the candi-
date item-sets on the database transactions sitting on the
3 processors. This operation is performed by executing the
IDD algorithm in each of 4 hypothetical processors. This is
shown in the step 1 of Figure 8. Note that processors in the
same row have exactly the same candidates and candidate
sets along the each column partition the total csdidate set.
At the end of this operation, each processor has complete
counts of local candidates for all the data of the processors
of the same column (or of a hypothetical processor). The
global reduction operation is broken into two parts corre-
spondbg to the step 2 and 3 of the Figure 8. In the step 2,
perform reduction operation [KGGK94] along the row such
283
Step 1: Partitioning of Candidate Sets and Data Movement Along the Columns
...-. ----- ,---~
m-; m m-h
----- -/
El “i
‘.
Candidate HashTree II
A,B 3
D,E 2
G.F 1
* Data Shift
El
Candidate Hash Tree
C,D 3
F,G 1
F,H 2
? Data Shlfi :
~1 j
CandicfafcHashTree /
C,D 1
F,G 2
F,H 2
,’‘. -. ---.”
t Data Shlfi
,,
ml
CandidateHash Tree
C,D 2
F,G 1
F,H 2
~. -- _---”
ml
Candidate Hash Tree
C,D 1
F,G 1
F.H 2
,,
8
,’
/’
,
/’‘. ------,
‘\
------- .
Step 2: Reduction Operation Along the Rows
v
----------------------- ~---- --------. -- ----- ~------ ------- ---- .-----,,.
ElCundidale Hash TreeA.B 1D,E 2G,F 2 Cumfidiuc Hush Tree❑A.B 3DE 2GF 1Candidate Hush Tree Cmtlidae Hush Tree❑A,B 7 ❑A.B 2D,E 7 D,E 1G,F 5 G,F 1
1 , ,
-----------------------. ~----- ------------------ ~----- ------------------
ml ml ~, ~,
-----------------------. ~ _ -------~ ---------
Evil
Candidate Hash Tree
C,D 7
F,G 5
F.H B
..
mlcandidate Hash TreeC,D 2F,G IF,H 2 CandIdatc Hxh Tree❑C,D 1F,G 2F,H 2 mC,D 1F,G 1F.fi 2
Step 3: All-to-all Broadcast Operation Along the First Column
Follwed by One-to-all Broadcast Operation Along the Rows
, ---------------------- * ----------------------- ~ ------------------ ,----- ,
t -t
Frequent Item Set
m ;,
F,H 8
H,l 8 .’\. ,
‘\,.
Frequmt Item.%
m
F,H 8
H,I 8
Frequent Item Sd
m
F.H 8
H.I 8
Frequmt Item Su
m
F,H 8
H,I 8
L I I
; ‘1, --------- ,--- = --------- ---------- ------------- ------------------------ ,
L-1
,t
Frequent km Se! ) ~
m
/’
F,H 8 ‘ A1l-1,>-nll:
H,l 8 , Brmukasi !.
.1
,,
,,
Frequent Item Set
m
F,H 8
H,l 8
Frequent Item set
m
F,H lf
H.f 8
Frquem Item Sc4
m
F,H 8
H,l 8
L I
------ ___________. -------- J---/---
,,
------ ------ r ----------------------- ,
t t
I I I I
T
;,/
Frequent Item set ,,’,,’
m
./
F,H 8 /’
H,I 8
Freq.ax Item Set Frequenthem S.d Frequent ftem S@
m
F,H 8
m
F,H 8
H.I 8 H,l SmF.H 8H.I 8
I 1 1 1
Figure 8: Hybrid Distribution (HD) Algorithm in 3 x 4 Processor Mesh (G= 3, P = 12)
that the processor in the first column of the same row has
the total counts for the candidates in the same row proces-
sors. In the step 3, all the processors in the first column
generate frequent set from the candidate set and perform
all-to-all broadcast operation along the first column of the
processor mesh. Then the processors in the first column
broadcast the full frequent sets to the processors along the
same row using one-to-all broadcast operation [KGGK94].
At this point, all the processors have the frequent sets and
ready to proceed to the next pass.
This algorithm inherits all the good features of the IDD
algorithm. It also provides good load balance and enough
computation work by maintaining minimum number of can-
didates per processor. At the same time, the amount of data
movement in this algorithm has been cut down to 1/G of the
IDD.
4 Experimental Results
We implemented our parrdlel algorithms on a 128-proceseor
Cray T3D parallel computer. Each processor on the T3D is
a 150Mhz Dec Alpha (EV4), and has 64 Mbytes of memory.
The processors are interconnected via a three dimensional
torus network that has a peak unidirectional bandwidth of
150Mbytes per second, and a small latency. For commu-
nication we used the message passing interface (MPI). Our
experiments have shown that for 16Kbytes we obtain a band-
width of 74Mbytes/seconds and an effective startup time of
150 microseconds.
We generated a synthetic dataset using a tool provided
by [Pro96] and described in [AS94]. The parameters for the
data set chosen are average transaction length of 15 and av-
erage size of frequent item sets of 6. Data sets with 1000
transactions (6.3KB) were generated for different processors.
Due to the disk limitations of the T3D system we have kept
the small transactions in the buffer and read the transac-
tions from the buffer instead of the actual disks. For the
experiments involving larger data sets, we read the same
data set multiple times. 1
We performed scaleup tests with lOOK transactions per
processor and minimum support of 0.25%. We could not
use lower minimum support because the CD algorithm ran
out of main memory. For this experiment, in the IDD and
HD algorithms we have set the minimum number of candi-
dates for switching to the CD algorithm very low to show
the validity of our approaches. With 0.25~0 support, both
algorithms switched to CD algorithm in pass 7 of total 12
passes and 90.7~0 of the overall response time of the serial
code was spent in the fist 6 passes. These scaleup results
are shown in Figure 9.
As noted in {AS96], the CD algorithm scales very well.
Looking at the performance obtained by IDD, we see that
its response time increases as we increase the number of
processors. This is due to the load balancing problem dis-
cussed in Section 3, where the number of candidates per
processor decreases as the number of processors increases.
However, the performance achieved by IDD is much bet-
ter than that of the DD algorithm of [AS96]. In particular,
IDD haa 4.4 times less response time than DD on 32 proces-
sors. It can be seen that the performance gap between IDD
and DD is widening as the number of processors increases.
This is due to the improvement we made on lDD with the
1We also performed similar experiments on an IBM SP2 in which
the entire database resided on disks. Our experiments show that
the 1/0 requirements do not change the relative performance of the
various schemes.
01
0 20 40 m
d%9r01F—Ors
100 120 1 0
Figure 9: Scaleup result with lOOK transactions and 0.25%
minimum support.
better communication mechanism for data movements and
the intelligent partitioning of the candidate set. Looking at
the performance of the HDalgorithm, we see that response
time remains almost constant as we increase the number of
processors while keeping the number of transactions per pro-
cessor and the minimum support fixed. Comparing against
CD, we see that HDactually performs better as the number
of processors increases. Its performance on 128 processors
is 9,5’?70better than CD. This performance advantage of HD
over CD is due to that the number of processors involved
in global reduction operation of counts is much less in HD
thanin CD.
We measured how our algorithms perform as we increase
the number of transactions per processor from 50K(3.2MB)
to 800K(50.4MB). For these experiments, we fixed the num-
ber of processors at 16 and the minimum support at 0.25%.
These results are shown in Figure 10. From this figure, we
can see that CD and HD perform almost identically. For
both algorithms, the response time incre~es linearly with
the number of transactions. IDD also scales linewly, but
because of its load imbalance problem, its performance is
somewhat worse.
Our experiments so far have shown that the performance
of HD and CD are quite comparable. However, the real
advantage of HD (and IDD) over CD is thatthey do not
require the whole haah tree to reside on each processor, and
thus better exploit the available memory. This allows us to
use a smaller minimum support in the Aprion” algorithm.
To verify this, we performed the experiments in which we
fixed the number of transactions per processor to 50K and
successively decreased the minimum support level. These
experiments for 16 and 64 processors are shown in Figures 11
and 12 respectively. A couple of interesting observations
can be made from these results. First, both IDD and HD
successfully ran using lower support levels that CD could not
run with. In particular, IDD and HD ran down to a support
level of 0.06% on 16 processors and 0.04% on 64 processors.
In contrast, CD could only run down to a support level of
o.$?s~. and ran out of memory for the lower supports. The
difference between the smaller support levels on 16 and 64
processors is due to the fact that the IDD and HD algorithms
can exploit the aggregate memory of the larger number of
processors.
285
Figure 10: Sizeup result with 16 processors and 0.25% min-
imum support.
The second thing to notice is that HD performs better
than IDD both on 16 and 64 processors, and the relative per-
formance of IDD compared to HD get worse as the number of
processors increases. As discussed earlier, this performance
difference is due to the load imbalance. As the number of
processors increases, this load imbalance gets worse. How-
ever, on 16 processors IDD is 3770 worse than HD for sup-
port level 0.25Y0, but only 18% worse for support of 0.06%.
Thii is because aa the support level decreases, the number
of candidates (shown in parenthesis in Figures 11 and 12)
incresses which improves the load balance.
Figures 11 and 12 also show the performance of a sim-
ple hybrid algorithm obtained by combining CD and IDD.
In this scheme, in each pass of the Apriori algorithm, we
perform CD if the hash table can fit in the memory of each
processors or IDD if it can not. As we can see from these re-
sults, this simple hybrid algorithm performs worse than HD.
In particular, the relative performance of this scheme com-
pare to HD gets worse as the number of processors increases.
For example, for a support level of 0.06%, it is 6% worse on
16 processors and 17% worse on 64 processors. Thus the
HD algorithm, by gradually adjusting the subsets of proces-
sors that perform IDD and CD, achieves better performance.
This is because of the following two reasons. First, the can-
didate set is split among fewer number of processors which
minimizes load imbalance and second, the reduction opera-
tion to obtain the counts in CD is performed among fewer
processors, which decreases the communication overhead.
In another experiment, we varied the number of proces-
sors from 2 to 64 and measured how low we can go with
minimum support for the IDD and HD algorithms. Table 1
shows the result for these algorithms. The result shows that
as we have more processors, these algorithms can handle
lower minimum support. Table 2 shows how the HD algo-
rithm chose the proc~or configuration based on the num-
ber of candidates at each pass with 64 processors and 0.04%
minimum support.
5 Conclusion
cum +
irndlqem&la -i--
w m- -sin@OIl@nd*-
*
,;
,/ ,x
,.-
[211K]
0.6 0.25 0.1 0.06
MklkmunCqx# (%)
Figure 11: Response time on 16 processors with 50K trma-
actions as the minimum support varies. At each support
level, the total number of candidate item-sets is shown in
parenthesis
Icoo
800
SW
4m
203
0
0.5 0.25 0.1 0.M.04
Mhh!mslppnl(n)
Figure 12: Response time on 64 processors with 50K trtms-
actions ss the minimum support varies. At each support
level, the total number of candidate item-sets is shown in
pwenthesis
In this paper, we proposed two parcdlel algorithms for min-
ing association rules. The IDD algorithm utilizes total main
memory available more effectively than the CD algorithm.
286
Number of processors 1 2 4 8 16 32 64
Successful down to 0.25 0.2 0.15 0.1 0.06 0.04 0.03
Ran out of memory at 0.2 0.15 0.1 0.06 0.04 0.03 0.02
Table 1: Minimum support (%) reachable with different number of processors in our algorithms.
Pass 2 3 4 5 6 7 8 9 10
Configuration 8x8 64x1 4x16 2x32 2x32 2x32 2x32 2x32 1x64
No of Canal. 351K 4348K 115K 76K 56K 34K 16K 6K 2K
Table 2: Processor configuration and number of candidates of the HD algorithm with 64 processors and 0.04% minimum
support for each pass. Note that 64 x 1 configuration is the same as the DD algorithm and 1 x 64 is the same as the CD
algorithm. The total number of pass was 13 and all passes after 9 had 1 x 64 configuration.
This algorithms improves over the DD algorithm which has
high communication overhead and redundant work. The
communication overhead was reduced using a better data
movement communication mechanism, and redundant work
was reduced by partitioning the candidate set intelligently
and using bit maps to prune away unnecessary computation.
However, as the number of processors available increases, the
efficiency of thk algorithm decreases unless the amount of
work is increased by having more number of candidates.
The HD combines advantages of the CD and IDD. This
algorithm partitions candidate sets just like the IDD to ex-
ploit the aggregate main memory, but dynamically deter-
mines the number of partitions such that the partitioned
candidate set fits into the main memory of each processor
and each processor has enough number of candidates for
computation. It also exploits the advantage of the CD by
just exchanging counts information and moving around the
minimum number of transactions among the smaller subset
of processors.
The experimental results on a 128-processor Cray T3D
parallel machine show that the HD algorithm scales just aa
well as the CD afgorithm with respect to the number of
transactions. It also exploits the aggregate main memory
better and thus is able to find out more association rules
with much smaller minimum support with a single scan of
database per pass. The IDD algorithm also outperforms the
DD algorithm, but is not as scalable as HD and CD.
Future works include applying these algorithms to real
data like retail sales transaction, mail order history database
and World Wide Web server logs [MJHS96] to confirm the
experimental results in the real life domain. We plan to
perform experiments on different platforms including Cray
T3E, IBM SP2 and SGI SMP clusters. We also plan on im-
plementing our ideas in generalized association rules [HF95,
SA95], and sequential patterns [MTV95, SA96].
Referetices
[AIS93] R. Agrawal, T. Imielinski, and A. Swami. Min-
ing association rules between sets of items
in large databases. In Prac. of 1993 ACM-
SIGMOD Int. Conf. on Management of Data,
Washington, D. C., 1993.
[AS94] &r Agrawal and R. Srikant. Fast algorithms
mining association rules. In Proc. of the
287
[AS96]
[HF95]
[HKK97]
[HS95]
20th VLDB Conference, pages 487499, Santi-
ago, Chile, 1994.
R. AgrawaJ and J.C. Shafer. Parallel mining of
association rules. IEEE IFansactions on Knowl-
edge and Data Eng., 8(6) :962–969, December
1996.
J. Han and Y. Fu. Discovery of multiple=level
association rules from large databases. In Proc.
of the 21st VLDB Conference, Zurich, Switzer-
kmd, 1995.
E.H. Han, G. Karypis, and V. Kumar. Scalable
parallel data mining for association rules. Tech-
nicaf Report TR-97-??, Department of Com-
puter Science, University of Minnesota, M in-
neapolis, 1997.
M. A. W. Houtsma and A. N. Swami. Set-
oriented mining for association rules in rela-
tional databases. In Proc. of the i lth Znt ‘i Conj.
on Data Eng., pages 25–33, Taipei, Taiwan,
1995.
(KGGK941 ViDin Kumar, Ananth Grama, Anshul Gupta,
L .-
and Geor~e KarvDis. Introduction to P~ral~
[MJHS96]
[MTV95]
[Pro96]
[PS82]
lel Compu~ing: A“f~orithm Design and Analysis.
Benjamin Cummings/ Addison Wesley, Redwod
City, 1994.
B. Mobasher, N. Jain, E.H. Hau, and J. Sri-
vaatava. Web mining: Pattern discovery from
world wide web transact ions. Technical Report
TR-96-050, Department of Computer Science,
University of Minnesota, M inneapolis, 1996.
H. Mannila, H. Toivonen, and A. I. Verkamo.
Discovering frequent episodes in sequences. In
Proc. of the First Int’1 Conference on Knowl-
edge Discovery and Data Mining, pages 210-215,
Montreal, Quebec, 1995.
IBM Quest Data Mining Project. Quest syn-
thetic data generation code.
http://www.almaden. ibm. comlcslquestjsyndata. html,
1996.
C. H. Papadimitriou and K. Steiglitz. Combina-
torial Optimization: Algorithms and Complex-
ity. Prentice-Hall, Englewood Cliffsl NJ, 1982.
[SA95] R. Srikant and R. Agrawal. Mining generalized
association rules. In Proc. of the 21st VLDB
Conference, pagea 407-419, Zurich, Switzerland,
1995.
[SA96] R. Srikant and R. Agrawal. Mining sequential
patterns: Generalizations and performance im-
provements. In Proc. of the Fifth Int’1 Con-
ference on Extending Database Technology, Avi-
gnon, France, 1996.
[SAD+ 93] M. Stonebraker, R. Agrawal, U. Dayal, E. J.
Neuhold, and A. Reuter. DBMS research at a
crossroads: The vienna update. In Proc. of the
19th VLDB Conference, pages 688-692, Dublin,
Ireland, 1993.
[SON95] A. Savasere, E. Omiecinski, and S. Navathe. An
efficient algorithm for mining association rules
in large databases. In Proc. of the 21st VLDB
Conference, pages 432-443, Zurich, Switzerland,
1995.
288
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-10\kdd10.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
Lecture 10 - Discovery of Association 
Rules
 Basic Concepts
 The Apriori Algorithm for mining of (single-
dimensional Boolean) association rules in 
transactional databases
 Visualization of Association Rules 
 Improving the Efficiency of Apriori 
Lecture No. 10 1
Data Mining (BGU) Prof. Mark Last
Motivating Examples
Lecture No. 10 2
Market Basket Analysis
Data Mining (BGU) Prof. Mark Last
What Is Association Rule Mining?
 Motivation: finding regularities in data
 What products were often purchased together? —
Beer and diapers?!
 What are the subsequent purchases after buying a 
PC?
 What kinds of DNA are sensitive to this new drug?
 Can we automatically classify web documents?
 Association rule mining:
Finding frequent patterns, associations, correlations, 
or causal structures among sets of items or objects 
in transaction databases, relational databases, and 
other information repositories.
 Frequent pattern: pattern (set of items, sequence, 
etc.) that occurs frequently in a database [AIS93]
Lecture No. 10 3
Data Mining (BGU) Prof. Mark Last
Why Is Frequent Pattern or Association 
Mining an Essential Task in Data Mining?
 Foundation for many essential data mining tasks
 Association, correlation, causality
 Sequential patterns, temporal or cyclic association, partial 
periodicity, spatial and multimedia association
 Associative classification, cluster analysis, iceberg cube, 
fascicles (semantic data compression based on similar 
attribute values)
 Broad applications
 Basket data analysis, cross-marketing, catalog design, sale 
campaign analysis
 Web log (click stream) analysis, DNA sequence analysis, etc.
Lecture No. 10 4
Data Mining (BGU) Prof. Mark Last
Basic Concepts: Frequent Patterns and 
Association Rules
Lecture No. 10 5
 Itemset T = {x1, …, xk}
 X  T, Y  T
 Find all the rules XY with min 
confidence and support
 Support (XY) , s, probability that 
a transaction contains the union of 
X and Y
 P(XY)
 Confidence (XY) , c, conditional 
probability that a transaction 
having X also contains Y
 P(Y / X)
Customer
buys diaper
Customer
buys both
Customer
buys beer
Transaction-id Items bought
10 A, B, C
20 A, C
30 A, D
40 B, E, F
Data Mining (BGU) Prof. Mark Last
Mining Association Rules—an Example
Lecture No. 10 6
For rule A  C:
support = support({A}{C}) = 50%
confidence = support({A}{C})/support({A}) 
= 66.6%
Min. support 50%
Min. confidence 50%Transaction-id Items bought
10 A, B, C
20 A, C
30 A, D
40 B, E, F
Frequent pattern Support
{A} 75%
{B} 50%
{C} 50%
{A, C} 50%
Data Mining (BGU) Prof. Mark Last
Mining Association Rules – Main Steps
 Find all frequent itemsets
 Each itemset will occur in at least min_sup
database transactions
 Generate strong association rules from the 
frequent itemsets
 These rules must satisfy min_sup and 
min_conf
Lecture No. 10 7
Data Mining (BGU) Prof. Mark Last
Closed Patterns and Max-Patterns
 A long pattern contains a combinatorial number of sub-
patterns, e.g., {a1, …, a100} contains (100
1) + (100
2) + … + 
(1
1
0
0
0
0) = 2100 – 1  1.27*1030 sub-patterns!
 Amazon.com: 1,800,000 book titles
 Solution: Mine closed patterns and max-patterns instead
 An itemset X is closed if X is frequent and there exists no 
super-pattern Y כ X, with the same support as X 
 An itemset X is a max-pattern if X is frequent and there 
exists no frequent super-pattern Y כ X
 Closed pattern is a lossless compression of freq. patterns
 Reducing the # of patterns and rules
Lecture No. 10 8
Data Mining (BGU) Prof. Mark Last
9
Closed Patterns and Max-Patterns
 Exercise.  DB = {<a1, …, a100>, < a1, …, a50>} 
 Min_sup = 1.
 What is the set of closed itemsets?
 <a1, …, a100>: 1
 < a1, …, a50>: 2
 What is the set of max-pattern?
 <a1, …, a100>: 1
 What is the set of all patterns?
 !!
Data Mining (BGU) Prof. Mark Last
10
Computational Complexity of Frequent 
Itemset Mining
 How many itemsets are potentially to be generated in the worst case?
 The number of frequent itemsets to be generated is sensitive to the 
minsup threshold
 When minsup is low, there exist potentially an exponential number of 
frequent itemsets
 The worst case: MN where M: # distinct items, and N: max length of 
transactions
 The worst case complexty vs. the expected probability
 Ex. Suppose Walmart has 104 kinds of products 
 The chance to pick up one product 10-4
 The chance to pick up a particular set of 10 products: ~10-40
 What is the chance this particular set of 10 products to be frequent 
103 times in 109 transactions?
Data Mining (BGU) Prof. Mark Last
Lecture 10 - Discovery of Association 
Rules
 Basic Concepts
 The Apriori Algorithm for mining of (single-
dimensional Boolean) association rules in 
transactional databases
 Visualization of Association Rules 
 Improving the Efficiency of Apriori 
Lecture No. 10 11
Data Mining (BGU) Prof. Mark Last
Apriori: A Candidate Generation-and-
test Approach
 Any subset of a frequent itemset must be frequent
 if {beer, diaper, nuts} is frequent, so is {beer, diaper}
 Every transaction having {beer, diaper, nuts} also contains {beer, 
diaper} 
 Apriori pruning principle: If there is any itemset which is 
infrequent, its superset should not be generated/tested!
 Method: 
 Generate length (k+1) candidate itemsets from length k
frequent itemsets by joining them with themselves
 Prune k-itemsets containing infrequent (k-1)-itemsets
 Test the remaining candidates against DB
 The performance studies show its efficiency and scalability
Lecture No. 10 12
Data Mining (BGU) Prof. Mark Last
The Apriori Algorithm — An Example
Lecture No. 10 13
Database TDB
1st scan
C1
L1
L2
C2 C2
2nd scan
C3 L33rd scan
Tid Items
10 A, C, D
20 B, C, E
30 A, B, C, E
40 B, E
Itemset sup
{A} 2
{B} 3
{C} 3
{D} 1
{E} 3
Itemset sup
{A} 2
{B} 3
{C} 3
{E} 3
Itemset
{A, B}
{A, C}
{A, E}
{B, C}
{B, E}
{C, E}
Itemset sup
{A, B} 1
{A, C} 2
{A, E} 1
{B, C} 2
{B, E} 3
{C, E} 2
Itemset sup
{A, C} 2
{B, C} 2
{B, E} 3
{C, E} 2
Itemset
{B, C, E}
Itemset sup
{B, C, E} 2
Min. support 50%
Data Mining (BGU) Prof. Mark Last
The Apriori Algorithm Outline
 Pseudo-code:
Ck: Candidate itemset of size k
Lk : frequent itemset of size k
Input: D, a database of transactions; min_sup
Output: L, frequent itemsets in D
L1 = {frequent items};
for (k = 1; Lk !=; k++) do begin
Ck+1 = candidates generated from Lk;
for each transaction t in database do
increment the count of all candidates in Ck+1
that are contained in t
Lk+1 = candidates in Ck+1 with min_support
end
return L = k Lk;
Lecture No. 10 14
Data Mining (BGU) Prof. Mark Last
Important Details of Apriori
 How to generate candidates?
 Step 1: self-joining Lk
 Step 2: pruning
 How to count supports of candidates?
 Example of Candidate-generation
 L3={abc, abd, acd, ace, bcd}
 Self-joining: L3*L3
 abcd from abc and abd
 acde from acd and ace
 Pruning:
 acde is removed because ade is not in L3
 C4={abcd}
Lecture No. 10 15
Data Mining (BGU) Prof. Mark Last
How to Generate Candidates?
 Suppose the items in Lk-1 are listed in a lexicographic 
order
 Step 1: self-joining Lk-1
insert into Ck
select p.item1, p.item2, …, p.itemk-1, q.itemk-1
from Lk-1 p, Lk-1 q
where p.item1=q.item1, …, p.itemk-2=q.itemk-2, p.itemk-1 < 
q.itemk-1 //First k - 2 items are identical
 Step 2: pruning
forall itemsets c in Ck do
forall (k-1)-subsets s of c do
if (s is not in Lk-1) then delete c from Ck
Lecture No. 10 16
Data Mining (BGU) Prof. Mark Last
How to Count Supports of 
Candidates?
 Why counting supports of candidates a 
problem?
 The total number of candidates can be very huge
 One transaction may contain many candidates
 Method:
 Candidate itemsets are stored in a hash-tree
 Leaf node of hash-tree contains a list of itemsets and 
counts
 Interior node contains a hash table
 Subset function: finds all the candidates contained in 
a transaction
Lecture No. 10 17
Data Mining (BGU) Prof. Mark Last
24
Further Improvement of the Apriori Method
 Major computational challenges
 Multiple scans of transaction database
 Huge number of candidates
 Tedious workload of support counting for candidates
 Improving Apriori: general ideas
 Reduce passes of transaction database scans
 Shrink number of candidates
 Facilitate support counting of candidates
Data Mining (BGU) Prof. Mark Last
Interestingness Measure: Correlations 
(Lift)
 play basketball  eat cereal [40%, 66.7%]  is misleading – why?
 The overall % of students eating cereal is 75% > 66.7%.
 play basketball  not eat cereal [20%, 33.3%] is more accurate, 
although with lower support and confidence
 Measure of dependent/correlated events: lift
89.0
5000/3750*5000/3000
5000/2000
),( CBlift
Basketball Not basketball Sum (row)
Cereal 2000 1750 3750
Not cereal 1000 250 1250
Sum(col.) 3000 2000 5000
)()(
)(
BPAP
BAP
lift


33.1
5000/1250*5000/3000
5000/1000
),( CBlift
Data Mining (BGU) Prof. Mark Last
Mining Association Rules in Temporal 
Databases
 Episode rule - applied to sequences of events, each event occurring at a 
particular time. An episode is a sequence of events in a partial order and 
an episode rule is defined as A → B, where A and B are episodes and A is 
a subepisode of B. Can be used for predicting certain events by sequences 
of earlier events ("if the Microsoft stock price goes up, then IBM goes up 
the next day")
 Trend dependencies - a ruleA→B, where A and B are patterns of the form 
(A, Θ), where A is a reference to a specific attribute and Θ is an element 
in {<,=,>,≥ , ≤ , = }. Can be used to discover patterns like: an 
employee’s salary always increases over time.
 Sequence rules - {AB, A} → {C, A}, where A and B appears at the same 
time followed by A, implies a sequence where C is followed by A.
 Calendric rules - predefined time unit and a calendar, given by a set of 
time intervals, are needed. Rules A→B restricted to the chosen calendar, 
which allows for seasonal changes to be analyzed.  Example: sales are up 
before Xmas.
 Intertransaction rules, Interval AR, etc.  
Data Mining (BGU) Prof. Mark Last
Lecture 10 - Discovery of Association 
Rules
 Basic Concepts
 The Apriori Algorithm for mining of (single-
dimensional Boolean) association rules in 
transactional databases
 Visualization of Association Rules 
 Improving the Efficiency of Apriori 
Lecture No. 10 27
Data Mining (BGU) Prof. Mark Last
June 12, 2017 Lecture No. 10 (based on Han & Kamber) 28
Visualization of Association Rules: 
Plane Graph
Data Mining (BGU) Prof. Mark Last
June 12, 2017 Lecture No. 10 (based on Han & Kamber) 29
Visualization of Association Rules: 
A Ball Graph
Active 
node
(volume = 
support)
Rule implication
Data Mining (BGU) Prof. Mark Last
Lesson 9 - Discovery of Association 
Rules
 Basic Concepts
 The Apriori Algorithm for mining of (single-
dimensional Boolean) association rules in 
transactional databases
 Visualization of Association Rules
 Improving the Efficiency of Apriori 
Lecture No. 10 30
Data Mining (BGU) Prof. Mark Last
Challenges of Frequent Pattern Mining
 Challenges
Multiple scans of transaction database
Huge number of candidates
Tedious workload of support counting for 
candidates
 Improving Apriori: general ideas
Reduce passes of transaction database scans
 Shrink number of candidates
 Facilitate support counting of candidates
Lecture No. 10 31
Data Mining (BGU) Prof. Mark Last
Partition: Scan Database Only Twice
 Any itemset that is potentially frequent in DB must be 
frequent in at least one of the partitions of DB
 Scan 1: partition database and find local frequent 
patterns
 Scan 2: consolidate global frequent patterns
 A. Savasere, E. Omiecinski, and S. Navathe. An efficient 
algorithm for mining association in large databases. In 
VLDB’95
 www: http://www.acm.org/sigmod/vldb/conf/1995/P432.PDF
Lecture No. 10 32
Data Mining (BGU) Prof. Mark Last
Bottleneck of Frequent-pattern Mining
 Multiple database scans are costly
 Mining long patterns needs many passes of 
scanning and generates lots of candidates
 To find frequent itemset i1i2…i100
 # of scans: 100
 # of Candidates: (100
1) + (100
2) + … + 
(1
1
0
0
0
0) = 2100-1  1.27*1030 !
 Bottleneck: candidate-generation-and-test
 Can we avoid candidate generation?
Lecture No. 10 33
Data Mining (BGU) Prof. Mark Last
Mining Frequent Patterns Without 
Candidate Generation
 Grow long patterns from short ones using local frequent 
items
 “abc” is a frequent pattern
 Get all transactions having “abc”: DB|abc
 “d” is a local frequent item in DB|abc  abcd is a potentially
frequent pattern
 “Divide-and-conquer” strategy
 Compress the database into a frequent-pattern (FP) tree
 Divide a compressed database into a set of conditional 
databases, each associated with one frequent item
 Mine each database separately
Lecture No. 10 34
Data Mining (BGU) Prof. Mark Last
Mining Frequent Patterns With FP-trees
 Idea: Frequent pattern growth
 Recursively grow frequent patterns by pattern and 
database partition
 Method 
 For each frequent item, construct its conditional 
pattern-base, and then its conditional FP-tree
 Repeat the process on each newly created 
conditional FP-tree 
 Until the resulting FP-tree is empty, or it contains 
only one path—single path will generate all the 
combinations of its sub-paths, each of which is a 
frequent pattern
Lecture No. 10 42
Data Mining (BGU) Prof. Mark Last
FP-Growth vs. Apriori: Scalability With the 
Support Threshold
Lecture No. 10 43
0
10
20
30
40
50
60
70
80
90
100
0 0.5 1 1.5 2 2.5 3
Support threshold(%)
R
u
n
 t
im
e
(s
e
c.
)
D1 FP-grow th runtime
D1 Apriori runtime
Data set T25I20D10K
Source: J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. 
In Proceedings of the 2000 ACM SIGMOD International Conference on Management of 
Data, Dallas, TX, May 2000.
Data Mining (BGU) Prof. Mark Last
Why Is FP-Growth the Winner?
 Divide-and-conquer: 
 decompose both the mining task and DB according 
to the frequent patterns obtained so far
 leads to focused search of smaller databases
 Other factors
 no candidate generation, no candidate test
 compressed database: FP-tree structure
 no repeated scan of entire database 
 basic ops—counting local freq items and building 
sub FP-tree, no pattern search and matching
Lecture No. 10 44
Data Mining (BGU) Prof. Mark Last
Lecture No. 10 45
Summary
 Frequent pattern mining—an important task in 
data mining
 Scalable frequent pattern mining methods
 Apriori (Candidate generation & test)
 Projection-based (FPgrowth, CLOSET+, ...)
 Vertical format approach (CHARM, ...)
 Which patterns are interesting? 
 Pattern evaluation methods
45
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-11\kdd11.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 1
Lesson 11. Cluster Analysis
 What is Cluster Analysis?
 Types of Data in Cluster Analysis
 A Categorization of Major Clustering Methods
 Partitioning Methods
 Hierarchical Methods
 Density-Based Methods
 Grid-Based Methods
 Model-Based Clustering Methods
 Outlier Analysis
 Summary 
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 2
What is Cluster Analysis?
 Cluster: a collection of data objects
 Similar to one another within the same cluster
 Dissimilar to the objects in other clusters
 Cluster analysis
 Grouping a set of data objects into clusters
 Clustering is unsupervised classification: no predefined 
classes
 Typical applications
 As a stand-alone tool to get insight into data 
distribution 
 As a preprocessing step for other algorithms
Data Mining (BGU) Prof. Mark Last
Examples of Clustering Applications
 Information retrieval: document 
clustering
 Land use: Identification of areas of 
similar land use in an earth 
observation database
 Marketing: Develop targeted 
marketing programs
 City planning: Identifying groups of 
houses according to their house type, 
value, and geographical location
 Earthquake studies: Observe 
earthquake epicenters
 Climate: understanding earth 
climate, find patterns of atmospheric 
and ocean
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 3
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 4
What Is Good Clustering?
 A good clustering method will produce high quality 
clusters with
 high intra-class similarity
 low inter-class similarity 
 The quality of a clustering result depends on both the 
similarity measure used by the method and its 
implementation.
 The quality of a clustering method is also measured by 
its ability to discover some or all of the hidden patterns.
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 5
Requirements of Clustering in Data 
Mining 
 Scalability (Big Data)
 Ability to deal with different types of attributes
 Discovery of clusters with arbitrary shape
 Minimal requirements for domain knowledge to 
determine input parameters
 Able to deal with noise and outliers
 Incremental clustering and insensitivity to input order
 High dimensionality
 Incorporation of user-specified constraints
 Interpretability and usability
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 6
Lesson 12. Cluster Analysis
 What is Cluster Analysis?
 Types of Data in Cluster Analysis
 A Categorization of Major Clustering Methods
 Partitioning Methods
 Hierarchical Methods
 Density-Based Methods
 Grid-Based Methods
 Model-Based Clustering Methods
 Outlier Analysis
 Summary 
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 7
Data Structures
 Data matrix
 p – number of variables
 n – number of objects
 xif - value of variable i in record f
 Dissimilarity matrix
 d (i,j) – distance between 
objects i and j


















npx...nfx...n1x
...............
ipx...ifx...i1x
...............
1px...1fx...11x
















0...)2,()1,(
:::
)2,3()
...ndnd
0dd(3,1
0d(2,1)
0
Data Mining (BGU) Prof. Mark Last
8
Attribute Types in Clustering Analysis
 Nominal: categories, states, or “names of things”
 Hair_color = {auburn, black, blond, brown, grey, red, white}
 marital status, occupation, ID numbers, zip codes
 Binary
 Nominal attribute with only 2 states (0 and 1)
 Symmetric binary: both outcomes equally important
 e.g., gender
 Asymmetric binary: outcomes not equally important.  
 e.g., medical test (positive vs. negative), TF
 Convention: assign 1 to most important outcome (e.g., 
HIV positive)
 Ordinal
 Values have a meaningful order (ranking) but magnitude 
between successive values is not known.
 Size = {small, medium, large}, grades, army rankings
Data Mining (BGU) Prof. Mark Last
9
Numeric Attribute Types
 Quantity (integer or real-valued)
 Interval
 Measured on a scale of equal-sized units
 Values have order
 E.g., temperature in C˚or F˚, calendar dates
 No true zero-point
 Ratio
 Inherent zero-point
 We can speak of values as being an order of 
magnitude larger than the unit of measurement 
(10 K˚ is twice as high as 5 K˚).
 e.g., temperature in Kelvin, length, counts, 
monetary quantities
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 10
Interval-scaled variables
 Goal: give all variables (age (years), height (cm), etc.) 
an equal weight
 Standardize data (n – number of objects)
 Calculate the mean absolute deviation for variable f:
 Where the mean value of f is
 Calculate the standardized measurement (z-score)
 Using mean absolute deviation is more robust than 
using standard deviation, since deviations are not 
squared 
.)...
21
1
nffff
xx(xn m 
|)|...|||(|1
21 fnffffff
mxmxmxns 
f
fif
if s
mx
 z


Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 11
Similarity and Dissimilarity Between Objects
 Distances are normally used to measure the similarity or 
dissimilarity between two data objects
 Requirements
 Non-negativity: d(i,j)  0
 Distance to itself: d(i,i) = 0
 Symmetry: d(i,j) = d(j,i)
 Triangular inequality: d(i,j)  d(i,k) + d(k,j)
 Some popular ones include: Minkowski distance:
 where  i = (xi1, xi2, …, xip) and j = (xj1, xj2, …, xjp) are two p-
dimensional data objects, and q is a positive integer
q
q
pp
qq
j
x
i
x
j
x
i
x
j
x
i
xjid )||...|||(|),(
2211

Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 12
Similarity and Dissimilarity Between Objects (Cont.)
 If q = 1, d is Manhattan distance
 If q = 2, d is Euclidean distance:
 Other dissimilarity measures are available 
)||...|||(|),( 22
22
2
11 pp j
x
i
x
j
x
i
x
j
x
i
xjid 
||...||||),(
2211 pp j
x
i
x
j
x
i
x
j
x
i
xjid 
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 13
Binary Variables
 A contingency table for binary 
data
 p – number of variables
 Distance measure for symmetric
binary variables: 
 Distance measure for 
asymmetric binary variables: 
 Jaccard coefficient (similarity
measure for asymmetric binary 
variables): 
dcba
cb jid

),(
cba
cb jid

),(
pdbcasum
dcdc
baba
sum



0
1
01
Object i
Object  j
cba
a jisim
Jaccard 
),(
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 14
Dissimilarity between Binary Variables
 Example
 gender is a symmetric attribute
 the remaining attributes are asymmetric binary
 let the values Y and P be set to 1, and the value N be set to 0
Name Gender Fever Cough Test-1 Test-2 Test-3 Test-4
Jack M Y N P N N N
Mary F Y N P N P N
Jim M Y P N N N N
75.0
211
21
),(
67.0
111
11
),(
33.0
102
10
),(












maryjimd
jimjackd
maryjackd
642
3300
3121
01
sum
sum
Mary
Jack
cba
cb jid

),(
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 15
Nominal Variables
 A generalization of the binary variable in that it can take 
more than 2 states, e.g., red, yellow, blue, green
 Method 1: Simple matching
 m: # of matches, p: total # of variables
 Method 2: use a large number of binary variables
 creating a new binary variable for each of the M
nominal states
p
mp
jid

),(
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 16
Ordinal Variables
 An ordinal variable can be discrete or continuous
 Order is important, e.g., rank
 Can be treated like interval-scaled 
 replace xif by their rank 
 map the range of each variable onto [0, 1] by replacing
i-th object in the f-th variable by
 Example: academic ranks in Israel
 compute the dissimilarity using methods for interval-
scaled variables
1
1



f
if
if M
r
z
},...,1{
fif
Mr 
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 17
Ratio-Scaled Variables
 Ratio-scaled variable: a positive measurement on a 
nonlinear scale, approximately at exponential scale, such 
as AeBt or Ae-Bt
 Examples: salaries, web links
 Methods:
 treat them like interval-scaled variables—not a good 
choice! (why?—the scale can be distorted)
 apply logarithmic transformation
yif = log(xif)
 treat them as continuous ordinal data treat their rank 
as interval-scaled
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 18
Variables of Mixed Types
 A database may contain all the six types of variables
 symmetric binary, asymmetric binary, nominal, ordinal, interval and ratio
 One may use a weighted formula to combine their effects
 ij(f) – binary indicator (ij(f) = 0 if a variable f should be skipped)
 dij
(f) – contribution of variable f to dissimilarity between i and j
 Variable f is binary or nominal:
dij
(f) = 0  if xif = xjf , or dij
(f) = 1 otherwise
 Variable f is interval-based: use the normalized distance
 Variable f is ordinal or ratio-scaled
 compute ranks rif and  
 and treat zif as interval-scaled
)(
1
)()(
1),(
f
ij
p
f
f
ij
f
ij
p
f
d
jid







1
1



f
if
M
r
z
if
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 19
Vector Objects
 Vector objects: keywords in documents, gene 
features in micro-arrays, etc.
 Broad applications: information retrieval, biologic 
taxonomy, etc.
 Cosine measure
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 20
Lesson 12. Cluster Analysis
 What is Cluster Analysis?
 Types of Data in Cluster Analysis
 A Categorization of Major Clustering Methods
 Partitioning Methods
 Hierarchical Methods
 Density-Based Methods
 Grid-Based Methods
 Model-Based Clustering Methods
 Outlier Analysis
 Summary 
Data Mining (BGU) Prof. Mark Last
Major Clustering Approaches (I)
 Partitioning approach: 
 Construct various partitions and then evaluate them by some 
criterion, e.g., minimizing the sum of square errors
 Typical methods: k-means, k-medoids, CLARANS
 Hierarchical approach: 
 Create a hierarchical decomposition of the set of data (or objects) 
using some criterion
 Typical methods: Diana, Agnes, BIRCH, CAMELEON
 Density-based approach: 
 Based on connectivity and density functions
 Typical methods: DBSACN, OPTICS, DenClue
 Grid-based approach: 
 based on a multiple-level granularity structure
 Typical methods: STING, WaveCluster, CLIQUE
21
Data Mining (BGU) Prof. Mark Last
Major Clustering Approaches (II)
 Model-based: 
 A model is hypothesized for each of the clusters and tries to find 
the best fit of that model to each other
 Typical methods: EM, SOM, COBWEB
 Frequent pattern-based:
 Based on the analysis of frequent patterns
 Typical methods: p-Cluster
 User-guided or constraint-based: 
 Clustering by considering user-specified or application-specific 
constraints
 Typical methods: COD (obstacles), constrained clustering
 Link-based clustering:
 Objects are often linked together in various ways
 Massive links can be used to cluster objects: SimRank, LinkClus
22
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 23
Typical Alternatives to Calculate the 
Distance between Clusters
 Single link:  smallest distance between an element in one cluster and an 
element in the other, i.e.,  dis(Ki, Kj) = min(tip, tjq)
 Complete link: largest distance between an element in one cluster and an 
element in the other, i.e.,  dis(Ki, Kj) = max(tip, tjq)
 Average: avg distance between an element in one cluster and an element in 
the other, i.e.,  dis(Ki, Kj) = avg(tip, tjq)
 Centroid: distance between the centroids of two clusters, i.e.,  dis(Ki, Kj) = 
dis(Ci, Cj)
 Medoid: distance between the medoids of two clusters, i.e.,  dis(Ki, Kj) = 
dis(Mi, Mj)
 Medoid: one chosen, centrally located object in the cluster
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 24
Centroid, Radius and Diameter of a 
Cluster (for numerical data sets)
 Centroid:  the “middle” of a cluster
 Radius: square root of average distance from any 
point of the cluster to its centroid
 Diameter: square root of average mean squared 
distance between all pairs of points in the cluster
N
tN
i ip
mC
)(
1


N
mcip
tN
i
mR
2)(
1




)1(
2)(
11







NN
iq
t
ip
tN
i
N
i
mD
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 25
Lesson 12. Cluster Analysis
 What is Cluster Analysis?
 Types of Data in Cluster Analysis
 A Categorization of Major Clustering Methods
 Partitioning Methods
 Hierarchical Methods
 Density-Based Methods
 Grid-Based Methods
 Model-Based Clustering Methods
 Outlier Analysis
 Summary 
Data Mining (BGU) Prof. Mark Last
Partitioning Algorithms: Basic Concept
 Partitioning method: Partitioning a database D of n objects into a set 
of k clusters, such that the sum of squared distances is minimized 
(where ci is the centroid or medoid of cluster Ci)
 Given k, find a partition of k clusters that optimizes the chosen 
partitioning criterion
 Global optimal: exhaustively enumerate all partitions
 Heuristic methods: k-means and k-medoids algorithms
 k-means (MacQueen’67, Lloyd’57/’82): Each cluster is represented 
by the center of the cluster
 k-medoids or PAM (Partition around medoids) (Kaufman & 
Rousseeuw’87): Each cluster is represented by one of the objects 
in the cluster  
2
1 )( iCp
k
i cpE i  
26
Data Mining (BGU) Prof. Mark Last
The K-Means Clustering Method
 Given k, the k-means algorithm is implemented in 
four steps:
 Partition objects into k nonempty subsets
 Compute seed points as the centroids of the 
clusters of the current partitioning (the centroid is 
the center, i.e., mean point, of the cluster)
 Assign each object to the cluster with the nearest 
seed point  
 Go back to Step 2, stop when the assignment does 
not change
27
Data Mining (BGU) Prof. Mark Last
An Example of K-Means Clustering
K=2
Arbitrarily 
partition 
objects into 
k groups
Update the 
cluster 
centroids
Update the 
cluster 
centroids
Reassign  objectsLoop if 
needed
28
The initial data set
 Partition objects into k nonempty 
subsets
 Repeat
 Compute centroid (i.e., mean 
point) for each partition 
 Assign each object to the 
cluster of its nearest centroid  
 Until no change
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 29
K-Means Example (k = 2)
Iteration 1
Cluster 1
Cluster 2
Objects Re-
assignment
Rec No X1 X2 X3 X4 X5
1 0 0 0 0 0
2 0 0 0 0 0
3 1 1 1 1 1
Mean 0.33 0.33 0.33 0.33 0.33
Rec No X1 X2 X3 X4 X5
4 1 0 1 0 1
5 0 1 0 1 0
6 1 0 1 1 1
7 1 1 1 1 1
Mean 0.75 0.5 0.75 0.75 0.75
Rec No Old to Cluster 1 to Cluster 2 Min New
1 1 0.745 1.581 0.745 1
2 1 0.745 1.581 0.745 1
3 1 1.491 0.707 0.707 2
4 2 1.247 1.000 1.000 2
5 2 1.106 1.414 1.106 1
6 2 1.374 0.707 0.707 2
7 2 1.491 0.707 0.707 2
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 30
K-Means Example (k = 2)
Iteration 2
Cluster 1
Cluster 2
Objects Re-
assignment
Rec No X1 X2 X3 X4 X5
1 0 0 0 0 0
2 0 0 0 0 0
5 0 1 0 1 0
Mean 0.00 0.33 0.00 0.33 0.00
Rec No X1 X2 X3 X4 X5
3 1 1 1 1 1
4 1 0 1 0 1
6 1 0 1 1 1
7 1 1 1 1 1
Mean 1 0.5 1 0.75 1
Rec No Old to Cluster 1 to Cluster 2 Min New
1 1 0.471 1.953 0.471 1
2 1 0.471 1.953 0.471 1
3 2 1.972 0.559 0.559 2
4 2 1.795 0.901 0.901 2
5 1 0.943 1.820 0.943 1
6 2 1.886 0.559 0.559 2
7 2 1.972 0.559 0.559 2
Data Mining (BGU) Prof. Mark Last
Comments on the K-Means Method
 Strength: Efficient: O(tkn), where n is # objects, k is # clusters, and t  
is # iterations. Normally, k, t << n.
 Comparing: PAM: O(k(n-k)2 ), CLARA: O(ks2 + k(n-k))
 Comment: Often terminates at a local optimal. 
 Weakness
 Applicable only to objects in a continuous n-dimensional space 
 Using the k-modes method for categorical data
 In comparison, k-medoids can be applied to a wide range of 
data
 Need to specify k, the number of clusters, in advance (there are 
ways to automatically determine the best k (see Hastie et al., 
2009)
 Sensitive to noisy data and outliers
 Not suitable to discover clusters with non-convex shapes
31
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 32
Variations of the K-Means Method
 A few variants of the k-means which differ in
 Selection of the initial k means
 Dissimilarity calculations
 Strategies to calculate cluster means
 Handling categorical data: k-modes (Huang’98)
 Replacing means of clusters with modes
 Using new dissimilarity measures to deal with categorical objects
 Using a frequency-based method to update modes of clusters
 A mixture of categorical and numerical data: k-prototype method
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 33
What is the problem of k-Means Method?
 The k-means algorithm is sensitive to outliers !
 Since an object with an extremely large value may substantially 
distort the distribution of the data.
 K-Medoids:  Instead of taking the mean value of the object in a 
cluster as a reference point, medoids can be used, which is the most 
centrally located object in a cluster. 
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Data Mining (BGU) Prof. Mark Last
34
PAM: A Typical K-Medoids Algorithm
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Total Cost = 20
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
K=2
Arbitrary 
choose k
objects 
as initial 
medoids
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Assign 
each 
remaining 
object to 
nearest 
medoids
Randomly select a 
nonmedoid object,Orandom
Compute 
total cost of 
swapping
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Total Cost = 26
Swapping O 
and Orandom
If quality is 
improved.
Do loop
Until no 
change
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Data Mining (BGU) Prof. Mark Last
The K-Medoid Clustering Method
 K-Medoids Clustering: Find representative objects (medoids) in clusters
 PAM (Partitioning Around Medoids, Kaufmann & Rousseeuw 1987)
 Starts from an initial set of medoids and iteratively replaces one 
of the medoids by one of the randomly selected non-medoids if 
it decreases the total distance of the resulting clustering
 Each object is assigned to a cluster represented by the nearest medoid
 PAM works effectively for small data sets, but does not scale 
well for large data sets (due to the computational complexity)
 Efficiency improvement on PAM
 CLARA (Kaufmann & Rousseeuw, 1990): PAM on samples
 CLARANS (Ng & Han, 1994): Randomized re-sampling
35
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 36
Lesson 12. Cluster Analysis
 What is Cluster Analysis?
 Types of Data in Cluster Analysis
 A Categorization of Major Clustering Methods
 Partitioning Methods
 Hierarchical Methods
 Density-Based Methods
 Grid-Based Methods
 Model-Based Clustering Methods
 Outlier Analysis
 Summary 
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 37
Hierarchical Clustering
 Use distance matrix as clustering criteria.  This method 
does not require the number of clusters k as an input, 
but needs a termination condition 
Step 0 Step 1 Step 2 Step 3 Step 4
b
d
c
e
a
a b
d e
c d e
a b c d e
Step 4 Step 3 Step 2 Step 1 Step 0
agglomerative
(AGNES)
divisive
(DIANA)
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 38
AGNES (Agglomerative Nesting)
 Introduced in Kaufmann and Rousseeuw (1990)
 Implemented in statistical analysis packages, e.g., Splus
 Use the Single-Link method and the dissimilarity matrix.  
 Merge nodes that have the least dissimilarity
 Go on in a non-descending fashion
 Eventually all nodes belong to the same cluster
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
0
1
2
3
4
5
6
7
8
9
10
0 1 2 3 4 5 6 7 8 9 10
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 39
Dendrogram: Shows How the Clusters are Merged
Decompose data objects into a several levels of nested 
partitioning (tree of clusters), called a dendrogram. 
A clustering of the data objects is obtained by cutting the 
dendrogram at the desired level, then each connected 
component forms a cluster.
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 40
Agglomerative Clustering Example
Rec No X1 X2 X3 X4 X5
1 0 0 0 0 0
2 0 0 0 0 0
3 1 1 1 1 1
4 1 0 1 0 1
5 0 1 0 1 0
6 1 0 1 1 1
7 1 1 1 1 1
Data Objects:
Dissimilarity 
Matrix 
(Euclidean 
Distances):
Rec No Dist1 Dist2 Dist3 Dist4 Dist5 Dist6 Dist7
1
2 0.000
3 2.236 2.236
4 1.732 1.732 1.414
5 1.414 1.414 1.732 2.236
6 2.000 2.000 1.000 1.000 2.000
7 2.236 2.236 0.000 1.414 1.732 1.000
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 41
Agglomerative Clustering Example (cont.)
1 2 5 43 7 6
0.000 0.000 1.000
1.0001.414
D
is
ta
n
ce
0.000
1.000
1.414
L
ev
el
1
2
3
0
4
1.732
1.732
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 42
More on Hierarchical Clustering Methods
 Major weakness of agglomerative clustering methods
 do not scale well: time complexity of at least O(n2), 
where n is the number of total objects
 can never undo what was done previously
 Integration of hierarchical with distance-based clustering
 BIRCH (1996): uses CF-tree and incrementally adjusts 
the quality of sub-clusters
 CURE (1998): selects well-scattered points from the 
cluster and then shrinks them towards the center of the 
cluster by a specified fraction
 CHAMELEON (1999): hierarchical clustering using 
dynamic modeling
Data Mining (BGU) Prof. Mark Last
June 19, 2017 Lecture No. 11 (based on Han & Kamber) 43
Summary
 Cluster analysis groups objects based on their similarity
and has wide applications
 Measure of similarity can be computed for various types 
of data
 Clustering algorithms can be categorized into partitioning 
methods, hierarchical methods, density-based methods, 
grid-based methods, and model-based methods
 Outlier detection and analysis are very useful for fraud 
detection, etc. and can be performed by statistical, 
distance-based or deviation-based approaches
 There are still lots of research issues on cluster analysis, 
such as efficient clustering in big data environment
Data Mining (BGU) Prof. Mark Last
References (1)
 R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan. Automatic subspace 
clustering of high dimensional data for data mining applications. SIGMOD'98
 M. R. Anderberg. Cluster Analysis for Applications. Academic Press, 1973.
 M. Ankerst, M. Breunig, H.-P. Kriegel, and J. Sander.  Optics: Ordering points 
to identify the clustering structure, SIGMOD’99.
 Beil F., Ester M., Xu X.: "Frequent Term-Based Text Clustering", KDD'02
 M. M. Breunig, H.-P. Kriegel, R. Ng, J. Sander. LOF: Identifying Density-Based 
Local Outliers. SIGMOD 2000.
 M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for 
discovering clusters in large spatial databases. KDD'96.
 M. Ester, H.-P. Kriegel, and X. Xu. Knowledge discovery in large spatial 
databases: Focusing techniques for efficient class identification. SSD'95.
 D. Fisher. Knowledge acquisition via incremental conceptual clustering. 
Machine Learning, 2:139-172, 1987.
 D. Gibson, J. Kleinberg, and P. Raghavan. Clustering categorical data: An 
approach based on dynamic systems. VLDB’98. 
 V. Ganti, J. Gehrke, R. Ramakrishan. CACTUS Clustering Categorical Data 
Using Summaries. KDD'99. 
44
Data Mining (BGU) Prof. Mark Last
References (2)
 D. Gibson, J. Kleinberg, and P. Raghavan. Clustering categorical data: An 
approach based on dynamic systems. In Proc. VLDB’98.
 S. Guha, R. Rastogi, and K. Shim. Cure: An efficient clustering algorithm for 
large databases. SIGMOD'98.
 S. Guha, R. Rastogi, and K. Shim. ROCK: A robust clustering algorithm for 
categorical attributes. In ICDE'99, pp. 512-521, Sydney, Australia, March 
1999. 
 A. Hinneburg, D.l A. Keim: An Efficient Approach to Clustering in Large 
Multimedia Databases with Noise. KDD’98.
 A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Printice Hall, 1988.
 G. Karypis, E.-H. Han, and V. Kumar. CHAMELEON: A Hierarchical Clustering 
Algorithm Using Dynamic Modeling. COMPUTER, 32(8): 68-75, 1999. 
 L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: an Introduction to 
Cluster Analysis. John Wiley & Sons, 1990.
 E. Knorr and R. Ng. Algorithms for mining distance-based outliers in large 
datasets. VLDB’98.
45
Data Mining (BGU) Prof. Mark Last
References (3)
 G. J. McLachlan and K.E. Bkasford. Mixture Models: Inference and Applications to 
Clustering. John Wiley and Sons, 1988.
 R. Ng and J. Han. Efficient and effective clustering method for spatial data mining. 
VLDB'94.
 L. Parsons, E. Haque and H. Liu, Subspace Clustering for High Dimensional Data: A 
Review, SIGKDD Explorations, 6(1), June 2004
 E. Schikuta. Grid clustering: An efficient hierarchical clustering method for very large 
data sets. Proc. 1996 Int. Conf. on Pattern Recognition
 G. Sheikholeslami, S. Chatterjee, and A. Zhang. WaveCluster: A multi-resolution 
clustering approach for very large spatial databases. VLDB’98.
 A. K. H. Tung, J. Han, L. V. S. Lakshmanan, and R. T. Ng. Constraint-Based Clustering 
in Large Databases, ICDT'01. 
 A. K. H. Tung, J. Hou, and J. Han. Spatial Clustering in the Presence of Obstacles, 
ICDE'01
 H. Wang, W. Wang, J. Yang, and P.S. Yu. Clustering by pattern similarity in large data 
sets, SIGMOD’02
 W. Wang, Yang, R. Muntz, STING: A Statistical Information grid Approach to Spatial 
Data Mining, VLDB’97
 T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH : An efficient data clustering method 
for very large databases. SIGMOD'96
 X. Yin, J. Han, and P. S. Yu, “LinkClus: Efficient Clustering via Heterogeneous Semantic 
Links”, VLDB'06
46
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-12\kdd12.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
1
Lecture No. 12 – Business Intelligence 
and Data Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
2
The Facts Gap
Year
Available Data 
(petabytes)
Critical 
Decisions
Available 
Knowledge
Analyzed 
Data
The FOUR V’s of Big Data
• Volume
• Variety
• Velocity
• Veracity
One petabyte = 
… movies ?
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
Example: Customer Relationship Management 
(CRM)
From: http://megaslides.com/doc/892947/facts-about-customer-relations
• Cost of selling to a new customer is six times as 
high as to existing customer
• Odds of selling to a new customer = 1/7 to an 
existing customer = 1/2
• Each dissatisfied customer tells 8 to 10 people
• 70% of dissatisfied customers will do business 
again if they feel their complains are handled well
• 1 extra % of customer retention can boost turnover 
by as much as 15%
3Lecture No. 12
Data Mining (BGU) Prof. Mark Last
Business Intelligence (BI)
• “The processes, technologies and tools needed 
to turn data into information and information 
into knowledge and knowledge into plans that 
drive profitable business action. BI 
encompasses data warehousing, business 
analytics and knowledge management.
The Data Warehouse Institute, Q4/2002
Data Mining (BGU) Prof. Mark Last
What is Business Intelligence?
• Relationship of intelligence to various levels 
of summarisation
– Data – unstructured data
– Information – structured data useful for analysis
– Knowledge   - obtained from experts based on 
actual experience
– Intelligence – keen insight into understanding 
important relationships
Thierauf (2001)
Data Mining (BGU) Prof. Mark Last
Role of BIS 
• Provide decision makers with timely data, 
information and knowledge for problem solving, 
and problem finding
• Past : Decision making as Problem Solving activity 
– Reactive approach –use of appropriate management 
technologies to resolve current problems as they arise
• Current: Business intelligence activity as problem 
solving, as well as problem finding
– Proactive, preventive approach – anticipating future 
company problems; looking for future opportunities
Data Mining (BGU) Prof. Mark Last
7
Lecture No. 12 – Business Intelligence 
and Data Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
8
Data Warehouse Concept
(Example: CRM)
Operational System
(Online Transaction 
Processing - OLTP)
Data Warehouse / 
Online Analytical 
Processing - OLAP
Loading
Data 
Updates
Data Access
Current 
Data
Historic 
Data
Data 
Mining
Database 
adjustment and 
optimization
Security
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
9
Data Warehouse - Definitions
• A subject-oriented, integrated, time-variant, and 
non-volatile collection of data in support of 
management’s decision making - Inmon, 1994
• A read-only analytical database that is used as the 
foundation of a decision support process - Poe and 
Reeves, 1995
• Managed data situated after and outside the 
operational systems - Gupta, 1997
• A few terabytes of data stored in a dark and cool 
place - Last, 1998
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
10
DW - Characteristics
• Subject-Oriented: production floor control, 
marketing, purchasing, QC, customer support, etc.
• Integrated: encoding, measurement units, 
naming conventions, key structures.
• Time-Variant: long time horizon, time keys. 
• Nonvolatile: no updates of data (only data 
loading and data access).
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
11
OLTP vs. DWH
 On-Line Transaction 
Processing 
Data Warehouse 
Users Front-line workers Management 
Purpose Supports day-to-day 
operations 
Supports strategic 
decisions 
Data Raw data (entered by users) Filtered and transformed 
data 
Source of 
data 
Internal sources only Internal and external 
sources 
Time 
horizon 
Current data  Historical data 
Level of 
detail 
Only detail data Detail and summary data 
Data 
structure 
3NF (Why?) De-normalized tables 
Design goal Maximum update efficiency Maximum query 
efficiency 
 
H
ig
h
 c
o
n
cu
rr
en
cy
L
o
w
 l
at
en
cy
L
o
w
 co
n
cu
rren
cy
R
elax
ed
 laten
cy
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
12
Business Intelligence
and Data Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
13
Data Models
• What is a data model?
– The perception of the data (at different levels of 
abstraction) by users, designers, and developers of an 
information system
– The same data may be modeled in many different ways!
• The ER / Relational Model
– The business keeps track of data about entities
– Entities are stored in tables
– All entities are created equal
• The Dimensional (Star-Join) Model
– A few entities (called “facts”) are much more important 
than the other entities
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
14
Entity-Relationship Model
An Example
Store Region
Sales Order
Sales Item
Customer
Product
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
15
Star-Join Model
An Example
Store Region
Sales Item Customer
ProductTime
Facts of 
Interest*
* - also called KPI – Key Performance Indicators
Static 
Information 
של ומימדיםעובדות 
?א"מז
במוסד ומימדיםעובדות
?אקדמי
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
16
Multidimensional Logical Model / Star 
Schema Analysis
A dimension is a logical grouping of attributes 
arranged according to business area
•Examples: Customer, Product, Location, and Time
•Most dimensions are represented by descriptive values (e.g., 
customer name, product code, order number, etc.)
•Other names for descriptive: qualitative, categorical, nominal, 
non-ordinal.
•Dimensional data (e.g., information about customers) is stored 
in reference entities
Definition 1
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
17
Multidimensional Logical Model / Star 
Schema Analysis (cont.)
• Example: product sales in a particular location during a 
given period of time
• Most facts are represented by quantitative (numeric) values 
(e.g., Dollar amount of sales, number of units produced, 
etc.)
• Facts are stored in fact entities
• Quantitative values can be summed arithmetically
• Example of semiadditive fact: account balance (can be added 
along the customer dimension but not along the time dimension)
Facts (business metrics) - points of dimensional 
intersection
Definition 2
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
18
Detailed Example of Star Schema
time_key
day
day_of_the_week
month
quarter
year
time
location_key
street
city
state_or_province
country
location
time_key
item_key
branch_key
location_key
units_sold
dollars_sold
avg_sales
Measures
item_key
item_name
brand
type
supplier_type
item
branch_key
branch_name
branch_type
branch
Fact entities and reference entities?
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
19
Detailed Example of Snowflake Schema
time_key
day
day_of_the_week
month
quarter
year
time
location_key
street
city_key
location
time_key
item_key
branch_key
location_key
units_sold
dollars_sold
avg_sales
Measures
item_key
item_name
brand
type
supplier_key
item
branch_key
branch_name
branch_type
branch
supplier_key
supplier_type
supplier
city_key
city
state_or_province
country
city
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
20
Information Granularity
• The “optimal” level of granularity is not 
necessarily the lowest level of detail!
• Reducing the amount of information
– Keep the detail data for only the minimum 
amount of time
– Store only aggregated data
– Provide both detail and summary data for a 
limited amount of time
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
21
DW Physical Schema
• Lookup Tables
– Dimensional attribute data (static information)
– Qualitative, independent data
• Relationship Tables
– Relating attributes within a dimension
– One-to-many relationships
– Many-to-many relationships
• Fact Tables
– Primary keys: attributes from multiple dimensions
– Facts or business metrics (dynamic information)
– Quantitative, dependent data
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
22
DW Physical Schema
An Example - 3NF
Division Lookup
Division_id
Division_desc
Dept. Lookup
Dept_id
Dept_desc
Division_id
Region Lookup
Region_id
Region_desc
Market Lookup
Market_id
Market_desc
Region_id
Month Lookup
Month_id
Month_desc
Week Lookup
Week_id
Week_desc
Month_id
Sales Facts
Dept_id
Market_id
Week_id
Sales Lecture No. 12
Data Mining (BGU) Prof. Mark Last
23
DW Physical Schema
An Example – De-normalization
Division Lookup
Division_id
Division_desc
Dept. Lookup
Dept_id
Dept_desc
Division_id
Division_desc
Region Lookup
Region_id
Region_desc
Market Lookup
Market_id
Market_desc
Region_id
Region_desc
Month Lookup
Month_id
Month_desc
Week Lookup
Week_id
Week_desc
Month_id
Month_desc
Sales Facts
Dept_id
Market_id
Week_id
Sales Lecture No. 12
Data Mining (BGU) Prof. Mark Last
24
Physical transformation of 
operational data
• More de-normalization
– Store calculation results (e.g., DOB + age)
– Enumerate measured values (e.g., file size + category)
• Attribute name matching
– Example: WO, Job, Batch_No, Batch_ID
• Different lengths and data types
– Example: YY vs. YYYY 
• Different values for the same meaning
– Example: M/F vs. 1/2
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
25
Physical transformation of operational 
data (cont’d)
• Complex data values
– Example: Product type included in 
batch ID: N1002
• Missing and corrupt data
– Using default values
– Referencing other current data
– Leaving the missing values as blank
– Assigning a specific value to missing 
values
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
26
DW Optimization
• Aggregation / Summary Tables
– Benefit: improved query performance
– Drawback: increased size of the warehouse
• Partitioning
– Time-based partitioning (helps to remove outdated 
data)
– Organizational-based partitions (vulnerable to 
organization changes)
– Sampling (by using random numbers)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
27
DW Optimization (cont’d)
• Operational keys
– Benefit: reduced transformation effort
– Drawbacks: compound and textual keys
• Surrogate keys
– Benefits: a layer of abstraction between DW and the 
source system; simple, numeric keys
– Drawback: increased batch processing
• Indexes
– Objective: speed-up the retrieval of records
– Types of Indexes
• Primary Index: specified on the physical ordering key field
• Clustering Index: non-unique physical ordering field
• Secondary Index: specified on any non-ordering field
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
28
DW Project - Practical Example
Manpower Efficiency System
Objective: calculating average efficiency in 
every production department (work cell)
Average Efficiency = 
Total Standard Time
Total Paid Time
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
29
Manpower Efficiency System
Why Data Warehouse ?
•Average Amount of Transactions: 700 per day
•Number of Records in Std_Times Table: 400
•Amount of Table Joins Required to calculate standard time of 
execution for every transaction (a Cartesian Product):
•280,000 per day
•1,400,000 per week
•6,160,000 per month
Conclusion: Efficiency calculations can be very inefficient !
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
30
Manpower Efficiency System
Star Schema - Transactions
TransactionsStd_Times Process
Time_Period
Product_Type
Chip_Size
Process_Code
Process_Code
Process _Desc
Batch Std. Time
Unit Std. Time
Dept_ID
Product_Type
Chip_Size
Process_Code
Batch_ID
Batch_Size
Date
Day
Month
Year
Dept_ID
Dept_Desc
Department
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
31
Manpower Efficiency System
Star Schema - Paid Hours
Paid_Time
Worker_ID
Date
Worker_Name
Dept_ID
Entry
Exit
Paid_Time
Paid_Hours
Worker_ID
Date
Entry
Exit
Paid_Time
Worker_ID
Worker_Name
Workers
Worker_ID
Date
Dept_ID
Assignments
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
32
Manpower Efficiency System
Surrogate Keys
Worker_ID:105
Date: 01/10/95 (34973)
Dept_ID
Assignments
(OLTP) Paid_Time
(DW)
Key: 34973.105
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
33
Manpower Efficiency System
Denormalization of Tables
Process
Process_Code
Process_Desc
Dept_ID
Department
Dept_ID
Dept_Desc
Transactions
Batch Std. Time
Product_Type
Chip_Size
Process_Code
Dept_ID
Batch_ID
Worker_ID
Date
Saved join
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
34
Manpower Efficiency System
Partition and Aggregation
One week
Transactions
Std. Time
Product_Type
Chip_Size
Process_Code
Dept_ID
Batch_ID
Worker_ID
Date
Paid_Time
Worker_ID
Worker_Name
Dept_ID
Date
Entry
Exit
Paid_Time
Six Months
Eff_Data
Date
Dept_ID
Total_Std_Time
Total_Paid_Time
Efficiency
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
35
Manpower Efficiency System
Star Schema - Aggregated Data
Eff_Data
Date
Dept_ID
Total_Std_Time
Total_Paid_Time
Efficiency
Sum_Transactions
Date
Dept_ID
Total_Std_Time
Date
Dept_ID
Total_Paid_Time
Sum_Paid_Time
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
36
Business Intelligence and Data 
Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
37
Metadata - Definitions
• Metadata is “data about data” - Inmon, 
1994
• Metadata is high-level data that describes 
lower-level data - APT Data Group 1996
• Considering that we don't know exactly what it is, 
or where it is, we spend more time talking about it, 
worrying about it, and feeling guilty we aren't 
doing anything about it than any other topic -
Kimball, 1998
• A map to the data in the data warehouse –
Sperley, 1999
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
38
Data without Metadata
Examples
• Country = 972
• City = 068 
• Department = 372
• Date = 07/04/98
• Gender = 0 
• Age = 370
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
39
Data with Metadata
Examples
• Country = 972  (Israel)
• City = 068 (Tel-Aviv)
• Department = 372 (Information Systems 
Engineering)
• Date = 07/04/98  (4 July 1998)
• Gender = 0 (Female)
• Age = 370 (70 years)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
40
Sources of Existing Metadata
• Code Documentation
• OLTP Applications
• DBMS (Database Management System)
• Middleware (Extraction Software)
• DW Design Documentation
• CASE Tools
• User Tools
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
41
Classification of Metadata
• Implementation-time metadata
– Entities, locations, and motivations
• Run-time metadata
– Active metadata
• Manages security and access to data
• Provides usage data
– Passive metadata
• Creates the context for the business data
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
42
Manpower Efficiency System
Data Transformation and Metadata
Paid_Time
(DW)
Paid_Hours
(OLTP)
Day: 13
Month: 5
Year: 97
Entry_Time: 7.25
Exit_Time: 16.14
Date: 13/05/97
Entry: 07:25
Exit:16:14
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
43
Business Intelligence and Data 
Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
44
Causes of Poor Data Quality
• Process Problems
– Data entered at the wrong point in the 
operational process
– Inaccurate measuring and counting equipment
• People Problems
– Failing to update the data on time
– Entering incorrect data by mistake (“keying 
errors”)
– Entering incorrect data on purpose (fraud)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
Examples of Data Quality Problems
Source: http://www.bcs.org/upload/pdf/ewrazen-120607.pdf
• Retail company found over 1m records contained home tel
number of “000000000” and addresses containing flight 
numbers
• Insurance company found customer records with 99/99/99 
in creation date field of policy
• Car rental company discovered duplicate agreement 
numbers in their European data warehouse
• Healthcare company found 9 different values in gender 
field
• Food/Beverage retail chain found the same product was 
their No 1 and No 2 best sellers across their business
Lecture No. 12 45
Data Mining (BGU) Prof. Mark Last
46
Data Quality Dimensions
(based upon Wand and Wang, Comm. of the ACM, Nov. 1996)
1. Proper Presentation of Data
Real World
Information System
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
47
Data Deficiency
2. Incomplete Presentation
Real World
Information System
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
48
Data Deficiency (cont.)
3. Ambiguous Presentation
Real World
Information System
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
49
Data Deficiency (cont.)
4. Meaningless State
Real World
Information System
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
50
Data Deficiency (cont.)
5. Data Garbling
Real World
Information System
A. Wrong state
B. 
Meaningless 
state
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
54
Business Intelligence and Data 
Warehousing
• Business Intelligence
• Why Data Warehousing?
• Data Modeling
• Metadata (“data about data”)
• Data Quality
• ETL (Extraction, Transformation, and 
Loading)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
55
ETL – The Words Behind the Acronym 
• Extraction
– Extract data from a source database
• Transformation
– Transform the data into a format suitable for a 
target database
• Loading
– Load the data into the target database
• Common Practice
– ETL requires about 80% of the efforts of building a 
DW
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
56
Steps of DW Loading
1. Read the legacy data
– choosing data repository
– physical transformation of data
– using metadata
2. Decide what changed
– new facts
– changes in dimension tables
– using record timestamps
(based upon Ralph Kimball, Mastering Data Extraction, DBMS 
and Internet Systems, June 1996)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
57
Steps of DW Loading (cont’d)
3.Generating keys for changing dimensions
– objective: track dimension changes
4.De-normalization of dimensions
– objective: combining separate sources into 
single records
5.Create load record images
– only detail data
– no generated or aggregate (summary) records
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
58
Steps of DW Loading (cont’d)
6.Migrate the data from the Operational 
system to Data Warehouse
7.Create aggregate records
8.Generate artificial keys for aggregate 
records
9.Bulk load all the records
– enforce referential integrity (star schema)
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
59
Steps of DW Loading (cont’d)
10. Process load exceptions
– records failing the referential integrity check
11. Index the newly loaded records
12. Quality assurance
– data cleaning
– compare totals to the operational data
13. Publish the data
– email to all users
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
60
Manpower Efficiency System
• Transactions Table:
– Loading operational data for one day or a 
number of days
– New data is appended to existing table
• Paid_Time Table:
– Loading operational data for a full or a partial 
week (starting with Sunday)
– New data overwrites the existing data on the 
same week
Data Loading
Lecture No. 12
Data Mining (BGU) Prof. Mark Last
61
Lecture No. 2 - Summary
• DWH is a subject-oriented, integrated, time-
variant, and non-volatile collection of data in 
support of management’s decision making 
• DWH and OLTP are different in many aspects
• A DWH stores dimensions and facts
• It is important to optimize the DWH performance
• Metadata is a map to the data in the DWH
• Data quality is measured by various quality
dimensions
• The ETL process has 13 stages
Lecture No. 12
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-13\kdd13.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Lecture No. 13
June 16, 2018 1
Lecture No. 13 – OLAP and Big Data
• BI and OLAP
• The Data Warehouse Architecture
• Multidimensional Analysis
• OLAP Server Architectures
• From data warehousing to big data
Lesson No. 13
Data Mining (BGU) Lecture No. 13
2
Business Intelligence (BI) and OLAP (On-
Line Analytical Processing)
• OLAP and Data Mining differ in what they offer 
the user
– Complementary technologies 
– Online Analytical Processing (OLAP)
• “top-down” analysis
– Data Mining
• “bottom-up” analysis
• Data warehouse (or data marts) together with tools 
such as OLAP and /or data mining are referred to 
as Business Intelligence (BI) technologies
Source: Marut Buranarach
June 16, 2018 Lesson No. 13
Data Mining (BGU) Lecture No. 13
4
OLAP Applications
• OLAP applications usually have the 
following common features:
– Multi-dimensional views of data
• Data can be viewed from various perspectives, e.g. 
product, location, time, etc.
– Support for complex calculations
• e.g. sales forecasting, moving averages, percentage 
growth, etc. 
– Time intelligence
• e.g. comparisons of sales performance between 
different time periods
Source: Marut Buranarach
June 16, 2018 Lesson No. 13
Data Mining (BGU) Lecture No. 13
Example 1: OLAP Cube Report
June 16, 2018 5
Source: http://www.filebuzz.com/findsoftware/Olap_Cube_Report/1.html
Lesson No. 13
Data Mining (BGU) Lecture No. 13
Example 2: RadarCube Windows 
Forms Desktop OLAP v1.11
June 16, 2018 6
Source: http://www.filebuzz.com/fileinfo/23913/RadarCube_Windows_Forms_Desktop_OLAP.html
Lesson No. 13
Data Mining (BGU) Lecture No. 13
Example 3: Analyzer's 
Intelligent Mapping
June 16, 2018 7
Source: http://www.strategycompanion.com/mProduct/m_Dashboards.aspx
Lesson No. 13
Data Mining (BGU) Lecture No. 13
Example 4: The GainSeeker 
Enterprise Dashboard
June 16, 2018 8
Source: http://www.hertzler.com/php/products/gainseeker/dashboard.php
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 9
Lecture No. 13 – OLAP, Big Data, and BI
• BI and OLAP
• The Data Warehouse Architecture
• Multidimensional Analysis
• OLAP Server Architectures
• From data warehousing to big data
Lesson No. 13
Data Mining (BGU) Lecture No. 13
10
Data Warehouse: A Multi-Tiered Architecture
Data
Warehouse
Extract
Transform
Load
Refresh
OLAP Engine
Analysis
Query
Reports
Data mining
Monitor
&
Integrator
Metadata
Data Sources Front-End Tools
Serve
Data Marts
Operational 
DBs
Other
sources
Data Storage
OLAP Server
June 16, 2018 Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 15
Lecture No. 13 – OLAP, Big Data, and BI
• BI and OLAP
• The Data Warehouse Architecture
• Multidimensional Analysis
• OLAP Server Architectures
• From data warehousing to big data
Lesson No. 13
Data Mining (BGU) Lecture No. 13
16
A Concept Hierarchy: 
Dimension (location)
all
Europe North_America
MexicoCanadaSpainGermany
Vancouver
M. WindL. Chan
...
......
... ...
...
all
region
office
country
TorontoFrankfurtcity
June 16, 2018 Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 17
Multidimensional Data
• Sales volume as a function of product, 
month, and region
P
ro
d
u
ct
Month
Dimensions: Product, Location, Time
Hierarchical summarization paths
Industry   Region Year
Category   Country  Quarter
Product City     Month Week
Office         Day
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 18
A Sample Data Cube
Total annual sales
of  TV in U.S.A.
Date
C
o
u
n
tr
ysum
sum
TV
VCR
PC
1Qtr 2Qtr 3Qtr 4Qtr
U.S.A
Canada
Mexico
sum
Lesson No. 13
Cube –
A model allowing rapid answers over multi-
dimensional, aggregative queries – in fact, this is a 
hyper-cube
Data Mining (BGU) Lecture No. 13
June 16, 2018 19
Cuboids Corresponding to the 
Cube
all
product date country
product,date product,country date, country
product, date, country
0-D(apex) cuboid
1-D cuboids
2-D cuboids
3-D(base) cuboid
Base vs. aggregate cells; ancestor vs. descendant cells; parent vs. child cells
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 22
Typical OLAP Operations
• Roll up (drill-up): summarize data
– by climbing up hierarchy or by dimension reduction
• Drill down (roll down): reverse of roll-up
– from higher level summary to lower level summary or detailed 
data, or introducing new dimensions
• Slice and dice: project and select
• Pivot (rotate):
– reorient the cube, visualization, 3D to series of 2D planes
• Other operations
– drill across: involving (across) more than one fact table
– drill through: through the bottom level of the cube to its back-
end relational tables (using SQL)
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 23
Fig. 3.10 Typical OLAP 
Operations
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 24
Lecture No. 13 – OLAP, Big Data, and BI
• BI and OLAP
• The Data Warehouse Architecture
• Main Characteristics of OLAP
• OLAP Server Architectures
• From data warehousing to big data
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 25
OLAP Server Architectures
• Relational OLAP (ROLAP) 
– Use relational or extended-relational DBMS to store and manage 
warehouse data and OLAP middle ware
– Include optimization of DBMS backend, implementation of 
aggregation navigation logic, and additional tools and services
– Greater scalability
• Multidimensional OLAP (MOLAP) 
– Sparse array-based multidimensional storage engine 
– Fast indexing to pre-computed summarized data
• Hybrid OLAP (HOLAP) (e.g., Microsoft SQLServer)
– Flexibility, e.g., low level: relational, high-level: array
• Specialized SQL servers
– Specialized support for SQL queries over star/snowflake schemas
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 33
Lecture No. 13 – OLAP, Big Data, and BI
• BI and OLAP
• The Data Warehouse Architecture
• Main Characteristics of OLAP
• OLAP Server Architectures
• From data warehousing to big data
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 34
Data Warehouse Usage
• Three kinds of data warehouse applications
– Information processing
• supports querying, basic statistical analysis, and reporting 
using crosstabs, tables, charts and graphs
– Analytical processing
• multidimensional analysis of data warehouse data
• supports basic OLAP operations, slice-dice, drilling, 
pivoting
– OLAM (on-line analytical mining)
• knowledge discovery from hidden patterns 
• supports associations, constructing analytical models, 
performing classification and prediction, and presenting 
the mining results using visualization tools
Lesson No. 13
Data Mining (BGU) Lecture No. 13
June 16, 2018 35
Summary: Data Warehouse and 
OLAP Technology
• Why data warehousing?
• A multi-dimensional model of a data warehouse
– Star schema, snowflake schema, fact constellations
– A data cube consists of dimensions & measures
• Data warehouse architecture
• OLAP servers: ROLAP, MOLAP, HOLAP
• OLAP operations: drilling, rolling, slicing, dicing 
and pivoting
• From OLAP to OLAM (on-line analytical mining)
Lesson No. 13
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-2\kdd02.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 1
Lesson 2- Data Preparation and 
Data Engineering
 Stages of a Knowledge Discovery Project
 Data in Reality
 Problems in Data Accessibility
 Data Pre-processing
 Data Cleaning
 Preparation of Time Series Data
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 2
Stages of a Knowledge Discovery Project
(based on Pyle, 1999)
Time to Complete
Data 
Preparation
%
Data 
Surveying
%
Data 
Modeling
%
Exploring 
the Solution
%
Implementati
on 
Specification
%
Exploring 
the Problem
%
Importance to Success
Data 
Preparation
%
Data 
Surveying
%
Data 
Modeling
%
Exploring 
the Solution
%
Implementati
on 
Specification
%
Exploring 
the Problem
%
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 3
Data in Reality
 What DM Tools Need?
 Data Availability
 One static data table
 Clear meaning of each 
attribute
 Well-defined domain of 
each attribute
 Values of all attributes
 Data reliability
 No duplicate information 
 Data consistency 
 What we have?
 Data is not readily 
accessible
 Several tables / 
databases / data 
streams
 Missing metadata
 Out-of-range values
 Missing values
 Noisy data
 Redundant information
 Inconsistent data
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 4
Data Accessibility Problems
 Legal Issues
 Information Privacy
 Information Security
 Departmental Access
 Political Reasons
 Data Format
 Architectural Reasons
 Timing
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 5
Why Is Data Dirty?
 Incomplete data comes from
 n/a data value when collected
 Different consideration between data collection and data analysis.
 Human/hardware/software problems (e.g., earthquake catalog)
 Noisy data comes from the process of data
 Collection (e.g., vital signs)
 Entry
 Transmission
 Inconsistent data comes from
 Different data sources
 Functional dependency violation
 Business process changes (e.g., wine quality)
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 6
Forms of data preprocessing 
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 7
Data Cleaning Tasks
 Handle missing values
 Identify outliers and smooth out noisy data 
 Correct inconsistent data
 Resolve redundancy caused by data integration
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 8
How to Handle Missing Data?
 Ignore the tuple
 Fill in the missing value manually
 Fill in it automatically with
 a global constant 
 the attribute mean
 the attribute mean for all samples of the same class: smarter
 the most probable value: inference-based
?
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 9
How to Handle Noisy Data?
 Binning method:
 first sort data and partition into (equi-depth) bins
 then one can smooth by bin means,  smooth by bin median, 
smooth by bin boundaries, etc.
 Clustering
 detect and remove outliers
 Combined computer and human inspection
 detect suspicious values and check by human (e.g., deal with 
possible outliers)
 Regression
 smooth by fitting the data into regression functions
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 10
Simple Discretization Methods: 
Binning
 Equal-width (distance) partitioning:
 It divides the range into N intervals of equal size: uniform 
grid
 if A and B are the lowest and highest values of the attribute, 
the width of intervals will be: W = (B –A)/N.
 The most straightforward
 But outliers may dominate presentation
 Skewed data is not handled well.
 Equal-depth (frequency) partitioning:
 It divides the range into N intervals, each containing 
approximately same number of samples
 Good data scaling
 Managing categorical attributes can be tricky.
0
50
100
150
200
0 2 4 6 8
N
(M
)
Richter Magnitude (M)
Aragonese Shocks 
(1983-2010)
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 11
Binning Examples
*  Sorted data for price (in dollars): 4, 8, 9, 15, 21, 21, 24, 25, 
26, 28, 29, 34 (11 distinct values)
*  Partition into (equi-depth) bins: 
- Bin 1: 4, 8, 9, 15
- Bin 2: 21, 21, 24, 25
- Bin 3: 26, 28, 29, 34
*  Smoothing by bin means: (3 distinct values)
- Bin 1: 9, 9, 9, 9
- Bin 2: 23, 23, 23, 23
- Bin 3: 29, 29, 29, 29
*  Smoothing by bin boundaries: (6 distinct values)
- Bin 1: 4, 4, 4, 15
- Bin 2: 21, 21, 25, 25
- Bin 3: 26, 26, 26, 34
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 12
Cluster Analysis
Example: customer locations in a city
Cluster centroid
OutlierN
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 13
Outliers (Source: Pyle, 1999)
Conclusion: an outlier may be an element of another 
cluster
Data Mining (BGU) Prof. Mark Last
Data Integration
 Data integration: 
 Combines data from multiple sources into a coherent store
 Schema integration: e.g., A.cust-id  B.cust-#
 Integrate metadata from different sources
 Entity identification problem: 
 Identify real world entities from multiple data sources, e.g., Bill Clinton 
= William Clinton
 Detecting and resolving data value conflicts
 For the same real world entity, attribute values from different sources are 
different
 Possible reasons: different representations, different scales, e.g., metric 
vs. British units
14
14
Data Mining (BGU) Prof. Mark Last
15
15
Handling Redundancy in Data Integration
 Redundant data occur often when integration of multiple 
databases
 Object identification:  The same attribute or object may 
have different names in different databases
 Derivable data: One attribute may be a “derived” attribute 
in another table, e.g., patient age
 Redundant attributes may be able to be detected by correlation 
analysis and covariance analysis
 Why reduce/avoid redundancies and inconsistencies before 
mining the data?
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 16
Data Transformation
 Smoothing: remove noise from data
 Aggregation: summarization, data cube construction
 Generalization: concept hierarchy climbing
 Normalization: scaled to fall within a small, specified 
range
 min-max normalization
 z-score normalization
 normalization by decimal scaling
 Attribute/feature extraction
 New attributes constructed from the given ones (e.g., DNN)
Data Mining (BGU) Prof. Mark Last
17
Normalization
 Min-max normalization: to [new_minA, new_maxA]
 Ex.  Let income range $12,000 to $98,000 normalized to [0.0, 1.0].  
Then $73,000 is mapped to  
 Z-score normalization (μ: mean, σ: standard deviation):
 Ex. Let μ = 54,000, σ = 16,000.  Then
 Normalization by decimal scaling
716.00)00.1(
000,12000,98
000,12600,73



AAA
AA
A
minnewminnewmaxnew
minmax
minv
v _)__(' 



A
Av
v


'
j
v
v
10
' Where j is the smallest integer such that Max(|ν’|) < 1
225.1
000,16
000,54600,73


73,600
100,000
= 0.736
Data Mining (BGU) Prof. Mark Last
18
Data Reduction Strategies
 Data reduction: Obtain a reduced representation of the data set that is much 
smaller in volume but yet produces the same (or almost the same) analytical 
results
 Why data reduction? — A database/data warehouse may store terabytes of 
data.  Complex data analysis may take a very long time to run on the complete 
data set.
 Data reduction strategies
 Dimensionality reduction
 Wavelet transforms
 Principal Components Analysis (PCA)
 Feature subset selection, feature extraction
 Numerosity reduction (some simply call it: Data Reduction)
 Regression and Log-Linear Models
 Histograms, clustering, sampling
 Data cube aggregation
 Data compression
Data Mining (BGU) Prof. Mark Last
19
Dimensionality Reduction
 Curse of dimensionality
 When dimensionality increases, data becomes increasingly sparse
 Density and distance between points, which is critical to clustering, 
outlier analysis, becomes less meaningful
 The possible combinations of subspaces will grow exponentially
 Dimensionality reduction
 Avoid the curse of dimensionality
 Help eliminate irrelevant features and reduce noise
 Reduce time and space required in data mining
 Allow easier interpretability and visualization of DM results
 Dimensionality reduction techniques
 Wavelet transforms
 Principal Component Analysis
 Supervised and nonlinear techniques (e.g., feature selection)
Data Mining (BGU) Prof. Mark Last
20
Feature Subset Selection
 Another way to reduce dimensionality of data
 Redundant attributes 
 Duplicate much or all of the information contained in one or 
more other attributes
 E.g., purchase price of a product and the amount of sales tax 
paid
 Irrelevant attributes
 Contain no information that is useful for the data mining task 
at hand
 E.g., students' ID is often irrelevant to the task of predicting 
students' GPA
Data Mining (BGU) Prof. Mark Last
Relevant, Redundant and Irrelevant Features
• Feature selection retains relevant features for 
learning and removes redundant or irrelevant ones
• For a binary classification task below, f1 is relevant, 
f2 is redundant given f1, and f3 is irrelevant
Source: Huan Liu
Data Mining (BGU) Prof. Mark Last
Feature selection selects an ‘optimal’ subset of 
relevant features from the original high-
dimensional data given a certain criterion
Feature Selection
Source: Huan Liu
Data Mining (BGU) Prof. Mark Last
Sampling / Record Selection
 Simple random sampling
 Sampling without replacement
 Sampling with replacement
 Stratified sampling
 Active sampling / learning
23
Data Mining (BGU) Prof. Mark Last
24
Attribute Creation (Feature Extraction)
 Create new attributes (features) that can capture the 
important information in a data set more effectively than the 
original ones
 Three general methodologies
 Attribute extraction
 Domain-specific
 Mapping data to new space
 E.g., Fourier transformation, wavelet transformation, 
manifold approaches, DNN
 Attribute construction 
 Combining features
 Data discretization
Data Mining (BGU) Prof. Mark Last
25
Concept Hierarchy Generation
 Concept hierarchy organizes concepts (i.e., attribute values) 
hierarchically
 Concept hierarchy formation: Recursively reduce the data by 
collecting and replacing low level concepts (such as numeric 
values for age) by higher level concepts (such as youth, adult, or 
senior)
 Concept hierarchies can be explicitly specified by domain 
experts and/or data warehouse designers
 Concept hierarchy can be automatically formed for both numeric 
and nominal data—For numeric data, use discretization methods
 Nominal data example: street < city < state < country
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 26
Example: Medical Records
(Source: Mortality Records, Israeli Ministry of Health)
 Input Attributes 
 Age
 Date of Death
 Gender
 Area of Residence (about 
30 areas)
 Religion (14 codes)
 Country of Birth
 Target Attribute
 Medical Diagnosis (6-
digit ICD-9 code)
 Additional data tables
 Areas (נפה)
 Regions (אזור)
 Religions
 Countries
 Places of Birth 
(Continents)
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 27
Example: Medical Records (cont.) 
Data Pre-Processing
 Generalizing diagnoses to 36 groups
 Decoding age codes
 Generalizing areas (נפות) to regions (אזורים)
 Generalizing country of birth to continent of 
birth
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 28
Sample Medical Records
Raw Data
ID סיבת מוות גיל נפה ארץ לידה
100 428000 403 51 400
200 496000 373 53 110
300 799900 202 51 900
400 745200 108 11 900
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 29
Generalizing Medical Diagnoses
(Based on first 3 digits of ICD-9-CM Codes)
Code Intervals Diagnosis
0 0, 209 Other
1 1 - 139
Infectious and Parasitic 
Diseases
2
140-152, 155-161, 163-
173, 175-184, 186-203 Other Malignant Neoplasms
3 153-154
Malignant Neoplasm of colon-
rectum
4 162
Malignant Neoplasm of trachea 
etc.
5 174
Malignant Neoplasm of female 
breast
6 185 Prostate
7 204-208 Leukaemia
8 210-239 Non-Malignant Neoplasms
9 240-249, 251-279 Other Endocrine Diseases
10 250 Diabetes
11 280-289 Diseases of Blood
12 290-319 Mental Disorders
13 320-389 Diseases of the Nervous System
14 390-409
Other Diseases of the 
Circlulatory System
15 410-414 Ischaemic heart disease
16 415-429
Diseases of pulmonary 
circulation
17 430-438 Cerebrovascular disease
18 439-459
Other Diseases of the 
Circlulatory System
Code Intervals Diagnosis
19
460-479, 488-489, 497-
519
Diseases of the Respiratory 
System
20 480-487 Pneumonia and Influenza
21 490-496
Chronic obstructive pulmonary 
disease
22 520-579
Diseases of the Digestive 
System
23 580-599 Diseases of the Urinary System
24 600-629 Diseases of the Genital Organs
25 630-639 Abortion
26 640-679 Pregnancy etc.
27 680-709 Diseases of the Skin
28 710-739
Diseases of the Muscoloskeletal 
System
29 740-759 Congenital Anomalies
30 760-779 Perinatal period
31 780-799
Symptoms and Ill-Defined 
Conditions
32
800-809, 820-949, 970-
999 Other Accidents
33 810-819 Motor Vehicle Traffic Accidents
34 950-959 Suicide and Self-inflicted injuries
35 960-969 Homicide
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 30
Generalizing Medical Diagnoses (cont.)
Code Intervals Diagnosis
16 415-429
Diseases of pulmonary 
circulation
21 490-496
Chronic obstructive pulmonary 
disease
31 780-799
Symptoms and Ill-Defined 
Conditions
29 740-759 Congenital Anomalies
ID סיבת מוות Reason
100 428000 16
200 496000 21
300 799900 31
400 745200 29
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 31
Decoding Age Codes
 Age code: XXX (3 digits)
 First digit
 1 – days
 2 – months
 3 – years (1-99)
 4 – years (100-)
ID גיל Age (years)
100 403 103
200 373 73
300 202 0
400 108 0
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 32
Calculating region (אזור) from area 
(נפה)
קוד נפה שם נפה קוד אזור שם אזור
11 ירושלים 1 ירושלים
51 תל-אביב 5 ת"א
53 חולון 5 ת"א
ID נפה Region_Code
100 51 5
200 53 5
300 51 5
400 11 1
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 33
Calculating place of birth (continent) 
from country of birth
Country Continent_Name Continent_Code
110 אסיה 0
400 אירופה אמריקה 1
900 ישראל 3
ID ארץ לידה Cont_Birth
100 400 1
200 110 0
300 900 3
400 900 3
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 34
Transformation Results
Raw Data
ID סיבת מוות גיל נפה ארץ לידה
100 428000 403 51 400
200 496000 373 53 110
300 799900 202 51 900
400 745200 108 11 900
Final Data
ID Reason Age (years) Region_Code Cont_B
100 16 103 5 1
200 21 73 5 0
300 31 0 5 3
400 29 0 1 3
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 35
Preparation of Time Series Data
 Time-series database
 Consists of sequences of values or events 
changing with time
 Data is recorded at regular intervals
 Characteristic time-series components
 Trend, cycle, seasonal, noise
 Time series data mining tasks
 Finding clusters of similar time series
 Detecting events (change points) in time series
 Predicting future values of time series
 etc.
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 36
Describing a Series
 Trend
 A non-cyclic, monotonically 
increasing or decreasing 
component of the waveform
 Cycle
 A trend over one period may be a 
cycle over a different period
 Seasonality
 Certain seasons (e.g., X-mas) are 
inherently different disregarding 
any other trend, cycle, or noise
 Noise
 The component left after the 
trend, cyclic, and seasonal 
components have been extracted
Seasonal Index
0
20
40
60
80
100
120
140
160
1 2 3 4 5 6 7 8 9 10 11 12
Month
Raw data from 
http://www.bbk.ac.uk/manop/man/docs/QII_2
_2003%20Time%20series.pdf
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 37
Preparation of Time Series Data: Example
Based on: Last, Klein, Kandel, Knowledge Discovery in Time Series Databases, IEEE 
Transactions on Systems, Man, and Cybernetics, 31:  Part B, No. 1, pp. 160-169, Feb. 2001
0 200 400 600 800 1000 1200 1400
0
50
100
150
200
250
300
Days
C
lo
s
e
 V
a
lu
e
s
Boeing - SNR
Stock 
value
Trend SNR
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 38
Moving Average Methods
 Goal
 Determining the trend of time series data
 Most common methods
 Simple Moving Average
 Weighted Moving Average
 Exponential Moving Average
From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 39
Simple Moving Average
 The forecast is simply the average of the most recent 
k observations:
k
YYY
Y ktttt
11
1
...ˆ 



From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 40
Selecting k
 Smoothing effect (large k)
 Responsiveness (small k)
 Useful to compare results with different k
values
From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 41
BCC Stock Price
with 10 & 40 week MA
0
5
10
15
20
25
30
35
40
45
50
26
0
25
1
24
2
23
3
22
4
21
5
20
6
19
7
18
8
17
9
17
0
16
1
15
2
14
3
13
4
12
5
11
6
10
7988980716253443526178
From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 42
Weighted Moving Average
 Moving average where each value in the window is 
assigned a unique weight
1...:
...ˆ
11
11111




kttt
ktktttttt
wwwwhere
YwYwYwY
From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 43
Selecting Weights
 Sum is 1.0
 More recent data is often more important
 Other knowledge may skew weights
 Equal weights is the same as single moving 
average (w=1/k)
From http://ie.d.umn.edu/MSEM/Courses/EMGT5230/Lectures/Ch_04a.ppt
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 44
Exponential Moving Average
 





321
3
2
21
)1()1(
)1()1(
tttt
tttt
YaYYF
YYYF


Ft : Forecast for period t
Ft-1: Last period forecast
Yt-1: Last period actual value
11 )1(   ttt FYF 
Data Mining (BGU) Prof. Mark Last
March 3, 2019 Lecture No. 2 45
Summary
 Data  preparation or preprocessing is a big issue for 
data engineers
 Descriptive data summarization is needed for quality 
data preprocessing
 Data preparation includes
 Data cleaning and data integration
 Data reduction and feature extraction + selection
 Discretization
 A lot a methods have been developed but data 
preprocessing is still an active area of research
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-3\kdd03.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 1
Lecture No. 3 – The Role of 
Information Theory in Data Mining
 Information Theory Overview
 Basic Concepts
 Data Compression
 Communication Channel
 Information-Theoretic 
Approaches to Data Mining
 The Uncertainty Approach 
 The Data Compression Approach
 Minimum Description Length 
(MDL) Principle
 Summary
Claude Elwood Shannon 
(1916-2001)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 2
Motivation: Why Information Theory?
 Data mining objectives – reminder
 Identify valid, novel, potentially useful, and ultimately 
understandable patterns in data
 Any large dataset contains a potentially infinite 
amount of patterns
 Most of them are not valid (random), completely useless, or 
too complex to be understood properly
 We need objective criteria for inducing the most 
informative patterns from data
 Information theory provides a nice formal 
framework for finding and evaluating patterns 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 3
Information and Uncertainty
 What is information?
 Attneave (1959): Information is that which removes or 
reduces uncertainty
 Uncertainty is our limited knowledge about the 
outcome of some (future) event
 Examples of uncertain events
 Credit card transaction (legitimate / fraudulent)
 A patient clinical condition (disease = ??? )
 Final grade point average of a student admitted to the 
university (outcome = value between 0 and 100)
 More?
Data Mining (BGU) Prof. Mark Last
How to measure uncertainty?
 A quantitative measure of uncertainty should 
have at least the following properties
 If the outcome of an event can be predicted with a 100% 
accuracy, then the uncertainty of an event is zero
 The uncertainty of an event increases with the number of 
possible outcomes (cc vs. student)
 For the same number of outcomes, the uncertainty is 
maximal if each outcome has the same probability 
(examples?)
March 18, 2019 Lecture No. 3 4
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 5
Thermodynamics: Maxwell’s Demon
(an air-conditioner that needs no power supply)
Outside 
temperature
Inside 
temperature
Background slide
A free 
app?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 6
The Bad News: 
Maxwell’s Demon is impossible
 The demon would still need to use energy to observe 
the molecules (in the form of photons for example).
 Leo Szilard (1929)
 The Demon has to process information in order to make his 
decisions, and, in order to preserve the first and second laws 
(of conservation of energy and of entropy), the energy 
requirement for processing this information is always 
greater than the energy stored up by sorting the molecules. 
 Shannon (1948)
 All transmissions of information require a physical channel,
Background slide
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 7
Represent system in a space whose coordinates are positions 
and momenta = mv (phase space).
momentum
position
Subdivide space into B
bins. 
pk = fraction of particles 
whose positions and 
momenta are in bin k. 
Thermodynamics: Boltzmann’s Entropy
Amount of uncertainty, or missing information, or 
randomness, of the distribution of the pk’s, can be 
measured by HB =  pk log(pk) (also called Gibbs H)
Entropy: S = - kBHB (kB - Boltzmann’s constant)
Background slide
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 8
• Entropy
• Goal: measure of uncertainty of X
• H(X) =  - p(x) log2 p(x)
Where
X - a discrete random variable
x - value of X
p (x) - probability of x
Properties of Entropy
1. H (X) = 0 if and only if the outcome is deterministic (all p(x) but one
are zero): – 0*log 0 - 1* log 1 = 0
2. H(X)  log [number of outcomes]. H (X) is a maximum, when all
outcomes are equiprobable: max H (X) = log [number of outcomes]
3. If all the outcomes have the same probability, then H (X) is a
monotonic increasing function of the number of outcomes
Shannon’s Information Theory
Basic Concepts
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 9
Entropy – Example
Calculating H (Test) and H (Disease)
Entropy of 
Test
Entropy of 
Disease
Data
Disease = 
Yes
Disease = 
No Total
Test = Negative 1 3 4
Test = Positive 4 2 6
Total 5 5 10
Max H (Test) = ?
Max H (Disease) = ?
H (test)
Test = 
Negative
Test = 
Positive Total
p(test) 0.4 0.6
-log p(test) 1.322 0.737
-p(test)*log p(test) 0.529 0.442 0.971
H (disease)
Disease = 
Yes
Disease = 
No Total
P(disease) 0.5 0.5
-log p(disease) 1.000 1.000
-p(disease)*log p (disease) 0.500 0.500 1.000
H(X) =  - p(x) log2 p(x)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 10
• Conditional Entropy
• Goal: measure of uncertainty of Y, when X is given. 
• H(Y/X) = -  p(x,y)*log p (y/x)
Where
X, Y – discrete random variables
p (x,y) – joint probability of x and y
p (y/x) – conditional probability of y given x
Properties of Conditional Entropy
1. If Y = f (X) then H(Y/X) = 0 
2. The uncertainty of Y is never increased by knowledge of X: H (Y/X) 
H(Y)
3. If X and Y are independent, then H(Y/X) = H (Y)
Information Theory
Basic Concepts (cont.)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 11
-p (test,disease)*log 
p(disease/test)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.200 0.125 0.325
Test = Positive 0.234 0.317 0.551
Total H(disease/test) 0.434 0.442 0.875
Conditional Entropy – Example 1
Calculating H (Disease/Test)
Conditional 
entropy of 
Disease/Test
Data
Disease = 
Yes
Disease = 
No Total
Test = Negative 1 3 4
Test = Positive 4 2 6
Total 5 5 10
P(test,disease)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.10 0.30 0.4
Test = Positive 0.40 0.20 0.6
Total 0.5 0.5 1.00
P(disease/test)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.25 0.75 1.00
Test = Positive 0.67 0.33 1.00
-log p(disease/test)
Disease = 
Yes
Disease = 
No
Test = Negative 2.000 0.415
Test = Positive 0.585 1.585
H(Y/X) = -  p(x,y)*log p (y/x)
Max H (Disease/Test) = ?
Data Mining (BGU) Prof. Mark Last
-p (test,disease)*log 
p(test/disease)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.232 0.221 0.453
Test = Positive 0.129 0.264 0.393
Total H (test/disease) 0.361 0.485 0.846
March 18, 2019 Lecture No. 3 12
Conditional Entropy – Example 2
Calculating H (Test/Disease)
Conditional 
entropy of 
Test/Disease
Data
Disease = 
Yes
Disease = 
No Total
Test = Negative 1 3 4
Test = Positive 4 2 6
Total 5 5 10
P(test,disease)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.10 0.30 0.4
Test = Positive 0.40 0.20 0.6
Total 0.5 0.5 1.00
P(test/disease)
Disease = 
Yes
Disease = 
No
Test = Negative 0.20 0.60
Test = Positive 0.80 0.40
Total 1.00 1.00
-log p(test/disease)
Disease = 
Yes
Disease = 
No
Test = Negative 2.322 0.737
Test = Positive 0.322 1.322
H(Y/X) = -  p(x,y)*log p (y/x)
Max H (Test/Disease) = ?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 13
• Mutual Information (of variables X and Y)
Goal: the reduction in the uncertainty of Y as a result of knowing X
I(X;Y) = H(Y) - H(Y/X) =
Properties of Mutual Information (MI)
1. Symmetry: I (X; Y) = I (Y ; X) = H (X) – H (X/Y)
2. Mutual information is a non-negative quantity: I (X; Y)  0
3. Maximum MI: If Y = f (X) then I (X; Y) = H(Y)
4. Minimum MI: If X and Y are independent, then I (X; Y) = 0
p x y
p y x
p yx y
( , ) log
( / )
( ),

Information Theory
Basic Concepts (cont.)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 14
Mutual Information – Example
Calculating I (Test; Disease)
Mutual 
information of 
Disease and TestP(disease/test)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.25 0.75 1.00
Test = Positive 0.67 0.33 1.00
P(disease/test) / 
P(disease)
Disease = 
Yes
Disease = 
No
test=0 0.50 1.50
test=1 1.33 0.67
p(test,disease)*log 
p(disease/test) / 
p(disease)
disease=
0
disease=
1 Total
test=0 -0.100 0.175 0.075
test=1 0.166 -0.117 0.049
Total I (test;disease) 0.066 0.058 0.125
H (test) - H (test/disease) 0.125
H(disease) - H(disease/test) 0.125
 
yx yp
xyp
yxpYXI
, )(
)/(
log),();(
P(test,disease)
Disease = 
Yes
Disease = 
No Total
Test = Negative 0.10 0.30 0.4
Test = Positive 0.40 0.20 0.6
Total 0.5 0.5 1.00
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 15
Information Theory
Additional Concepts (to be re-visited)
•Conditional Mutual Information
I(X;Y/Z) = H(X/Z) - H(X/Y,Z) =
Interpretation: the decrease in entropy of X as a result of 
knowing Y, when Z is given
p x y z
p x y z
p x z p y zx y
( , , ) log
( , / )
( / ) ( / ),



•Chain Rule
Interpretation: The decrease in entropy of Y as a result of knowing 
n variables (X1,..., Xn) 
I X Y Y X i( ,..., ; ) ; / ,..., )1 1 X  =  I(X  Xn i 1
i=1
n

•Fano’s Inequality: H (Y/ X1… Xn)  H (Pe) + Pe log2 (m-1)
•H(Pe) = -PelogPe - (1-Pe)log(1-Pe)
Interpretation: Relationship between the minimum prediction error 
Pe, the conditional entropy of the target H, and the number of classes m 
(“upper bound of predictability” - Song et al., 2010)
Data Mining (BGU) Prof. Mark Last
Fano’s Inequality - Example
March 18, 2019 Lecture No. 3 16
Data Disease = Yes Disease = No Total
Test = Negative 1 3 4
Test = Positive 4 2 6
Total 5 5 10
Error 1 2 3
P_e 0.30
H(P_e) 0.881
H(disease/test) 0.875
The classification 
model:
H (Y/ X1… Xn)  H (Pe) + Pe log2 (m-1)
Data Mining (BGU) Prof. Mark Last
A Brief History of Communication
Lecture No. 3 17
Persian Empire
Around 500 BC
Roman Empire
Around 100 AD
Background slide
March 18, 2019
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 18
Information Theory and Data Compression
 The Fundamental Problem of Communication 
(Shannon, 1948)
 To reproduce at one point (“destination”) either 
exactly or approximately a message (outcome) 
selected at another point (“data source”)
 Data Compression
 Minimizing the number of binary digits 
(“description length”) required to encode a random 
message sent by the data source
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 19
Information Theory and Data 
Compression (cont.)
 Optimal Code (Shannon, 1948)
 Assigning –log pi bits to encode message i
 pi – probability of the message (outcome) i
 - log pi – the informational value (“surprisal”) of the outcome i
 Interpretation
 Assign shorter codes (descriptions) to more frequent 
messages and vice versa
 Conclusion
 The shortest average description length of a random 
message (“minimum description length”) is the entropy 
of the data source )(log)()( ii xpxpXH 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 20
Communication Systems
Communication 
Channel
Transmitted 
Messages
(Sequences of 
symbols)
Received 
Messages
Noise source
Data 
Source
Destination
Communication goal: To reproduce at one 
point (“destination”) either exactly or 
approximately a message (outcome) selected 
at another point (“data source”) 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 21
Data Mining Models
Prediction / 
Classification 
Model
Inputs
(Feature 
vectors)
Predicted 
Outputs 
(Classes)
Noisy data
Reminder:
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
Classification goal: To predict either 
exactly or approximately an outcome of 
the actual system (“data source”) 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 22
Information Theory
Noisy Communication Channel
(based on Shannon, 1948)
 Received Signal E = f (S, N)
 Where
 S – Transmitted signal
 N – Noise
 Q.1: Can we reconstruct with certainty the original signal 
from the received signal?
 A.: No 
 Q.2: Can we eliminate the noise by transmitting the 
information in a certain way?
 A.: Yes  (By sending additional information to correct the 
received signal)
Background slide
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 23
Noisy Communication Channel
Example
 The information source X is sending two symbols (0 and 1) 
with the same probability (H(X) = ?)
 Gross rate of transmission: 1000 bits (symbols) /second
 The error rate of the communication channel is about 1% for 
any input x:
 P (Y = 1 / X = 0) = P (Y = 0 / X = 1) = 0.01
 What is the rate of transmission of information?
 The minimum number of bits required to send the correction 
information (“1” – incorrect, “0” – correct):
 H (X / Y) = -[.99 log .99 + 0.01 log 0.01] = 0.081 bits (81 bits per second)
 The net rate of information transmission
 1000 – 81 = 919 bits per second (with 1% error frequency!)
How much information is 
transmitted if P (Y = 1 / X = 0) = 
P (Y = 0 / X = 1) = 0.5?
Background slide
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 24
Channel Capacity
 Rate of transmission R - definitions:
 R = H (X) – H (X/Y)
 Amount of information sent less the uncertainty of what was sent
 R = H (Y) – H(Y/X)
 Amount received less the noise
 R = H(X) + H (Y) – H(X; Y)
 Amount sent + amount received less the joint entropy
 Channel Capacity (Maximum possible rate of 
transmission) 
 C = MaxX { H(X) – H (X/Y) }
Background slide
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 25
The Channel Coding Theorem
 If the entropy (uncertainty) per second of the information 
source H does not exceed the channel capacity C, it is 
possible to transmit the information over the channel with 
an arbitrarily small probability (frequency) of error
 Otherwise, the entropy (uncertainty) of the output will be 
at least H – C
 Data mining interpretation
 A model can be as accurate as the input data itself, but no more
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 26
Lecture No. 2 – The Role of 
Information Theory in Data Mining
 Information Theory Overview
 Basic Concepts
 Data Compression
 Communication Channel
 Information-Theoretic Approaches to Data 
Mining
 The Uncertainty Approach 
 The Data Compression Approach
 Minimum Description Length (MDL) Principle
 Summary
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 27
Information-Theoretic Approaches 
to Data Mining
 The Uncertainty Approach
 Data mining is aimed at reducing uncertainty of the 
target (predicted) variables 
 Uncertainty can be represented by entropy
 Data mining algorithms look for models that 
minimize entropy or maximize mutual information 
(information gain)
 Usage: ID3, C4.5, IFN, etc.
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 28
Information-Theoretic Approaches to Data 
Mining (cont.)
 The Data Compression Approach (see 
Mannila, 2000)
 Smaller models are more comprehensible to the 
user
 The goal of data mining is to compress the data by 
finding some structure (model) for it
 Data mining algorithms should choose a hypothesis 
that compresses data the most (the MDL Principle)
 Usage: Bayesian learning
Data Mining (BGU) Prof. Mark Last
Occam’s Razor
 Commonly attributed to William of Ockham (1290--
1349). In sharp contrast to the principle of multiple 
explanations, it states: 
 Entia non sunt multiplicanda praeter necessitatem
 Entities should not be multiplied beyond necessity.
 Commonly explained as: 
 when have choices, choose the simplest theory.
 Bertrand Russell: ``It is vain to do with more what 
can be done with fewer.'‘
 Newton (Principia):
 Natura enim simplex est, et rerum causis superfluis non 
luxuriat
הטבע פשוט ואין לו עודף של סיבות מיותרות למהות הדברים
March 18, 2019 Lecture No. 3 29
Data Mining (BGU) Prof. Mark Last
30
The Data Compression Approach
 Example: regression (line fitting)
 Which model is the best?
 Model selection and overfitting
 Complexity of the model vs. Goodness of fit
Source: Grunwald et al. (2005) Advances in Minimum Description 
Length: Theory and Applications.
1 2 3
March 18, 2019 Lecture No. 3
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 31
Minimum Description Length (MDL) Principle
 Problem Statement
 The attribute values in each case are available to both a 
sender and a receiver
 Only the sender knows the class to which each case belongs
 The sender must transmit the classification information to 
the receiver by using a minimum number of bits
 Decision Variable
 The model instance (“hypothesis”) to be used by the 
“sender” out of a given family of models (e.g., decision 
trees, info-fuzzy networks, kth degree polynomials, etc.) 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 32
The MDL Preliminaries
 L (h) - the length, in bits, of the description of the 
hypothesis (the theory cost)
 Also called parametric complexity (measure of the model 
“richness”, related to the number of model parameters)
 Example – decision tree: L (h) = f (number of nodes)
 In the case of noisy data, the exceptions to the 
hypothesis should also be transmitted
 L (D/h) - the length, in bits, of the description of the 
data under the assumption that both the sender and the 
receiver know the hypothesis (encoded with the help 
of the hypothesis)
 Complex hypotheses lead to small L (D/H) and vice versa
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 33
The MDL Principle
 Choose the hypothesis hMDL which satisfies the following
 L –description length (bits)
 C1 - the optimal encoding of the hypothesis h
 C2 – the optimal encoding of data D given the hypothesis
 Interpretation
 The MDL principle represents the trade-off between the model 
complexity and the number of errors committed by it in the 
training data
 Practical Usage
 The MDL principle proved to be an efficient tool for dealing with 
the problem of overfitting
)}/()({minarg
21
hDLhLh CC
Hh
MDL 

Data Mining (BGU) Prof. Mark Last
MDL Example: Learning a polynomial
March 18, 2019 Lecture No. 3 34
No x y 1st Degree 2nd degree 4th degree
1 2 37 37 37 37
2 3 74 74 74 74
3 5 190 148 190 190
4 6 225 185 269 225
5 9 590 296 590 590
•d  - number of bits required to 
describe each entry in a 
polynomial
•Description length of degree k-
1 polynomial: kd bits
•Description length of m points 
not on the polynomial:  md bits
•MDL Cost:
•4th degree: 5d
•2nd degree: 3d + d = 4d
•1st degree: 2d + 3d = 5d
•Which model is the best?
4th degree:
1st degree: y = -37+37x
2nd degree: y = 5+2x+7x2
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 3 35
Summary
 Information theory provides a nice formal framework 
for the process of data mining from both the 
uncertainty reduction aspect and the aspect of data 
compression
 The usage of information-theoretic heuristics in 
numerous data mining algorithms has brought 
satisfactory results in terms of predictive accuracy and 
model compactness
 Many other aspects of the information theory are still 
waiting for their implementation by the KDD 
researchers and practitioners (see Song et al., 2010)
Data Mining (BGU) Prof. Mark Last
Bibliography
 F. Attneave (1959).  Applications of Information Theory to Psychology. Holt, Rinehart 
and Winston.
 T. M. Cover (1991).  Elements of Information Theory. Wiley.
 O. Maimon and M. Last (2000), Knowledge Discovery and Data Mining – The Info-
Fuzzy Network (IFN) Methodology, Kluwer Publishers.
 H. Mannila (2000). Theoretical Frameworks for Data Mining. SIGKDD Explorations, 1 
(2): 30-32.
 T.M. Mitchell (1997). Machine Learning. McGraw-Hill.
 J.R. Quinlan (1986). Induction of Decision Trees. Machine Learning,  1 ( 1): 81-106.
 J. R. Quinlan (1993).  C4.5: Programs for Machine Learning. Morgan Kaufmann.
 J.R. Quinlan (1996). Improved Use of Continuous Attributes in C4.5. Journal of 
Artificial Intelligence Research,  4: 77-90.
 C.E. Shannon (1948), A Mathematical Theory of Communication, Bell Syst. Tech. J., 
27: 379-423.
 C. Song, Z. Qu, N. Blumm, and A.-L. Barabási, Limits of Predictability in Human 
Mobility.  Science 19 February 2010: 327 (5968), 1018-1021.
March 18, 2019 Lecture No. 3 36
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-4\kdd04.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 1
Lecture No. 4 – Decision Tree 
Learning I
 Classification and Prediction
 Overview of Decision Tree Learning
 Avoiding Overfitting
Data Mining (BGU) Prof. Mark Last
Classification vs. Prediction
 Classification  
 predicts categorical class labels (discrete or 
nominal)
 Default predicted class: majority voting
 Optional: class probability estimation
 Prediction / Regression  
 models continuous-valued functions, i.e., predicts 
unknown or missing values 
 Default prediction: expected value
 Optional: confidence interval
March 18, 2019 Lecture No. 4 2
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 3
Examples of typical classification / 
prediction tasks
 Typical classification tasks
 Credit approval: approve / deny
 Target marketing: will buy / will not buy
 Medical diagnosis: Hepatitis B / Hepatitis C
 Fraud detection: lawful transaction / fraudulent 
transaction
 Typical prediction / regression tasks
 Weather forecast: predict tomorrow’s temperature
 Stock trading: predict stock’s price tomorrow
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 4
4
Classification—A Two-Step Process 
 Model construction / induction
 Record labeling
 Building a training set
 Inducing the model(s)
 Model usage
 Comparing to the default (majority) rule
 Measuring the accuracy rate over time
Training Set Test Set Score Set
Future objects 
to be classified 
by the model
Used to estimate
the error rate of the 
induced model 
Used to induce the 
classification model
Data Mining (BGU) Prof. Mark Last
Model Evaluation and Selection
 Evaluation metrics: How can we measure accuracy?  Other metrics to 
consider?
 Use test set of class-labeled tuples instead of training set when 
assessing accuracy
 The model should generalize beyond the training instances
 Use validation set to tune model parameters
 Common splits: 50:20:30 or 40:20:40
5
Training Set Validation Set Test Set
Used to evaluate
the model 
performance
Used to tune the 
model parameters
Used to induce
the prediction 
model
5
• k-Fold Cross Validation
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 6
Process (1): Model Construction
Training
Data
NAME RANK YEARS TENURED
Mike Assistant Prof 3 no
Mary Assistant Prof 7 yes
Bill Professor 2 yes
Jim Associate Prof 7 yes
Dave Assistant Prof 6 no
Anne Associate Prof 3 no
Classification
Algorithms
IF rank = ‘professor’
OR years > 6
THEN tenured = ‘yes’ 
Classifier
(Model)
What is the 
accuracy of the 
majority rule?
What is the 
accuracy of this 
model on the 
training data?
Target (class) 
attribute
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 7
Process (2): Using the Model in Prediction
Classifier
Testing
Data
NAME RANK YEARS TENURED
Tom Assistant Prof 2 no
Merlisa Associate Prof 7 no
George Professor 5 yes
Joseph Assistant Prof 7 yes
Unseen Data
(Jeff, Professor, 4)
Tenured?
What is the 
accuracy of the 
model on the 
testing data?
IF rank = ‘professor’
OR years > 6
THEN tenured = ‘yes’ 
Is it a better 
model than the 
majority rule?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 8
Classification—Accuracy Estimation
 Estimate accuracy of the model
 The known label is compared to the predicted 
label
 Training Accuracy Rate = 1 - ErrTr
 The percentage of training set samples that are 
correctly classified by the model
 Testing Accuracy Rate = 1 - ErrTest
 The percentage of test set samples that are correctly 
classified by the model
 Test set is independent of training set, otherwise over-
fitting will occur
 If the accuracy is acceptable, use the model to 
classify data tuples whose class labels are not known
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 9
9
Confidence Interval for an Error Rate
 Test error ErrTest is an estimate of the true error rate 
ErrTrue on the entire population
 ErrTest is governed by Binomial distribution 
approximated by Normal when n 30
 Assumption: n test samples are drawn randomly and 
independently from the entire population
 With the probability 1 - , the true error  rate ErrTrue
lies in the confidence interval
]
)1(
 ;
)1(
[
n
ErrErr
zErr
n
ErrErr
zErr TestTestTest
TestTest
Test



 
How to 
estimate the 
true 
accuracy 
rate?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 10
Confidence Interval - Example
10
z
Error n alpha z_alpha min. max.
0.200 30 0.010 2.576 0.0119 0.3881
0.200 30 0.050 1.960 0.0569 0.3431
0.200 30 0.100 1.645 0.0799 0.3201
alpha 0.2 0.1 0.05 0.01 0.001
Z_alpha 1.282 1.645 1.960 2.576 3.291
1
2
α
2
α
ErrorMin. Max.
z
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 11
11
Difference between Classifiers
 Estimated difference between error rates
 d^ is governed by Binomial distribution 
approximated by Normal when n1, n2  30
 With the probability 1 - , the true difference d lies 
in the confidence interval
21
ˆ
TestTest ErrErrd 
2
22
1
11 )1()1(ˆ
n
ErrErr
n
ErrErr
zd TestTestTestTest



 
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 12
Difference between Classifiers –
Example (Error1 vs. Error 2)
12
Error1 n1 Error2 n2 d alpha z_alpha min. max.
0.200 30 0.400 40 0.20 0.010 2.326 -0.048 0.448
0.200 30 0.400 40 0.20 0.050 1.645 0.025 0.375
0.200 30 0.400 40 0.20 0.100 1.282 0.064 0.336
z
1
2
α
2
α
Error
z
alpha 0.2 0.1 0.05 0.01 0.001
Z_alpha 1.282 1.645 1.960 2.576 3.291
Which 
classifier 
is better?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 13
Lecture No. 5 – Decision Tree 
Learning
 Classification and Prediction
 Overview of Decision Tree Learning
 Avoiding Overfitting
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 14
Decision Tree Structure
Main Components
 Nodes - tests of some attribute
 Branch - one of possible values for the attribute
 Leaves (terminal nodes) - classifications
 Path (from the tree root to a leaf) - conjunction of attribute 
tests
Test
< 600 600 - 700 Over 700
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
David Cohen High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Ophir Levy Medium
Sharon Grosman High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Diana Liberman Medium
Anat Klein Low
Node
Branch
Leaf
Path = 
Rule
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 15
15
Decision Tree Learning
Appropriate Problems + Student Example
 Instances are described by a fixed set of attributes
 Example: Gender, Place of Birth, and Test Grade
 Each predicting attribute takes a small number of disjoint 
possible values
 Example: Place of Birth (Israel vs. Abroad)
 The target function has discrete output values (each value = 
class / concept)
 Example: GPA (Low, Medium, High)
 Disjunctive rules are required
 Example: 
 If (Test < 600) Then GPA = Low
 If (Test  600) Then GPA = Medium or High 
 The training data may contain errors (noise)
 The training data may contain missing attribute values
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 16
Algorithm for Decision Tree 
Induction
 Basic algorithm (a greedy algorithm – e.g., ID3)
 Tree is constructed in a top-down recursive divide-and-conquer manner
 At start, all the training examples are at the root
 Attributes are categorical (if continuous-valued, they are discretized in 
advance)
 Examples are partitioned recursively based on selected attributes
 Test attributes are selected on the basis of a heuristic or statistical 
measure (e.g., information gain)
 Conditions for stopping partitioning
 All samples for a given node belong to the same class
 There are no remaining attributes for further partitioning – majority 
voting is employed for classifying the leaf
 There are no samples left
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 17
Detailed Example: 
Student Admission
Historical (Training) Data:
New (Scoring) Data:
ת"ז שם פרטי שם משפחה מגדר תאריך לידה מקום לידה שנת קבלה
ציון 
פסיכומטרי שנת סיום
ממוצע 
ציונים
ID First Name Last Name Gender Date of Birth
Place of 
Birth
Admission 
Year
Test 
Grade
Graduation 
Year GPA
543406619 David Cohen M 18/12/1979 USA 2002 730 2006 93.5
951984264 Ophir Levy M 11/07/1980 Israel 2002 680 2006 87.5
683168092 Sharon Grosman F 19/05/1981 Israel 2002 640 2006 94.3
100900927 Diana Liberman F 11/02/1980 Russia 2002 585 2006 85.8
516120403 Anat Klein F 03/02/1982 Israel 2002 570 2006 78.7
?
All data in this 
example is 
fictional!
ת"ז שם פרטי שם משפחה מגדר תאריך לידה מקום לידה
שנת הגשת 
מועמדות
ציון 
פסיכומטרי
שנת סיום 
מתוכננת
ממוצע ציונים 
צפוי
ID First Name Last Name Gender Date of Birth
Place of 
Birth
Application 
Year
Test 
Grade
Expected 
Graduation 
Year
Expected 
GPA
537793401 Boaz Bazak M 22/09/1985 Israel 2007 580 2011 ?
808943728 Ophir Levy M 10/02/1985 Israel 2007 650 2011 ?
537362102 Maria Neuman F 03/12/1987 Ukraine 2007 720 2011 ?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 18
Pre-processing:
Removing Irrelevant Features
ת"ז שם פרטי שם משפחה מגדר תאריך לידה מקום לידה שנת קבלה
ציון 
פסיכומטרי שנת סיום
ממוצע 
ציונים
ID First Name Last Name Gender Date of Birth
Place of 
Birth
Admission 
Year
Test 
Grade
Graduation 
Year GPA
543406619 David Cohen M 18/12/1979 USA 2002 730 2006 93.5
951984264 Ophir Levy M 11/07/1980 Israel 2002 680 2006 87.5
683168092 Sharon Grosman F 19/05/1981 Israel 2002 640 2006 94.3
100900927 Diana Liberman F 11/02/1980 Russia 2002 585 2006 85.8
516120403 Anat Klein F 03/02/1982 Israel 2002 570 2006 78.7
Meaningless
Unique
Too many 
possible 
values
New value 
every year
New value 
every year
Remained features (attributes)
•Gender
•Place of Birth
•Test Grade
Predicted attribute: GPA
The problem: find the best (most 
accurate) model predicting GPA
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 19
Try 1:
Predict GPA Using a Linear Fit 
Predictive attribute: Test Grade
Test Grade Line Fit  Plot
93.5
87.5
94.3
85.8
78.7
76
78
80
82
84
86
88
90
92
94
96
500 550 600 650 700 750
Test Grade
G
P
A
GPA Predicted GPA
R2 = 0.547
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 20
More Pre-processing
Goal: reduce the number of values
שם פרטי שם משפחה מגדר מקום לידה ציון פסיכומטרי
ממוצע 
ציונים
First Name Last Name Gender
Place of 
Birth Test Grade GPA
David Cohen M Diaspora Over 700 High
Ophir Levy M Israel 600-700 Medium
Sharon Grosman F Israel 600-700 High
Diana Liberman F Diaspora 0-600 Medium
Anat Klein F Israel 0-600 Low
Use: Input Input Input Target
Generalize to two values 
(Israel vs. Diaspora)
Discretize to 
three intervals
Discretize to 
three intervals
Low 0-79
Medium 80-89
High 90-100
What is the 
accuracy of the 
majority rule?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 21
Try 2: Predict GPA Using a Tree
Predictive attribute: Test Grade
Test
< 600 600 - 700 Over 700
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
David Cohen High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Ophir Levy Medium
Sharon Grosman High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Diana Liberman Medium
Anat Klein Low
Prediction = HighPrediction = High
or Medium
(50%/50%)
Prediction = 
Medium or Low
(50%/50%)
Accuracy = 
100%
Accuracy = 
50%
Accuracy = 
50%
Average accuracy: 0.4*50% + 0.4*50% + 0.2*100% = 60%
Is it a better 
model than the 
majority rule?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 22
Try 2: Predict GPA Using a Tree
Predictive attribute: Gender
Gender
F M
שם פרטי שם 
משפחה
ממוצע 
ציונים
First Name
Last 
Name GPA
David Cohen High
Ophir Levy Medium
שם פרטי שם 
משפחה
ממוצע 
ציונים
First Name
Last 
Name GPA
Sharon Grosman High
Diana Liberman Medium
Anat Klein Low
Accuracy = 
50%
Prediction = High
or Medium
(50%/50%)Prediction = High
or Medium or Low
(33%/33%/33%)
Accuracy = 
33%
Average accuracy: 0.4*50% + 0.6*33% = 40%
Is it a better 
model than the 
majority rule?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 23
Try 2: Predict GPA Using a Tree
Predictive attribute: Place of Birth
Place of Birth
Israel Diaspora
שם פרטי שם משפחה
ממוצע 
ציונים
First Name Last Name GPA
Ophir Levy Medium
Sharon Grosman High
Anat Klein Low
שם פרטי שם משפחה
ממוצע 
ציונים
First Name Last Name GPA
David Cohen High
Diana Liberman Medium
Accuracy = 
50%
Prediction = High
or Medium
(50%/50%)Prediction = High
or Medium or Low
(33%/33%/33%)
Accuracy = 
33%
Average accuracy: 0.6*33% + 0.4*50% = 40%
Is it a better 
model than the 
majority rule?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 24
How to choose the best tree?
Test
< 600 600 - 700 Over 700
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
David Cohen High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Ophir Levy Medium
Sharon Grosman High
שם פרטי שם משפחה ממוצע ציונים
First Name Last Name GPA
Diana Liberman Medium
Anat Klein Low
Gender
F M
שם פרטי שם 
משפחה
ממוצע 
ציונים
First Name
Last 
Name GPA
David Cohen High
Ophir Levy Medium
שם פרטי שם 
משפחה
ממוצע 
ציונים
First Name
Last 
Name GPA
Sharon Grosman High
Diana Liberman Medium
Anat Klein Low
Place of Birth
Israel Diaspora
שם פרטי שם משפחה
ממוצע 
ציונים
First Name Last Name GPA
Ophir Levy Medium
Sharon Grosman High
Anat Klein Low
שם פרטי שם משפחה
ממוצע 
ציונים
First Name Last Name GPA
David Cohen High
Diana Liberman Medium
Test, 
Gender 
or Place 
of 
Birth?
ID3: Use 
Information 
Gain
Training accuracy: 
0.4*50% + 
0.4*50% + 
0.2*100% = 60%
Training accuracy: 
0.4*50% + 0.6*33% 
= 40%
Training accuracy: 
0.6*33% + 0.4*50% = 
40%
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 25
Attribute Selection Measure in ID3: 
Information Gain
 Select the attribute with the highest information gain
 Let pi be the probability that an arbitrary tuple in D
belongs to class Ci, estimated by |Ci, D|/|D|
 Expected information (entropy) needed to classify a tuple 
in D:
 Information (conditional entropy) needed (after using A to 
split D into v partitions) to classify D:
 Information gained (mutual information) by branching on 
attribute A
)(log)( 2
1
i
m
i
i ppDInfo 


)(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

(D)InfoInfo(D)Gain(A) A
Data Mining (BGU) Prof. Mark Last
Student Admission Example
26
)(log)( 2
1
i
m
i
i ppDInfo 


Low 1
p 0.200
-logp 2.322
Medium 2
p 0.400
-logp 1.322
High 2
p 0.400
-logp 1.322
Total 5
p 1.00
Entropy 1.522
Expected information
(entropy) needed to 
classify a tuple in D:
Number of classes (m): 3
The classes: 
•Low (one example)
•Medium (two examples)
•High (two examples)
0.2*2.322 + 0.4*1.322 + 0.4*1.322 = 1.522
March 18, 2019 Lecture No. 4
Data Mining (BGU) Prof. Mark Last
Student Admission Example (cont.)
27
522.1)( DInfo
Test 
Grade Total
0-600 600-700 Over 700
Low 1 0 0 1
p 0.500 0.000 0.000
-logp 1.000 0.000 0.000
Medium 1 1 0 2
p 0.500 0.500 0.000
-logp 1.000 1.000 0.000
High 0 1 1 2
p 0.000 0.500 1.000
-logp 0.000 1.000 0.000
Total 2 2 1 5
p 0.40 0.40 0.20 1.00
Entropy 1.000 1.000 0.000 0.800
Gain 0.722
Attribute:
Test Grade
)(log)( 2
1
i
m
i
i ppDInfo 


)(
||
||
)(
1
j
v
j
j
A DInfo
D
D
DInfo 

(D)InfoInfo(D)Gain(A) A
Values: 
•0-600
•600-700
•Over 700
0.5*1.0 + 0.5*1.0 = 1.0 0.4*1.0 + 0.4*1.0 + 0.2*0.0 = 0.8
March 18, 2019 Lecture No. 4
Data Mining (BGU) Prof. Mark Last
Student Admission Example (cont.)
28
522.1)( DInfo
Gender Total
M F
Low 0 1 1
p 0.000 0.333
-logp 0.000 1.585
Medium 1 1 2
p 0.500 0.333
-logp 1.000 1.585
High 1 1 2
p 0.500 0.333
-logp 1.000 1.585
Total 2 3 5
p 0.40 0.60 1.00
Entropy 1.000 1.585 1.351
Gain 0.171
)(log)( 2
1
i
m
i
i ppDInfo 


)(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

(D)InfoInfo(D)Gain(A) A
Attribute:
Gender
March 18, 2019 Lecture No. 4
Data Mining (BGU) Prof. Mark Last
Student Admission Example (cont.)
29
522.1)( DInfo
Attribute:
Place of Birth
)(log)( 2
1
i
m
i
i ppDInfo 


)(
||
||
)(
1
j
v
j
j
A DI
D
D
DInfo 

(D)InfoInfo(D)Gain(A) A
Place of 
Birth Total
Israel Abroad
Low 1 0 1
p 0.333 0.000
-logp 1.585 0.000
Medium 1 1 2
p 0.333 0.500
-logp 1.585 1.000
High 1 1 2
p 0.333 0.500
-logp 1.585 1.000
Total 3 2 5
p 0.60 0.40 1.00
Entropy 1.585 1.000 1.351
Gain 0.171
March 18, 2019 Lecture No. 4
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 30
Student Admission Example (cont.)
 Gain (Test Grade) = 0.722 
 Gain (Gender) = 0.171 
 Gain (Place of Birth) = 0.171
 Selected attribute: Test Grade
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 31
Student Admission Example
Complete Decision Tree
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Diaspora
Low Medium
Anat Klein Diana Liberman Sharon Grosman Ophir Levy
David Cohen
Training accuracy: 100%
Data Mining (BGU) Prof. Mark Last
32
Student Admission Example
Classification with Decision Tree
First Name Boaz
Last Name Bazak
Gender M
Place of 
Birth Israel
Test Grade 580
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
First Name Ophir
Last Name Levy
Gender M
Place of 
Birth Israel
Test Grade 650
First Name Maria
Last Name Neuman
Gender F
Place of 
Birth Ukraine
Test Grade 720
March 18, 2019 Lecture No. 4
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 33
Lecture No. 5 – Decision Tree 
Learning
 Classification and Prediction
 Overview of Decision Tree Learning
 Avoiding Overfitting
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 34
Overfitting
 Overfitting:  An induced tree may overfit the training 
data
 Too many branches, some may reflect anomalies due to 
noise or outliers
 Poor accuracy for unseen samples
 Definition of overfitting
 h – hypothesis (e.g., decision tree) in a hypothesis space H
 Training data: errortrain (h)
 Entire population D: errorD (h)
 Hypothesis h  H overfits training data if there is an 
alternative hypothesis h’  H such that
 errortrain (h) < errortrain (h’) and
 errorD (h) > errorD (h’)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 35
35
Overfitting – Student Example
Tree1 (h’):
Test
< 600 600 - 700 Over 700
MediumMedium High
Tree2 (h): Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
ID First Name Last Name Expected GPA Actual GPA Error
537793401 Boaz Bazak Low Medium Yes
808943728 Ophir Levy Medium Medium No
537362102 Maria Neuman High High No
ID First Name Last Name Expected GPA Actual GPA Error
537793401 Boaz Bazak Medium Medium No
808943728 Ophir Levy Medium Medium No
537362102 Maria Neuman High High No
errortrain (h’) = 40%errorD (h’) = 0%
errortrain (h) = 0%errorD (h) = 33%
errortrain (h) < errortrain (h’)
errorD (h) > errorD (h’)
Which tree is 
better?
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 36
Overfitting in Decision Tree Learning
(Source: Mitchell, 1997)
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 37
Avoiding Overfitting
 Two approaches to avoid overfitting 
 Prepruning: Halt tree construction early —do not 
split a node if this would result in the goodness 
measure falling below a threshold
 Difficult to choose an appropriate threshold
 Postpruning: Remove branches from a “fully 
grown” tree—get a sequence of progressively 
pruned trees
 Use a set of data different from the training data 
to decide which is the “best pruned tree”
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 38
38
Avoiding Overfitting - Student 
Example
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
Test
< 600 600 - 700 Over 700
MediumMedium High
“Fully grown” tree:
Pruned tree:
Prune two nodes:
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 39
Approaches to Avoid Overfitting 
and Determine the Final Tree Size
 Separate training (2/3) and testing (1/3) sets
 Use cross validation, e.g., 10-fold cross validation
 Use all the data for training
 but apply a statistical test (e.g., chi-square) to estimate 
whether expanding or pruning a node may improve the 
entire distribution
 Use minimum description length (MDL) principle
 halting growth of the tree when the encoding is minimized
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 40
40
Chi-Square Test 
(based on Quinlan, Induction of Decision Trees, 1986)
 Notation
 A - splitting (branching) attribute (e.g., Test)
 v - domain size of attribute A (3: 0-600, 600-700, 700+)
 Ci - subset of records containing value i of attribute A (Test 
< 600: 2)
 c – number of classes (3: low, medium, high)
 ej - number of records belonging to class j in the entire data 
set C (Low = 1, Medium = 2, High = 2)
 oij - number of records belonging to class j in subset Ci
(Test < 600,  GPA = Low : 1)
  - significance level
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 41
41
Chi-Square Test (cont.)
 Null Hypothesis: attribute A is irrelevant to classifying the 
records in data set C
 Alternative Hypothesis: attribute A affects the class 
distribution in data set C
 Expected number of class j records in subset Ci:
 ej – actual number of records in class j
 oij - actual number of class j records in subset Ci
))1)(1((~
'
)'( 2
1
2
1




cv
e
eov
i ij
ijij
c
j

Statistic:

 


c
j
ijc
j
j
j
ij o
e
e
e
1
1
'
v - domain size of A
c – number of classes
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 42
Chi-Square Test – Student 
Example
 Entire data set (before splitting): eLow = 1, eMedium = 2, eHigh = 2
 Splitting by Test
 Test < 600: Low = 1, Medium = 1
 e’low= (1/5)*2 = 0.4, e’medium = (2/5)*2 = 0.8. e’high = (2/5)*2 = 0.8. 
 Test =  600-700: Medium = 1, High = 1
 e’low= (1/5)*2 = 0.4, e’medium = (2/5)*2 = 0.8. e’high = (2/5)*2 = 0.8.
 Test > 700: High = 1
 e’low= (1/5)*1 = 0.2, e’medium = (2/5)*1 = 0.4. e’high = (2/5)*1 = 0.4.
42

 


c
j
ijc
j
j
j
ij o
e
e
e
1
1
'
2
1


c
j
ijo
2
1


c
j
ijo
1
1


c
j
ijo
Data Mining (BGU) Prof. Mark Last
Chi-Square Test – Student Example (cont.)
March 18, 2019 Lecture No. 4 43
Test Grade (i) Total p_j
GPA (j) 0-600 600-700 Over 700
Low Actual 1 0 0 1 0.2
Expected 0.4 0.4 0.2 1
Statistic 0.9 0.4 0.2 1.5
Medium Actual 1 1 0 2 0.4
Expected 0.8 0.8 0.4 2
Statistic 0.05 0.05 0.4 0.5
High Actual 0 1 1 2 0.4
Expected 0.8 0.8 0.4 2
Statistic 0.8 0.05 0.9 1.75
Total 2 2 1 3.75 5
Statistic:
75.3
'
)'(
1
2
1




v
i ij
ijij
c
j e
eo 9.49)4(05.0
2 
Conclusion: do not split the node on Test Grade
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 44
44
Pessimistic Error Pruning (PEP)
 Uses training set to estimate error on new 
data
 Error estimate (relative frequency with 
continuity correction)
 probability of error (apparent error rate)
 where
 N = #examples
 nC = #examples in majority class
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 45
45
Pessimistic Error Pruning (cont.)
 Error of a node v (if pruned)
 where
 Nv = #examples at node v
 nC,v = #examples in majority class at node v
 Error of a subtree T
 Where
 l = leaf node of sub-tree T
 Prune if 
 Prunes in bottom-up fashion
 fast
 considered a weakness (on accuracy)






)(
)(
, )5.0(
)(
Tleafsl
l
Tleafsl
lCl
N
nN
Tq
Data Mining (BGU) Prof. Mark Last
March 18, 2019 Lecture No. 4 46
46
Example of Post-Pruning
Class = Yes 20
Class = No 10
Error = 10/30
Training Error (Before splitting) = 10/30
Pessimistic error = q(v) = (10 + 0.5)/30 = 10.5/30
Training Error (After splitting) = 9/30
Pessimistic error (After splitting) q(T) = (9 + 4 
0.5)/30 = 11/30
PRUNE!
Class = Yes 8
Class = No 4
Class = Yes 3
Class = No 4
Class = Yes 4
Class = No 1
Class = Yes 5
Class = No 1
A?
A1
A2 A3
A4
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-5\kdd05.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 1
Lecture No. 5 – Decision Tree 
Learning II
 Rule Extraction
 Discretization of Continuous Attributes
 Alternative Splitting Rules
 Information Gain Ratio
 Gini Index
 Twoing
 CART Overview
 Comparison of Decision Trees
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 2
2
Rule Extraction – Student Admission
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
Complete set of extracted rules
If (Test <600) and (Place of Birth = Israel) Then Grade = Low
If (Test <600) and (Place of Birth = Abroad) Then Grade = Medium
If (600<Test <700) and (Gender = F) Then Grade = High
If (600<Test <700) and (Gender = M) Then Grade = Medium
If (Test >700) Then Grade = High
Data Mining (BGU) Prof. Mark Last
Rule Coverage and Accuracy
 Coverage of a rule:
 Fraction of records that satisfy the antecedent of a rule
 Accuracy of a rule:
 Fraction of records that satisfy both the antecedent and 
consequent of a rule
 What are the coverage and the accuracy of each rule 
extracted from the following trees?
Lecture No. 5 3
Test
< 600 600 - 700 Over 700
MediumMedium High
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel Abroad
Low Medium
Data Mining (BGU) Prof. Mark Last
Characteristics of Rule-Based Classifier
 Mutually exclusive rules (חוקים זרים)
 Classifier contains mutually exclusive rules if the rules are 
independent of each other
 Every record is covered by at most one rule
 Exhaustive rules ( חוקים בעלי כיסוי מלא)
 Classifier has exhaustive coverage if it accounts for every 
possible combination of attribute values
 Each record is covered by at least one rule
 Example: mutually exclusive and exhaustive?
 If (Test < 600) Then Grade = Low
 If (600 ≤ Test < 700) Then Grade = Medium
 If )Test  ≥ 700) Then Grade = High
4Lecture No. 5
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 5
Lecture No. 5 – Decision Tree 
Learning II
 Rule Extraction
 Discretization of Continuous Attributes
 Alternative Splitting Rules
 Information Gain Ratio
 Gini Index
 Twoing
 CART Overview
 Comparison of Decision Trees
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 6
Discretization Algorithm
Attribute 
range: T
Computational 
Complexity: d log d
d – number of distinct values
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 7
Discretization Algorithm
(continued)
 Notation
 S - entire set of instances
 A - attribute (feature)
 T - threshold (partition boundary)
 S1 - set of instances below the threshold (v  T)
 S2 - set of instances above the threshold (v > T)
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 8
Discretization Algorithm
(continued)
 Entropy induced by T:
 Information Gain:
 Gain (A,T;S) = Ent (S) - E(A,T;S)
 Example:
)(
||
||
)(
||
||
);,( 2
2
1
1 SEnt
S
S
SEnt
S
S
STAE 
Record 1 2 3 4 5
Value 1 1.5 1.5 1.7 2.1
Class 0 1 0 1 1
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 9
Discretization Example
Value <= Pos (0) Neg (1) Total Prob (0) Prob (1)
1 1 0 1 1.00 0.00
1.5 2 1 3 0.67 0.33
1.7 2 2 4 0.50 0.50
2.1 2 3 5 0.40 0.60
Value > Pos (0) Neg (1) Total Prob (0) Prob (1)
1 1 3 4 0.25 0.75
1.5 0 2 2 0.00 1.00
1.7 0 1 1 0.00 1.00
2.1 0 0 0
Record 1 2 3 4 5
Value 1 1.5 1.5 1.7 2.1
Class 0 1 0 1 1
Value <= plogp plogp Total Entropy Info Gain
1 0.000 0.000 0.649 0.322
1.5 0.390 0.528 0.918 0.551 0.420
1.7 0.500 0.500 1.000 0.800 0.171
2.1 0.529 0.442 0.971 0.971
Value > plogp plogp
1 0.500 0.311 0.811
1.5 0.000 0.000
1.7 0.000 0.000
2.1
The best 
threshold
)(
||
||
)(
||
||
);,( 2
2
1
1 SEnt
S
S
SEnt
S
S
STAE 
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 10
Lecture No. 5 – Decision Tree 
Learning II
 Rule Extraction
 Discretization of Continuous Attributes
 Alternative Splitting Rules
 Information Gain Ratio
 Gini Index
 Twoing
 CART Overview
 Comparison of Decision Trees
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 11
Splitting Rules
 General Requirements
 Function of class probabilities f (p1, p2, …)
 Symmetric around 1/2 f (p) = f (1-p)
 Convex function
 Possible splitting functions (rules)
 Entropy (Information Gain and Gain Ratio)
 Twoing
 Gini Index
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 12
Information Gain Ratio
 ID3 selects the attribute which maximizes the  
mutual information (information gain): 
 gain(A) = I (p, n) - E(A)
 I (p, n) – unconditional entropy (does not depend on the choice of 
A)
 E (A) - conditional entropy after splitting the root node by the test 
attribute A
 The information gain is maximal when E (A) is equal 
to zero
 E (A) = 0 if for each value of A
 Either all examples are positive
 Or all examples are negative
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 13
Gain Ratio (cont.)
 The problem with multi-valued and continuous 
attributes in noisy databases
 The probability of  a subset of examples to have the same 
class increases monotonically with a decrease in the subset 
size
 The extreme case is a subset of one example
 The average size of a subset decreases with an increase in 
the total number of attribute values (e.g., attribute Date)
 Conclusion
 Information gain is biased towards multi-valued and 
continuous attributes 
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 14
Gain Ratio (cont.)
 The Gain Ratio Approach
 “Punish” the multi-valued attributes via dividing 
(normalizing) their information gain by the Split 
Information:
 The Split Information represents the entropy of the tested 
attribute (in contrast to the entropy of the target attribute)
 The Gain Ratio: Gain(A)/SplitInfo(A)
)
||
||
(log
||
||
)( 2
1 D
D
D
D
DSplitInfo
j
v
j
j
A  

Data Mining (BGU) Prof. Mark Last
Lecture No. 5 15
Gain Ratio – Student Example
שם פרטי שם משפחה מגדר מקום לידה ציון פסיכומטרי ממוצע ציונים
First Name Last Name Gender Place of Birth Test Grade GPA
David Cohen M USA Over 700 High
Ophir Levy M Israel 600-700 Medium
Sharon Grosman F Israel 600-700 High
Diana Liberman F Russia 0-600 Medium
Anat Klein F Israel 0-600 Low
Let us assume that the attribute 
“Place of Birth” has three
possible values: USA, Israel, 
and Russia
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 16
Information Gain – Place of Birth
Place of 
Birth Total
Israel USA Russia
Low 1 0 0 1
p 0.333 0.000 0.000
-logp 1.585 0.000 0.000
Medium 1 0 1 2
p 0.333 0.000 1.000
-logp 1.585 0.000 0.000
High 1 1 0 2
p 0.333 1.000 0.000
-logp 1.585 0.000 0.000
Total 3 1 1 5
p 0.60 0.20 0.20 0.80
Entropy 1.585 0.000 0.000 0.951
Gain 0.571
522.1)( DInfo
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 17
Split Information and Gain Ratio
Student Example
Place of 
Birth Total
Israel USA Russia
Total 3 1 1 5
p 0.60 0.20 0.20 1.00
-logp 0.737 2.322 2.322 1.371
Gain 0.571
Gain Ratio 0.416
•Split Information
Information Gain
Information Gain 
Ratio
Gain Ratio (Test Grade) = 0.474
Gain Ratio (Gender) = 0.176
Gain Ratio (Place of Birth) = 0.416
522.1)( DInfo
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 18
Gini index
 All attributes are assumed continuous-valued
 Assume there exist several possible split 
values for each attribute
 May need other tools, such as clustering, to 
get the possible split values
 Can be modified for categorical attributes
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 19
Gini Splitting Rule
 Looks for the largest class in the data set and 
strives to isolate it from all other classes
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 20
Gini Index
 If a data set T contains examples from n classes, 
gini index, gini(T) is defined as
 where pj is the relative frequency of class j in T.
 If a data set T is split into two subsets T1 and T2
with sizes N1 and N2 respectively, the gini index 
of the split data contains examples from n
classes, the gini index gini(T) is defined as
 Reduction in Impurity:
 The attribute provides the smallest ginisplit(T)
(or the largest reduction in impurity) is chosen 
to split the node (need to enumerate all possible 
splitting points for each attribute).



n
j
p jTgini
1
21)(
)()()( 2
2
1
1
Tgini
N
N
Tgini
N
NTginisplit 
0.00
1.00
2.00
0.00.51.0
p
Gini vs. Entropy
Entropy
gini
)()()( DginiDginiAgini
A

Data Mining (BGU) Prof. Mark Last
Lecture No. 5 21
Gini Splitting Rule - Example
Prob
A B C D Gini Gini Split Gini Drop
0.80 0.00 0.00 0.20 0.32 0.400 0.300
0.00 0.60 0.40 0.00 0.48
0.40 0.30 0.20 0.10 0.70
Prob
A B C D Gini Gini Split Gini Drop
1.00 0.00 0.00 0.00 0.00 0.367 0.333
0.00 0.50 0.33 0.17 0.61
0.40 0.30 0.20 0.10 0.70
Better 
split
Split 1
Split 1
Split 2
Split 2
Class: A B C D Total Pl/Pr
Age <= 65 40 0 0 10 50 0.5
Age > 65 0 30 20 0 50 0.5
Total 40 30 20 10 100
Class: A B C D Total Pl/Pr
Age <= 65 40 0 0 0 40 0.4
Age > 65 0 30 20 10 60 0.6
Total 40 30 20 10 100



n
j
p jTgini
1
21)(
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 22
Twoing Splitting Rule (CART)
Source: L. Breiman, J. Friedman, R. Olshen, and C. Stone (1984), Classification and Regression Trees, 
Pacific Grove: Wadsworth
 Maximize
 Notation
 pL – proportion of cases going to the left node
 pR – proportion of cases going to the right 
node
 j – class index
 p (j/tL) – probability of class j at the left node
 p (j/tR) – probability of class j at the right 
node
 Attempts to find groups of up to 50% of the 
data each
 If impossible - power-modified twoing
2
)/()/(
4







j
RL
RL tjptjp
pp
t
tL tR
pL pR
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 23
Twoing Splitting Rule - Example
Which split is better?
Split 1 Split 2
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 24
Twoing Splitting Rule - Example
Class: A B C D Total Pl/Pr
Age <= 65 40 0 0 10 50 0.5
Age > 65 0 30 20 0 50 0.5
Total 100
Class: A B C D Total Pl/Pr
Age <= 65 40 0 0 0 40 0.4
Age > 65 0 30 20 10 60 0.6
Total 100
Prob
A B C D
0.80 0.00 0.00 0.20
0.00 0.60 0.40 0.00
Prob
A B C D
1.00 0.00 0.00 0.00
0.00 0.50 0.33 0.17
Abs
A B C D Total Twoing
0.800 0.600 0.400 0.200 2.000 0.250
Abs
A B C D Total Twoing
1.000 0.500 0.333 0.167 2.000 0.240
Better 
split
Split 1
Split 2
2
)/()/(
4







j
RL
RL tjptjp
pp
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 25
Using Splitting Rules
(Gini, Twoing, Entropy)
 Gini -- is usually best for yes/no outcomes
 Twoing - similar to entropy but more flexible 
because it has a tuning parameter
 excellent for multi-class outcomes
 twoing excellent for hard to classify problems
 problems where accuracy for all methods will be low
 inherently difficult problems or low signal/noise ratio
 Entropy- popular in Machine Learning literature
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 26
Lecture No. 5 – Decision Tree 
Learning II
 Rule Extraction
 Discretization of Continuous Attributes
 Alternative Splitting Rules
 Information Gain Ratio
 Gini Index
 Twoing
 CART Overview
 Comparison of Decision Trees
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 27
CART Algorithm
Main Steps
 Grow the maximal tree based on the entire data set
 A binary splitting procedure
 Splitting rules
 Stopping criteria
 Derive a set of pruned sub-trees
 Create “efficiency frontier”
 Select the best tree by using validation set or cross-
validation
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 28
CART : Binary Splitting 
Procedure
 Continuous (Ordinal) Attributes
 Each distinct value is considered for threshold
 Branching rule: x  C
 M possible splits (M - number of distinct values)
 Nominal (Categorical) Attributes
 The branching rule is determined separately for 
each possible value
 2M-1 - 1 possible splits (M - number of values)
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 29
CART : Stopping Criteria
 Splitting is impossible
 One case left in a node
 All the cases in the node have the same target 
value
 Other reasons
 Too few cases in the node (default = 10 cases)
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 30
Pruning Trees
t2
t4 t5
t6 t7
t1
t2 t3
t4 t5
t6 t7
t1
t2 t3
Branch 
Tt2Tree T
Sub-tree 
T - Tt2
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 31
Deriving a set of pruned sub-trees
 Objective: minimizing the cost-complexity 
function
 T - a tree
 R (T) - the training error rate of a tree
 R (T) - the cost-complexity of a tree
 - number of terminal nodes in a tree
  - complexity parameter (real number, greater than 
zero)
TTRTR
~
)()(  
T
~
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 32
CART Pruning Algorithm
 Step 1 - Initialize the list of optimal trees with the 
maximal tree
 Step 2 - Initialize  = 0
 Step 3 - Increase  until the tree ceases to be optimal
 Step 4 - Find a new sub-tree, which is optimal with 
the new value of 
 Step 5 - Add the new sub-tree to the list of optimal 
trees.  
 Step 6 - If the new sub-tree has more than one 
terminal node, go to Step 3.  Otherwise, stop.
TTRTR
~
)()(  
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 33
CART Student Example
Maximal Tree ( = 0)
Test
< 600 600 - 700 Over 700
High
GenderPlace of Birth
F M
High Medium
Israel (1) Abroad (1)
Low Medium
Error
Before pruning: 0% 
After pruning: 50%
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 34
CART Student Example )cont’d( 
Removing Place of Birth
 Cost-complexity of the single node t
 R({t}) = R(t) + *1 = 0.50 + 
 Cost-complexity of the branch Tt
 R(Tt) = R(Tt) + *|Ťt| = 0 +*2
 The critical value of 
 R({t}) = R(Tt) 
 0.50 +  = 2 
  = 0.50
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 35
CART Student Example )cont’d( 
New Sub-Tree ( = 0.50)
Test
< 600 600 - 700 Over 700
MediumLow High
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 36
Lecture No. 5 – Decision Tree 
Learning II
 Rule Extraction
 Discretization of Continuous Attributes
 Alternative Splitting Rules
 Information Gain Ratio
 Gini Index
 Twoing
 CART Overview
 Comparison of Decision Trees
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 37
Comparison of Decision Trees
(based on Lim et al., Machine Learning, 40, 203–228, 2000)
Computational Complexity
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 38
Comparison of Decision Trees
Error Rate
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 39
Comparison of Decision Trees
Tree Size
Data Mining (BGU) Prof. Mark Last
Lecture No. 5 40
Lectures No. 4-5: Summary
 Classification tasks involve model construction and 
model testing
 Decision trees are one of the most popular 
classification models
 Decision trees are usually constructed in a top-down 
recursive divide-and-conquer manner
 Overfitting can be avoided with pre-pruning and 
post-pruning techniques
 Most popular splitting criteria include Gini, Twoing, 
and Entropy
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-6\kdd06.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 1
Lecture No. 6 – Info-Fuzzy Network 
 IFN Overview
 Network Construction Procedure
 Prediction and Rule Extraction
 Main Characteristics
 Comparative Evaluation
 Software
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 2
Info-Fuzzy Network 
(IFN)
Full Description: 
1) O. Maimon and M. Last, Knowledge Discovery and Data Mining – The Info-
Fuzzy Network (IFN) Methodology, Kluwer Academic Publishers, Boston, 
December 2000.  119 citations (April 2019)
2) M. Last and O. Maimon, A Compact and Accurate Model for Classification, 
IEEE Transactions on Knowledge and Data Engineering, Vol. 16, No. 2, pp. 203-
215, February 2004. 80 citations (April 2019)
About  15,500 results on Google (April 2019)
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 3
The Fragmentation Problem of 
Decision Trees
 In the top-down decision-tree construction procedure, 
the number of training instances at a node decreases 
with every split 
 Conclusion 
 Recursive partitioning leads to statistically insignificant 
samples at each branch
 IFN (Info-Fuzzy Network) Solution to the 
Fragmentation Problem
 Repetitive partitioning of all training instances in every 
layer
 Statistical significance testing at every node 
3
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 4
Lecture No. 6 – Info-Fuzzy Network 
 IFN Overview
 Network Construction Procedure
 Prediction and Rule Extraction
 Main Characteristics
 Comparative Evaluation
 Software
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 5
IFN Example - Credit Approval Dataset
Source: http://archive.ics.uci.edu/ml/datasets/Credit+Approval
5
Attribute Domain Type Use in Network
A1 (Sex) 0, 1 Nominal Candidate input
A2 (Age) 13.75 - 80.25 Continuous Candidate input
A3 (Mean time at addresses) 0 - 28 Continuous Candidate input
A4 (Home status) 1, 2, 3 Nominal Candidate input
A5 (Current occupation) 1 - 14 Nominal Candidate input
A6 (Current job status) 1 - 9 Nominal Candidate input
A7 (Mean time with employers) 0 - 28.5 Continuous Candidate input
A8 (Other investments) 0, 1 Nominal Candidate input
A9 (Bank account) 0, 1 Nominal Candidate input
A10 (Time with bank) 0 - 67 Continuous Candidate input
A11 (Liability reference) 0, 1 Nominal Candidate input
A12 (Account reference) 1, 2, 3 Nominal Candidate input
A13 (Monthly housing expense) 0 - 2000 Continuous Candidate input
A14 (Savings account balance) 1 - 100001 Continuous Candidate input
Class (Accept / Reject) 0, 1 Nominal Target
C
an
d
id
ate In
p
u
t F
eatu
res
T
arg
et
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 6
IFN Construction Procedure (0)
Credit Approval Dataset
6
0
Root 
node Reject
Accept
Iteration No. 0: no input attributes, no hidden layers
Target layer
(Class)
0
1
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 7
IFN Construction Procedure (1) 
Credit Approval Dataset
7
0
1
2
0
1
Other 
investments 
= No
Other 
investments 
= Yes
Layer 1
(Other 
investments )
Root 
node
Hidden node
Reject
Accept
Target layer
(Class)
Iteration No. 1: one input attribute, one hidden layer
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 8
IFN Construction Procedure (2) 
Credit Approval Dataset
8
0
1
3
4
2
0
1
Other 
investments 
= No
Other 
investments 
= Yes
Balance 
between 
$1 and 
$445
Balance
 $445
Layer 1
(Other 
investments )
Layer 2
(Balance)
Root 
node
Hidden node
-0.089
0.3303
Reject
Accept
Target layer
(Class)
Iteration No. 2: two input attributes, two hidden layers
Terminal 
node
Rule weight
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 9
IFN Construction Procedure (3) 
Credit Approval Dataset
9
0
1
3
4
5
6
2
0
1
Other 
investments 
= No
Other 
investments 
= Yes
Balance 
between 
$1 and 
$445
Balance
 $445
Bank 
account=Yes
Bank 
account=No
Layer 1
(Other 
investments )
Layer 2
(Balance)
Layer 3
(Bank 
Account)Root 
node
Hidden node
-0.089
0.3303
-0.02
0.2106
-0.0141
0.016
-0.0492
0.1313
Reject
Accept
Target layer
(Class)
Iteration No. 3: three input attributes, three hidden layers
Rule weight
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 10
Network Induction Algorithm
Input
• Extended relation schema (partition of attribute set)
• Set of training records
• Minimum significance level (default = 0.1%)
Output
• Set of selected input attributes
• Information-theoretic network
Step 1 - Initialize the information-theoretic network.
Step 2 - While the maximum number of hidden layers is not exceeded:
Step 2.1 - Find a candidate input attribute maximizing the statistically 
significant conditional mutual information (“the best candidate 
attribute”).
Step 2.2 - If the maximum conditional mutual information is greater 
than zero, make the best candidate attribute an input attribute and 
define a new layer of hidden nodes; else stop.
Step 3 – Return the set of selected attributes and the network structure
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 11
IFN: Conditional mutual 
information (MI) at a node z
11
MI (Ai’ ; Ai / z) = 
)/()/(
)/(
log);;(
''
''
''
1
0
1
0'
'
zVPzVP
zVP
zVVP
ijji
ij
ji
jiij
M
j
M
j
i i






Ai - target attribute No. i
Ai’ - candidate input attribute No. i’
Vij - value No. j of attribute Ai
z - network node (representing a conjunction of input attribute values)
P (Vij/ z) - an estimated conditional (a posteriori) probability of Vij given the 
node z.
P (Vi’j’
ij/ z) - an estimated conditional (a posteriori) probability of Vi’j’ and Vij
given the node z.
P (Vij; Vi’j’; z) - an estimated joint probability of Vi’j’Vij, and the node z 
Data Mining (BGU) Prof. Mark Last
j'/ j 0Cond. Joint 1Cond. Joint Total Cond.
0 306 0.4435 0.4435 23 0.0333 0.0333 329 0.4768
1 77 0.1116 0.1116 284 0.4116 0.4116 361 0.5232
Total 383 0.5551 307 0.4449 690
Lecture No. 6 12
12
Conditional MI Example
Other Investments (Node 0)
Conditional mutual information MI (Ai’ ; Ai / z) = 0.426 bits
j'/ j 0 Cond. Joint 1 Cond. Joint Total Cond.
0 306 =306/690 =306/690 23 =23/690 =23/690 329 =329/690
1 77 =77/690 =77/690 284 =284/690 =284/690 361 =361/690
Total 383 =383/690 307 =307/690 690
• MI (Ai’ =0 ; Ai =0 / z) = 0.443 * log2(0.443 / (0.477*0.555))= 0.3303
• MI (Ai’ =1 ; Ai =0 / z) = 0.112 * log2(0.112 / (0.523*0.555))= -0.1540
• MI (Ai’ =0 ; Ai =1 / z) = 0.033 * log2(0.033 / (0.477*0.445))= 0.089-
• MI (Ai’ =1 ; Ai =1 / z) = 0.412 * log2(0.412 / (0.523*0.445))= 0.3384
j
M
j
M
ij i j
i j
ij
i j ij
ii
P V V z
P V z
P V z P V z'
' '
' '
' '
'
( ; ; ) log
( / )
( / ) ( / )



 
0
1
0
1
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 13
13
IFN: Testing Statistical Significance of MI
Likelihood-Ratio Statistic (Attneave, 1959):
 Ai - target attribute No. i
 Ai’ -candidate input attribute No. i’
 z - network node (representing a conjunction of input attribute values)
 E*– total number of training cases
 MI (Ai’ ; Ai / z) – conditional mutual information 
)/; ()2(ln2)/; ( '
*
'
2 zAAMIEzAAG iiii 
))1)(()1)(((~|
0)/;(:
'
22
'0
0


zNTzNIG
zAAMIH
iiH
ii

NI i’ (z) - number of values of a candidate input attribute i’ at node z
NT i (z) - number of values of a target attribute i at node z
A node is split if H0 is 
rejected at the 0.1% 
significance level
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 14
14
Likelihood-Ratio Example
Other Investments (Node 0)
• Conditional mutual information MI (Ai’ ; Ai / z) = 0.426 bits
• Likelihood-Ratio Statistic G2 (Ai’ ; Ai / z) = 2*ln2*690*0.426 = 407
• Degrees of Freedom = (2-1)*(2-1) = 1
• Significance level >> 0.1%
• Conclusion: reject H0 (consider Other Investments as the next input 
attribute)
j'/ j 0Cond. Joint 1Cond. Joint Total Cond.
0 306 0.4435 0.4435 23 0.0333 0.0333 329 0.4768
1 77 0.1116 0.1116 284 0.4116 0.4116 361 0.5232
Total 383 0.5551 307 0.4449 690
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 15
15
MI (Ai’ ; Ai) = 
)/;(
)(
'
'
zAAMI i
truezSplit
Layerz
i
i



IFN: Conditional Mutual Information in a 
Layer i’
Ai - target attribute No. i
Ai’ - candidate input attribute No. i’
z - network node
MI (Ai’ ; Ai / z) – conditional mutual information between Ai and
Ai’ given node z
Example: Layer i’ = 0; Ai’ = Other Investments
MI (Other Investments; Class) = MI (Other Investments; 
Class / z = 0) = 0.426 bits 
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 16
16
Global Discretization of Continuous Attributes
 Input: The first and the last distinct values in the interval S of 
a continuous attribute Ai’
 Step 1 – For every distinct value T included in the interval S
(except for the first distinct value) Do:
 Step 1.1 –For every node z of the final hidden layer Do: 
 Step 1.1.1 - Calculate the likelihood-ratio test for the partition of the interval S at 
the threshold T and the target attribute Ai given the node z
First interval:  Ai’ < T; Second interval: Ai’  T
 Step 1.1.2 - If the likelihood-ratio statistic is significant, mark the node as “split” 
by the threshold T
 Step 1.1.3 - End Do
 Step 1.2 – End Do
 Step 2 – Find the threshold Tmax maximizing the sum of 
conditional mutual information over all nodes
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 17
Global Discretization Procedure (2)
 Step 3 – If the maximum estimated conditional mutual 
information is greater than zero, then Do:
 Step 3.1 - For every node z of the final hidden layer Do:
 Step 3.1.1 – If the node z is split by the threshold Tmax, mark the node as split by
the candidate input attribute Ai’
 Step 3.2 - Partition each sub-interval of S 
 Step 3.3 - End Do
 Step 4 - Else return the list of threshold values for Ai’
17
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 18
Dynamic Discretization Procedure (cont.)
0
1 2
Layer No. 0
(the root node)
Layer No. 1
(First input 
attribute)
2 values
Example: discretization of the second input attribute 
in the network
First split
Th1
Th1
Th1
Th1
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 19
Dynamic Discretization Procedure (cont.)
0
1 2
Layer No. 0
(the root node)
Layer No. 1
(First input 
attribute)
2 values
Second split
First split
Th1
Th1
Th1
Th1
Th2
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 20
20
Conditional Mutual Information (Global 
Discretization Procedure) 
 P (S y/ S, z) - an estimated conditional (a posteriori) probability of a sub-
interval Sy given the interval S and the node z
 P (Ct / S, z) - an estimated conditional (a posteriori) probability of a value Ct
of the target attribute T given the interval S and the node z
 P (S y ; Ct / S, z) - an estimated joint probability of a value  of the target 
attribute T and a sub-interval Sy given the interval S and the node z
 P (S y; Ct ; z) - an estimated joint probability of a value  of the target attribute 
T, a sub-interval Sy, and the node z
),/;( zSTThMI
),/(),/(
),/;(
log);;(
1
0
2
1 zSCPzSSP
zSCSP
zCSP
ty
ty
ty
M
t y
i



 
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 21
21
Global Discretization 
Example (1) 
Balance (Node 1, T = 445)
• Conditional mutual information MI (Ai’ ; Ai / z) = 0.0000546 bits
• Likelihood-Ratio Statistic G2 (Ai’ ; Ai / z) = 0.0522
• Degrees of Freedom = (2-1)*(2-1) = 1
• Significance level < 18%
• Conclusion: do not reject H0 (do not split the Balance attribute at this node 
for the specified threshold)
0
1
2
Other 
investments 
= No
Other 
investments 
= Yes
Balance < $445
Balance  $445
j' / j 0 Cond. Joint 1 Cond. Joint Total Cond.
<445 271 0.824 0.393 20 0.061 0.029 291 0.8845
>=445 35 0.106 0.051 3 0.009 0.004 38 0.1155
Total 306 0.930 23 0.070 329
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 22
22
Global 
Discretization 
Example (2) 
Balance (Node 2, T = 445)
• Conditional mutual information MI (Ai’ ; Ai / z) = 0.0592 bits
• Likelihood-Ratio Statistic G2 (Ai’ ; Ai / z) = 56.6564
• Degrees of Freedom = (2-1)*(2-1) = 1
• Significance level >> 0.1%
• Conclusion: reject H0 (split the Balance attribute at this node for the specified 
threshold)
j' / j 0 Cond. Joint 1 Cond. Joint Total Cond.
<   445 74 0.205 0.107 156 0.432 0.226 230 0.637
>=    445 3 0.008 0.004 128 0.355 0.186 131 0.363
Total 77 0.213 284 0.787 361
0
1
2
Other 
investments 
= No
Other 
investments 
= Yes
Balance < $445
Balance  $445
Data Mining (BGU) Prof. Mark Last
Credit Dataset - Layer 0
23Lecture No. 6
Attribute Significant 
Conditional Mutual 
Information
A1 (Sex) 0
A2 (Age) 0.023
A3 (Mean time at addresses) 0.041
A4 (Home status) 0.03
A5 (Current occupation) 0.109
A6 (Current job status) 0.05
A7 (Mean time with employers) 0.123
A8 (Other investments) 0.426
A9 (Bank account) 0.156
A10 (Time with bank) 0.214
A11 (Liability reference) 0
A12 (Account reference) 0
A13 (Monthly housing expense) 0.051
A14 (Savings account balance) 0.123
Data Mining (BGU) Prof. Mark Last
Credit Dataset - Layer 1
24Lecture No. 6
Attribute Significant 
Conditional Mutual 
Information
A1 (Sex) 0
A2 (Age) 0
A3 (Mean time at addresses) 0.041
A4 (Home status) 0
A5 (Current occupation) 0
A6 (Current job status) 0
A7 (Mean time with employers) 0.018
A9 (Bank account) 0.055
A10 (Time with bank) 0.055
A11 (Liability reference) 0
A12 (Account reference) 0.022
A13 (Monthly housing expense) 0.027
A14 (Savings account balance) 0.059
Data Mining (BGU) Prof. Mark Last
Credit Dataset - Layer 2
25Lecture No. 6
Attribute Significant 
Conditional Mutual 
Information
A1 (Sex) 0
A2 (Age) 0
A3 (Mean time at addresses) 0.027
A4 (Home status) 0
A5 (Current occupation) 0
A6 (Current job status) 0
A7 (Mean time with employers) 0
A9 (Bank account) 0.031
A10 (Time with bank) 0.031
A11 (Liability reference) 0
A12 (Account reference) 0
A13 (Monthly housing expense) 0.022
Data Mining (BGU) Prof. Mark Last
Credit Dataset - Layer 3
26Lecture No. 6
Attribute Significant 
Conditional Mutual 
Information
A1 (Sex) 0
A2 (Age) 0
A3 (Mean time at addresses) 0
A4 (Home status) 0
A5 (Current occupation) 0
A6 (Current job status) 0
A7 (Mean time with employers) 0
A10 (Time with bank) 0
A11 (Liability reference) 0
A12 (Account reference) 0
A13 (Monthly housing expense) 0
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 27
Lecture No. 6 – Info-Fuzzy Network 
 IFN Overview
 Network Construction Procedure
 Prediction and Rule Extraction
 Main Characteristics
 Comparative Evaluation
 Software
Data Mining (BGU) Prof. Mark Last
Prediction
28
Predicted Value (maximum a posteriori):
(of the target attribute Ai at the node z)
)/(V maxarg* ij zPj
j

Lecture No. 6
28
0
1
3
4
5
6
2
0
1
Other 
investments 
= No
Other 
investments 
= Yes
Balance 
between 
$1 and 
$445
Balance
 $445
Bank 
account=Yes
Bank 
account=No
0.07
0.93
0.023
0.977
0.491
0.509
0.161
0.839
Reject
Accept
Lecture No. 6
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 35
Rule Extraction and Scoring
35
Connection Weight: w P V z
P V z
P Vz
ij
ij
ij
ij
 =  ( ; ) log
( / )
( )

Interpretation: mutual information between the node z and the 
value j of the target attribute Ai
wz
ij > 0: The probability of Vij at z is higher than average
wz
ij < 0: The probability of Vij at z is lower than average
Vij - value No. j of target attribute Ai
P (Vij; z) - an estimated joint probability of Vij and the node z
P (Vij) - an estimated unconditional (a priori) probability of Vij
P (Vij/ z) - an estimated conditional (a posteriori) probability of Vij
given the node z
Use ?
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 36
36
Rule Extraction Example
Credit Dataset
Example – Rule No. 1 (Connection 1  0)
Other investments = No: 329 records
Other investments = No and Class = Reject: 306 records
Class = Reject: 383 records
Total records: 690
P (Vij) = 383 / 690 = 0.5551 (unconditional probability)
P (Vij/ z) = 306 / 329 = 0.9301 (conditional probability)
P (Vij; z) = 306 / 690 = 0.4435 (joint probability)
Rule Weight: 0.4435*log (0.9301 / 0.5551) = 0.3303
w P V z
P V z
P Vz
ij
ij
ij
ij
 =  ( ; ) log
( / )
( )

0
1
3
4
5
6
2
0
1
Other 
investments 
= No
Other 
investments 
= Yes
Balance 
between 
$1 and 
$445
Balance
> $445
Bank 
account=Yes
Bank 
account=No
-0.089
0.3303
-0.02
0.2106
-0.0141
0.016
-0.0492
0.1313
Reject
Accept
units?
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 37
Lecture No. 6 – Info-Fuzzy Network 
 IFN Overview
 Network Construction Procedure
 Prediction and Rule Extraction
 Main Characteristics
 Comparative Evaluation
 Software
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 38
38
))1(),...,1(/);((
 = );(
'''
1


sAAAsAMI
IAMI
iiii
m
s
ii
The overall decrease in conditional entropy of the target 
attribute is equal to the sum of drops in conditional 
entropy across the network hidden layers (based on the 
Chain Rule)
Implication: IFN can be constructed incrementally
IFN Characteristic 1
Ai – target attribute i
Ii - set of input attributes in the network of the target attribute i
m - total number of layers (input attributes )
Ai’(s) –input attribute i’ associated with the layer s
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 39
39
IFN Characteristic 1 - Example
Credit Dataset - Summary Table
Attribute Mutual Conditional Conditional Split MI to
Iteration Name Information MI Entropy Nodes Attributes
0 Other investments (A8) 0.426 0.426 0.566 1 0.426
1 Balance (A14) 0.485 0.059 0.506 1 0.243
2 Bank account (A9) 0.516 0.031 0.475 1 0.172
516.0031.0059.0426.0))1(),...,1(/);(( = );( '''
1


sAAAsAMIIAMI iiii
m
s
ii
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 40
40
ij
z
M
jFz
ii wIAMI
i




1
0
);(
The sum of connection weights at all terminal nodes is 
equal to the estimated mutual information between the  set 
of input attributes Ii and the target attribute Ai (based on 
the definition of mutual information)
Implication: the rule weights represent the contribution of each 
terminal node to the overall mutual information
IFN Characteristic 2
F – set of terminal nodes z
Mi – number of distinct values (classes) of the target attribute Ai
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 41
41
IFN Characteristic 2 – Example
Credit Dataset - Extracted Rules
 No. Rule  Weight
1  If Other investments is      0  then Class is   0 0.330
2  If Other investments is      0  then Class is not   1 -0.089
3  If Other investments is      1 and Balance is more than 445.00000  then Class is not   0 -0.020
4  If Other investments is      1 and Balance is more than 445.00000  then Class is   1 0.211
5  If Other investments is      1 and Balance is between 1.00000 and 445.00000 and Bank account is      0  then Class is not   0 -0.014
6  If Other investments is      1 and Balance is between 1.00000 and 445.00000 and Bank account is      0  then Class is   1 0.016
7  If Other investments is      1 and Balance is between 1.00000 and 445.00000 and Bank account is      1  then Class is not   0 -0.049
8  If Other investments is      1 and Balance is between 1.00000 and 445.00000 and Bank account is      1  then Class is   1 0.131
Total 0.516
ij
z
M
jFz
ii wIAMI
i




1
0
);(
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 42
IFN Characteristic 3
Minimum Prediction Error Pe of a given info-fuzzy 
network can be estimated based on Fano’s 
inequality:
)1(log)()/( 2  ieeii MPPHIAH
Ai – target attribute i
Ii - set of input attributes in the network of the target attribute i
Mi – number of distinct values (classes) of the target attribute Ai
Implication: no testing set is needed to estimate the maximum 
achievable accuracy of a given network
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 43
IFN Characteristic 3 – Example
Credit Dataset 
Conditional entropy: H (Ai / Ii ) = 0.475
Number of classes Mi = 2
0475.0)12(log*102.0)898.0(log*898.0)102.0(log*102.0 222 
Mean Pe (10-fold cross-validation) = 0.159 > 0.102
0475.0)12(log*)1(log*)1()(log*
)/()1(log)(
222
2


eeeee
iiiee
PPPPP
IAHMPPH
Min Pe = 0.102
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 44
Lecture No. 6 – Info-Fuzzy Network 
 IFN Overview
 Network Construction Procedure
 Prediction and Rule Extraction
 Main Characteristics
 Comparative Evaluation
 Software
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 45
Comparison to Other Methods
From M. Last, O. Maimon, E. Minkov, “Improving Stability of Decision Trees”, International Journal of Pattern Recognition 
and Artificial Intelligence, Vol. 16, No. 2, pp. 145-159, 2002
Property CART / C4.5 EODG IFN
Tree construction strategy Recursive partitioning of a 
subset of training instances at 
each node
Repetitive partitioning of all 
training instances in every 
level
Repetitive partitioning of all 
training instances in every 
layer (except for instances at 
unsplit nodes)
Feature selection The best feature is selected for 
every node
All nodes in a given level are 
split on the same feature
All nodes in a given layer are 
split on the same feature
Splitting criteria CART: Gini, Twoing, Entropy
C4.5: Gain Ratio
Adjusted Mutual Information Conditional Mutual 
Information 
Splits on continuous features Binary (threshold) splits only
The same feature may be 
tested at different levels
Binary (threshold) splits only
The same feature may be 
tested at different levels
Multi-way splits
The same feature is not tested 
at more than one layer
Pre-pruning criteria Minimum number of cases for 
each outcome at a node
The instances are split on all 
features
Likelihood-Ratio Test
Post-pruning criteria CART: cost-complexity 
pruning
C4.5: Reduced error bottom-up 
pruning
Bottom-up error-based pruning
Top-down merging of nodes
No post-pruning
Target (Category) layer No Yes Yes
Kohavi (1995)
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 46
46
Number of Input Attributes
54% less attributes 
than C4.5
0.9% 
more 
errors 
than C4.5
0.0%
10.0%
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
Breast Chess Credit Diabetes Glass Heart Iris Liver Lung-
cancer
Wine
E
rr
o
r 
R
a
te
0
5
10
15
20
25
 
N
u
m
b
e
r 
o
f 
A
tt
ri
b
u
te
s
C4.5 Error EODG Error IFN Error IFN Attributes C4.5 Attributes
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 47
Network Size (Number of Nodes)
47
64% less nodes 
than C4.5
0.0%
10.0%
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
Breast Chess Credit Diabetes Glass Heart Iris Liver Lung-
cancer
Wine
E
rr
o
r 
R
a
te
0
10
20
30
40
50
60
70
80
N
u
m
b
e
r 
o
f 
N
o
d
e
s
C4.5 Error EODG Error IFN Error IFN Nodes EODG Nodes C4.5 Nodes
Data Mining (BGU) Prof. Mark Last
Lecture No. 6 48
IFN – Selected References
 O. Maimon and M. Last, “Knowledge Discovery and Data Mining – The Info-Fuzzy 
Network (IFN) Methodology”, Kluwer Academic Publishers, Boston, December 
2000.
 O. Maimon, A. Kandel, and M. Last, “Information-Theoretic Fuzzy Approach to Data 
Reliability and Data Mining”, Fuzzy Sets and Systems, Vol. 117, No. 2, pp. 183-194, 
Jan. 2001.
 M. Last, Y. Klein, A. Kandel, “Knowledge Discovery in Time Series Databases”, 
IEEE Transactions on Systems, Man, and Cybernetics, Volume 31: Part B, No. 1, pp. 
160-169, Feb. 2001.
 M. Last, A. Kandel, O. Maimon, “Information-Theoretic Algorithm for Feature 
Selection”, Pattern Recognition Letters, 22 (6-7), pp. 799-811, 2001.
 M. Last, “Online Classification of Nonstationary Data Streams”, Intelligent Data 
Analysis, Vol. 6, No. 2, pp. 129-147, 2002.
 M. Last, O. Maimon, E. Minkov, “Improving Stability of Decision Trees”, 
International Journal of Pattern Recognition and Artificial Intelligence, Vol. 16, No. 
2, pp. 145-159, 2002.
 M. Last and O. Maimon, “A Compact and Accurate Model for Classification”, IEEE 
Transactions on Knowledge and Data Engineering, Vol. 16, No. 2, pp. 203-215, 
February 2004. 
Data Mining (BGU) Prof. Mark Last
Summary
 Info-Fuzzy Network is constructed by repetitive 
partitioning of all training instances in every layer
 Each network layer is uniquely related to a single 
input attribute
 No testing set is needed to estimate the maximum 
achievable accuracy of a given network
 The IFN algorithm produces much more compact 
models than C4.5
 Recommended when interpretability overweighs accuracy!
Lecture No. 6 49
Lecture No. 6 50
IFN SOFTWARE
Location: Moodle
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-7\kdd07.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
1
Lecture No. 7 – Artificial Neural 
Networks
 Overview
 Classification by Backpropagation
 Deep Neural Networks
Data Mining (BGU) Prof. Mark Last
Artificial Neural Networks
Biological Motivation
 Human brain contains 
about 1011 neurons
 Each neuron is connected, 
on average, to 104 other 
neurons
 Neuron switching times: 
about 10-3 seconds 
(computers: 10-10 seconds)
 Complex human decisions 
(e.g., face recognition): 
very fast (~0.1 sec.)
 Massive Parallel 
Computation
2
Data Mining (BGU) Prof. Mark Last
3
Properties of Artificial Neural 
Networks
 Many neural-like threshold switching units
 Many weighted interconnections among units
 Highly parallel, distributed process
 Emphasis on tuning weights automatically
 Basic idea
 Model the target as a nonlinear function of multiple 
features (also known as …)
Data Mining (BGU) Prof. Mark Last
Neural Network Representation
Example: Autonomous Vehicle
 Input image: 30 X 32 = 960 
pixels
 Each input node is associated 
with one pixel
 There are four hidden units and 
30 output units 
 Each input node is connected to 
each hidden unit with a positive 
/ negative weight
 Each output unit is associated 
with a command (left, straight, 
right, etc.)
4
ALVINN 
(Autonomous Land Vehicle In 
a Neural Network)
Data Mining (BGU) Prof. Mark Last
5
Appropriate Problems for ANN
 Input and target attributes can be discrete or 
real-valued (have to be numeric)
 Output may include more than one target 
attribute (“a vector of attributes”)
 The training data may contain errors
 Each target attribute is a smooth, continuous 
function of input attributes 
 Form of target function is unknown
 Long training times are acceptable
 Human readability of results is unimportant
Data Mining (BGU) Prof. Mark Last
6
Sample Applications of ANN
(Define inputs and outputs)
 Optical Character Recognition (OCR)
 Voice Recognition
 Image Classification
 Information Retrieval
 Financial Prediction
 Natural Language Processing (Word 
Embedding)
Data Mining (BGU) Prof. Mark Last
7
Lecture No. 7 – Artificial Neural 
Networks
 Overview
 Classification by Backpropagation
 Deep Neural Networks
Data Mining (BGU) Prof. Mark Last
8
Classification by Backpropagation
 A neural network: A set of connected input/output units (neurons) where 
each connection has a weight associated with it
 During the learning phase, the network learns by adjusting the weights
 Also referred to as connectionist learning
Data Mining (BGU) Prof. Mark Last
Node Biases
Recall: A node’s output is weighted function 
of its inputs and a ‘bias’ term
Input
bias
1
Output
These biases also need to be learned!
9
Data Mining (BGU) Prof. Mark Last
Training Biases ( Θ’s )
A node’s output (assume ‘step function’ for simplicity)
1 if  W1 X1 + W2 X2 +…+ Wn Xn ≥ Θ
0 otherwise
Rewriting
W1 X1 + W2 X2 + … + Wn Xn – Θ ≥ 0
W1 X1 + W2 X2 + … + Wn Xn + Θ  (-1) ≥ 0
weight
‘activation’
10
Data Mining (BGU) Prof. Mark Last
Training Biases (cont.)
Hence, add another unit whose activation is 
always -1
The bias is then just another weight!
E.g.
-1 Θ
Θ
11
Data Mining (BGU) Prof. Mark Last
Sigmoid Activation Units
(Input: ? Output: ?)
Individual Units’ Computation
bias
output
input
output j = F(Sweight i,j  output i + bias j)
weight i,j – weight of the connection ij
F(input) = 
i
1
1+e                   
-input
Piecewise Linear (and Gaussian) nodes 
can also be used
12
Called the 
‘sigmoid’ and 
‘logistic’
(hyperbolic 
tangent also 
used)
Data Mining (BGU) Prof. Mark Last
Neural Network as a Classifier
 Weakness
 Long training time 
 Require a number of parameters typically best determined empirically
 Poor interpretability
 Strength
 High tolerance to noisy data 
 Ability to classify untrained patterns 
 Well-suited for continuous-valued inputs and outputs
 Successful on an array of real-world data, e.g., hand-written letters
 Algorithms are inherently parallel
 Techniques have recently been developed for the extraction of rules from 
trained neural networks
13
Data Mining (BGU) Prof. Mark Last
14
A Multi-Layer Feed-Forward Neural Network 
Output layer
Input layer
Hidden layer
Output vector
Input vector: X
wij
ij
k
ii
k
j
k
j xyyww )ˆ(
)()()1(  
Gradient Descent Rule:
k – iteration
xij - input vector
yi – target value
 - learning rate 
(e.g., 0.05)
Data Mining (BGU) Prof. Mark Last
15
How A Multi-Layer Neural Network Works
 The inputs to the network correspond to the attributes measured for each 
training tuple 
 Inputs are fed simultaneously into the units making up the input layer
 They are then weighted and fed simultaneously to a hidden layer
 The number of hidden layers is arbitrary (one, two, or more)
 The weighted outputs of the last hidden layer are input to units making up 
the output layer, which emits the network's prediction
 The network is feed-forward: None of the weights cycles back to an input 
unit or to an output unit of a previous layer
 From a statistical point of view, networks perform nonlinear regression: 
Given enough hidden units and enough training samples, they can closely 
approximate any function
Data Mining (BGU) Prof. Mark Last
16
Defining a Network Topology
 Decide the network topology: Specify # of units in the input 
layer, # of hidden layers (if > 1), # of units in each hidden layer, 
and # of units in the output layer
 Normalize the input values for each attribute measured in the 
training tuples to [0.0—1.0]
 One input unit per domain value, each initialized to 0
 Output, if for classification and more than two classes, one 
output unit per class is used
 Once a network has been trained and its accuracy is 
unacceptable, repeat the training process with a different 
network topology or a different set of initial weights
Data Mining (BGU) Prof. Mark Last
17
Backpropagation
 Iteratively process a set of training tuples & compare the network's prediction 
with the actual known target value
 For each training tuple, the weights are modified to minimize the mean 
squared error between the network's prediction and the actual target value 
 Modifications are made in the “backwards” direction: from the output layer, 
through each hidden layer down to the first hidden layer, hence 
“backpropagation”
 Steps
 Initialize weights to small random numbers, associated with biases 
 Propagate the inputs forward (by applying activation function) 
 Backpropagate the error (by updating weights and biases)
 Terminating condition (when error is very small, etc.)
Data Mining (BGU) Prof. Mark Last
Backpropagation Algorithm
18
Data Mining (BGU) Prof. Mark Last
Weight Space
 Given a neural-network layout, the weights and 
biases are free parameters that define a space
 Each point in this Weight Space specifies a 
network
weight space is a continuous space we search
 Associated with each point is an error rate, E, 
over the training data
 Backprop performs gradient descent in weight 
space
19
Data Mining (BGU) Prof. Mark Last
Backprop Seeks LOCAL Minima
(in a continuous space)
20
Weight Space
Error on Train Set
Note: a local min 
might over fit the 
training data, so 
often ‘early 
stopping’ used 
(later)
Data Mining (BGU) Prof. Mark Last
w
w  =  -   E ( w )
or wi =  - 
E
wi
The Gradient-Descent Rule
The 
‘gradient’ This is a N+1 dimensional vector (ie, the ‘slope’ in weight space)
Since we want to reduce errors, we want to go ‘down hill’
We’ll take a finite step in weight space:
E
W1
W2
‘delta’ = the 
change to w
 E
21
E(w)  [                ]E
w0
E
w1
E
w2
E
wN
,     ,     ,  … , _
Data Mining (BGU) Prof. Mark Last
‘On Line’ vs. ‘Batch’ 
Backprop
 Technically, we should look at the error gradient for the entire 
training set, before taking a step in weight space (‘batch’ 
backprop)
 However, in practice we take a step after each example (‘on-
line’ backprop)
 Much faster convergence (learn after each example)
 Called ‘stochastic’ gradient descent
 Stochastic gradient descent quite popular at Google, Facebook, 
Microsoft, etc. due to easy parallelism
22
Data Mining (BGU) Prof. Mark Last
= - (T – o) ∂ o
∂ Wk
Gradient Descent for the Perceptron
(for the simple case of linear output units)
Error  ½  ( T – o )
2
Network’s output
Teacher’s answer 
(a constant wrt the weights)
=  (T – o) 
∂ E
∂ Wk
∂ (T – o)
∂ Wk
23
Data Mining (BGU) Prof. Mark Last
Continuation of Derivation
∂ E
∂ Wk
= - (T – o) 
∂ Wk
∂(∑ w j  x j)
= - (T – o) x k
So    ΔWk = η (T – o) xk The Perceptron Rule
Stick in formula 
for output
Also known as the delta 
rule and other names 
(with some variation in 
the calculation)
24
We’ll use for both LINEAR 
and STEP-FUNCTION activation
∂ E
∂ Wk
Recall ΔWk  - η
Data Mining (BGU) Prof. Mark Last
Perceptron Example: Autonomous Vehicle
Current 
Speed
Speed 
Limit
Correct Output
40 50 1 (acceleration)
55 50 0 (deceleration)
75 90 1 (acceleration)
25
Training Set
X1
-1
X2
w1 = -0.1
w2 = 0.2
w0 = 0.5
ΔWk = η (T – Out) xk
η = 0.01
Perceptron Learning Rule
Out = Step Function:
Cur_Sp*w1 + Sp_Limit*w2 – w0 =
40*-0.1 + 50*0.2 - 1*0.5 = 5.5 => 1
Correct solution?
No wgt changes, since correct
Data Mining (BGU) Prof. Mark Last
Perceptron Example: Autonomous Vehicle
Current 
Speed
Speed 
Limit
Correct Output
40 50 1 (acceleration)
55 50 0 (deceleration)
75 90 1 (acceleration)
26
Training Set
X1
-1
X2
w1 = -0.1
w2 = 0.2
w0 = 0.5
ΔWk = η (T – Out) xk
η = 0.01
Perceptron Learning Rule
Out = Step Function:
Cur_Sp*w1 + Sp_Limit*w2 – w0 =
55*-0.1 + 50*0.2 - 1*0.5 = 4 => 1
// So need to update weights
W1 = -0.1+0.01*(-1)*55 = -0.65
W2 = 0.2+0.01*(-1)*50 = -0.30
W0 = 0.50+0.01*(-1)*(-1) = 0.51
Data Mining (BGU) Prof. Mark Last
Perceptron Example: Autonomous Vehicle
Current 
Speed
Speed 
Limit
Correct Output
40 50 1 (acceleration)
55 50 0 (deceleration)
75 90 1 (acceleration)
27
Training Set
X1
-1
X2
w1 = -0.65
w2 = -0.30
w0 = 0.51
ΔWk = η (T – Out) xk
η = 0.01
Perceptron Learning Rule
Out = Step Function:
Cur_Sp*w1 + Sp_Limit*w2 – w0 =
75*(-0.65) + 90*(-0.30) - 1*0.51 = -76.3 
=> 0
// So need to update weights
W1 = -0.65+0.01*1*75 = 0.10
W2 = -0.30+0.01*1*90 = 0.60
W0 = 0.51+0.01*1*(-1) = 0.50
Data Mining (BGU) Prof. Mark Last
28
Efficiency and Interpretability
 Efficiency of backpropagation: Each epoch (one iteration through the training 
set) takes O(|D| * w), with |D| tuples and w weights, but # of epochs can be 
exponential to n, the number of inputs, in worst case
 For easier comprehension: Rule extraction by network pruning
 Simplify the network structure by removing weighted links that have the 
least effect on the trained network
 Then perform link, unit, or activation value clustering
 The set of input and activation values are studied to derive rules describing 
the relationship between the input and hidden unit layers
 Sensitivity analysis: assess the impact that a given input variable has on a 
network output.  The knowledge gained from this analysis can be represented 
in rules
Data Mining (BGU) Prof. Mark Last
29
Lecture No. 7 – Artificial Neural 
Networks
 Overview
 Classification by Backpropagation
 Deep Neural Networks
Data Mining (BGU) Prof. Mark Last
The new way to train multi-layer NNs…
EACH of the (non-output) layers is 
trained to be an auto-encoder
Basically, it is forced to learn good 
features that describe what comes from 
the previous layer 30
Data Mining (BGU) Prof. Mark Last
An auto-encoder is trained, with an absolutely standard 
weight-adjustment algorithm  to reproduce the input
By making this happen with (many) fewer units than the 
inputs, this forces the ‘hidden layer’ units to become good 
feature detectors
31
Data Mining (BGU) Prof. Mark Last
Intermediate layers are each trained to be 
auto encoders (or similar) 
32
The number of units in the k-th layer is the 
same as that in the (M − k + 1) th layer.
Aggarwal, Charu C.. Neural Networks and Deep 
Learning: A Textbook. 2018
1
2
3
4
5
6
7
Data Mining (BGU) Prof. Mark Last
Training Deep Neural Networks
Source: Aggarwal, 2018
 Mini-batch stochastic gradient descent often provides the best trade-off 
between stability, speed, and memory requirements. 
 Commonly used sizes of a mini-batch: 32, 64, 128, or 256.
 Use a validation set for tuning hyperparameters (e.g., network topology) and a 
separate training set for gradient descent . Why?
 Hyperparameter optimization: coarse-to-fine-grained grid search 
 Feature pre-processing
 Normalize the values of each input feature
 Apply whitening: extract a new set of de-correlated features (using … ?)
 Initialize each weight to a value drawn from a Gaussian distribution with 
standard deviation 
1
𝑟
, where r is the number of inputs to that neuron. Why?
 Use ReLU activation function (f(x) = x, x ≥ 0, f’(x) = ?) 
 Use decaying learning rate
 Training acceleration and model compression. How?
33
Data Mining (BGU) Prof. Mark Last
What is this?
34
Data Mining (BGU) Prof. Mark Last
Daniel Whiteson
Peter Sadowski
35
Data Mining (BGU) Prof. Mark Last
Higgs Boson Detection
Deep network improves AUC by 8%
Nature Communications, July 2014BDT= Boosted  Decision Trees in TMVA package
(TPR)
(T
N
R
)
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-8\kdd08.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 1
Lecture No. 8 – Bayesian 
Learning
 Introduction to Bayesian Learning
 Bayes Theorem
 Naïve Bayes Algorithm
 Bayesian Belief Networks 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 2
Introduction to Bayesian 
Learning
 Basic Assumption
 The observed data is governed by probability distributions
 Features
 Using prior knowledge on probability distributions 
 Incremental learning of probabilities
 Each observed example can incrementally increase or decrease the 
estimated probability
 Probabilistic predictions of target values
 Prediction by multiple hypotheses
 A standard of optimal decision making
 Practical Algorithms
 Naïve Bayes Classifier
 Bayesian Belief Networks
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 3
Basic Formulas for Probabilities
 Product Rule
 Probability P (A  B) of a conjunction of two 
events A and B:
 P (A  B) = P(A / B) P(B) = P(B / A) P(A)
 Sum Rule
 Probability of a disjunction of two events A
and B:
 P (AB) = P(A) + P(B) - P (A  B) 
 Theorem of Total Probability
 If events A1,…, An are mutually exclusive with 
i P(Ai) = 1, then
 P(B) = i P(B/Ai)P(Ai) 
B
A
A – has other investments
B – credit = “Yes”
A1 – has other investments
A2 – no other investments
B – credit = “Yes”
Independent 
events?
Disjoint 
events?
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 4
Lecture No. 8 – Bayesian 
Learning
 Introduction to Bayesian Learning
 Bayes Theorem
 Naïve Bayes Algorithm
 Bayesian Belief Networks 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 5
5
Bayesian Theorem: Basics
 Let H be a hypothesis that assigns X to class C
 Optional classes: Buys_Computer = Yes and Buys_Computer = No
 Classification is to determine P(H|X), the probability that the 
hypothesis holds given the observed data sample X
 Let X be a record (“evidence”):
Record ID age income student credit_rating
1 <=30 high no fair
2 <=30 high no excellent
3 30…40 high no fair
4 >40 medium no fair
5 >40 medium no excellent
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 6
Bayesian Theorem: Basics (cont.)
6
 P(H) (prior probability), the initial probability that the 
hypothesis H is correct
 E.g., X will buy computer, regardless of age, income, …
 P(X) (evidence): probability to observe a given record
 E.g., the prob. that X is 31..40, medium income, etc.
 P(X|H) (likelihood), the probability of observing the record X, 
given that the hypothesis holds
 E.g., Given H (X will buy computer), the prob. that X is 31..40, medium 
income, etc.
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 7
7
Bayes Theorem
 Given training data X, posteriori probability of a hypothesis H, 
P(H|X) follows the Bayes theorem
 Example 
 P(Buys_Computer = Yes / Age = 31..40; income=medium; student = no)=
 Informally, this can be written as 
 posterior =likelihood x prior / evidence
)(
)()|()|(
XP
HPHXPXHP 
)31..40 is (
)_()_|nostudent medium;income 31..40; is (
XP
YesComputerBuysPYesComputerBuysAgeP 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 8
8
MAP (Maximum Posteriori) Hypothesis
 MAP (maximum posteriori) hypothesis
 D – training data set
 Optional hypotheses: ?
 Example 
 X = 31..40
 Practical difficulty: require initial knowledge of many probabilities, 
curse of dimensionality, significant computational cost
.)()|(maxarg)|(maxarg hPhDP
Hh
DhP
HhMAP
h




.)}()|40..31(  ),()|40..31({max NoPNoPYesPYesP
MAP
h 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 9
Bayesian Theorem Example
Does patient have cancer or not?
 A patient takes a lab test and the result comes 
back positive. 
 It is known that the test returns a correct positive 
result in only 98% of the cases and a correct 
negative result in only 97% of the cases.
 Furthermore, only 0.008 of the entire population 
has this disease.
1. What is the probability that this patient has cancer?
2. What is the probability that he does not have cancer?
3. What is the most probable diagnosis?
Source: Mitchell (1997)
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 10
Does patient have cancer or not? 
(cont’d)
 Medical diagnosis problem (D = ?, h = {…}?)
P (cancer) = .008, P (cancer) = .992
P ( / cancer) = .98, P (Ө / cancer) = .02
P ( / cancer) = .03 P (Ө / cancer) = .97
 Maximum A Posteriori Hypothesis
P(|cancer)P(cancer) = (.98).008 = .0078
P( |cancer)P(cancer) = (.03).992 = .0298
hMAP = cancer
 Probability of Cancer
P(cancer|) =    0.0078 / (0.0078 + 0.0298) =  .21
 Practical implications?
Diagnosis?
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 11
Lecture No. 8 – Bayesian 
Learning
 Introduction to Bayesian Learning
 Bayes Theorem
 Naïve Bayes Algorithm
 Bayesian Belief Networks 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 12
Towards Naïve Bayesian Classifier
 Let D be a training set of tuples and their associated 
class labels, and each tuple is represented by an n-D 
attribute vector X = (x1, x2, …, xn)
 Suppose there are m classes C1, C2, …, Cm
 Classification is to derive the maximum posteriori, i.e., 
the maximal P(Ci|X)
 This can be derived from Bayes’ theorem
 Since P(X) is constant for all classes, only                                        
needs to be maximized
)(
)()|(
)|(
X
X
X
P
i
CP
i
CP
i
CP 
)()|()|(
i
CP
i
CP
i
CP XX 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 13
Naïve Bayes Classifier (NBC) 
 A simplified assumption: attributes 
are conditionally independent:
 The probability of occurrence of x1
and x2 given the current class is C:
P([x1,x2]/C) = P(x1/C) * P(x2/C)
 No dependence relation between 
attributes 
 Greatly reduces the computation 
cost, only count the class 
distribution.
 Once the probability P(X|Ci) is 
known, assign X to the class with 
maximum P(X|Ci)*P(Ci)



n
k
CixkPCiXP
1
)|()|(



n
k
CixkPCPC i
C
NB
i 1
)|(*)(maxarg
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 14
Training dataset – Example 1
age income student credit_rating buys_computer
<=30 high no fair no
<=30 high no excellent no
30…40 high no fair yes
>40 medium no fair yes
>40 low yes fair yes
>40 low yes excellent no
31…40 low yes excellent yes
<=30 medium no fair no
<=30 low yes fair yes
>40 medium yes fair yes
<=30 medium yes excellent yes
31…40 medium no excellent yes
31…40 high yes fair yes
>40 medium no excellent no
Class:
C1:buys_computer=
‘yes’
C2:buys_computer=
‘no’
Data record
X =(age<=30,
Income=medium,
Student=yes
Credit_rating=
Fair)
Class = ?
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 15
Naïve Bayesian Classifier:  Example 1
 Compute P(X/Ci) for each class
 P(age=“<=30” | buys_computer=“yes”)  = 2/9=0.222
 P(age=“<=30” | buys_computer=“no”) = 3/5 =0.6
 P(income=“medium” | buys_computer=“yes”)= 4/9 =0.444
 P(income=“medium” | buys_computer=“no”) = 2/5 = 0.4
 P(student=“yes” | buys_computer=“yes)= 6/9 =0.667
 P(student=“yes” | buys_computer=“no”)= 1/5=0.2
 P(credit_rating=“fair” | buys_computer=“yes”)=6/9=0.667
 P(credit_rating=“fair” | buys_computer=“no”)=2/5=0.4
 X=(age<=30 , income =medium, student=yes, credit_rating=fair)
 P(X|Ci) : P(X|buys_computer=“yes”)= 0.222 x 0.444 x 0.667 x 0.667 =0.044
 P(X|buys_computer=“no”)= 0.6 x 0.4 x 0.2 x 0.4 =0.019
 P(X|Ci)*P(Ci ) : P(X|buys_computer=“yes”)  *  P(buys_computer=“yes”)=0.044 * 
9/14=0.028
 P(X|buys_computer=“no”) * P(buys_computer=“no”)=0.019 * 5/14 = 0.007
 X belongs to  class “buys_computer=yes”
 P(buys_computer =“yes” / X) = ?
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 16
Naïve Bayes Classifier (NBC)
Example 2: Text Classification
16
Documents are classified as being scientific or 
commercial by the occurrence of the following three 
words: “paper”, “research”, and “product”.  The data 
obtained from 100 scientific documents and 100 
commercial documents is summarized below:
Document "Paper" "Research" "Product"
Scientific 80 90 20
Commercial 50 20 90
Explanation: 80 scientific documents included 
the word “paper”, 90 commercial documents 
included the word “product”, etc.
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 17
17
NBC Example 2 (cont.)
Classify the following text using the Naïve Bayes 
algorithm:
To combine the power of new and existing security investments made 
by our customers, the IBM Threat Protection System leverages 
information gathered from the Ready for IBM Security Intelligence 
ecosystem of more than 400 third-party products from over 90 
vendors. You can take advantage of these third-party solutions to 
increase visibility into security events, collapse information silos and 
gain insights on advanced attacks.
Source: http://www-03.ibm.com/security/threat-protection/?lnk=ushpls1
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 18
18
NBC Example 2 (cont.)
Training Data
Document "Paper" "Research" "Product"
Scientific 80 90 20
Commercial 50 20 90
Document "Paper" "Research" "Product"
Scientific 0.8 0.9 0.2
Commercial 0.5 0.2 0.9
Test Data
Apriori "Paper" "Research" "Product" Total Rel.
P (Scientific) 0.5 0.2 0.1 0.2 0.002 0.011
P (Commercial) 0.5 0.5 0.8 0.9 0.180 0.989
0.182
Relative 
probability of 
each class
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 19
Estimating Probabilities
 Two difficulties of estimating probability
1. produces a biased underestimate of the probability
2. When this probability estimate is zero, this probability term will 
dominate the Bayes classifier
 Solution: using the m-estimate defined as follows
m-estimate of probability: 
nc : number of examples for which class v = vj and attribute a = ai
n : number of training examples for which class v = vj
m : equivalent sample size
p : prior (e.g., uniform)
If m = 0, the m-estimate is equivalent to
n
nc
mn
mpnc


n
nc



n
k
CixkPCPC i
C
NB
i 1
)|(*)(maxarg
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 20
20
m-Estimate Example
 Ex. Suppose a dataset with 1000 tuples, 
income=low (0), income= medium (990), and 
income = high (10)
 Let equivalent sample size m = 100
 Uniform prior: 1/3
 mp = 331/3
 Use m- estimate
Prob(income = low) = 33.33/1100 = 0.030
Prob(income = medium) = (990+33.33)/1100 = 0.930
Prob(income = high) = (10+33.33)/1100 = 0.039
 What if m = 0 ?
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 21
821יחידה 
Laplacian Estimator
 Ex. Suppose a dataset with 1000 tuples, 
income=low (0), income= medium (990), and 
income = high (10) 
 Use Laplacian correction (or Laplacian 
estimator)
 Adding 1 to each case
Prob(income = low) = 1/1003 = 0.001
Prob(income = medium) = 991/1003 = 0.988
Prob(income = high) = 11/1003 = 0.011
 The “corrected” prob. estimates are close to their 
“uncorrected” counterparts
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 22
Naïve Bayesian Classifier: 
Comments
 Advantages : 
 Easy to implement 
 Good results obtained in most of the cases
 Disadvantages
 Assumption: class conditional independence , therefore loss of 
accuracy
 Practically, dependencies exist among variables 
 E.g.,  hospitals: patients: Profile: age, family history etc 
 Symptoms: fever, cough etc., Disease: lung cancer, diabetes etc 
 Dependencies among these cannot be modeled by Naïve Bayesian 
Classifier
 How to deal with these dependencies?
 Bayesian Belief Networks 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 23
Lecture No. 8 – Bayesian 
Learning
 Introduction to Bayesian Learning
 Bayes Theorem
 Naïve Bayes Algorithm
 Bayesian Belief Networks 
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Bayesian Belief Networks
 Naïve Bayes is based on assumption of 
conditional independence
 Bayesian networks provide a tractable 
method for specifying dependencies among 
variables
24
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Example (from Judea Pearl*)
You have a new burglar alarm installed at home.  It is fairly 
reliable at detecting a burglary, but also responds on occasion 
to minor earthquakes.  You also have two neighbors, John 
and Mary, who have promised to call you at work when they 
hear the alarm.  John always calls when he hears the alarm, 
but sometimes confuses the telephone ringing with the alarm 
and calls then, too.  Mary, on the other hand, likes rather loud 
music and sometimes misses the alarm altogether.  Given the 
evidence of who has or has not called, we would like to 
estimate the probability of a burglary.
* - 2011 winner of the ACM Turing Award
25
Data Mining (BGU) Prof. Mark Last
Terminology (see the example)
 A Bayesian Belief Network describes the probability 
distribution over a set of random variables Y1, Y2, …Yn
 Each variable Yi can take on the set of values V(Yi)
 The joint space of the set of variables Y is the cross product 
 V(Y1)  V(Y2) …  V(Yn)
 Each item in the joint space corresponds to one possible 
assignment of values to the tuple of variables <Y1, …Yn>
 Joint probability distribution:  specifies the probabilities of 
the items in the joint space
 A Bayesian Network provides a way to describe the joint 
probability distribution in a compact manner.
Lesson No. 8 26
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Conditional Independence
(see the example)
 Let X, Y, and Z be three discrete-valued random 
variables.
 We say that X is conditionally independent of Y
given Z if the probability distribution governing X
is independent of the value of Y given a value for Z
)|(),|(,, kikjikji zZxXPzZyYxXPzyx 
)|(),|( ZXPZYXP 
27
Data Mining (BGU) Prof. Mark Last
Bayesian Belief Network
 A set of random variables makes up the 
nodes of the network
 A set of directed links or arrows 
connects pairs of nodes.
 Each node has a conditional 
probability table that quantifies the 
effects that the parents have on the 
node. 
 The graph has no directed cycles (it is a 
DAG)
Lesson No. 8 28
X Y
Z
P
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Step 1
 Determine what the propositional (random) 
variables should be
 Determine causal (or another type of 
influence) relationships and develop the 
topology of the network
29
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Topology of Belief Network
Burglary Earthquake
Alarm
JohnCalls MaryCalls
30
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Step 2
 Specify a conditional probability table or CPT for 
each node.
 Each row in the table contains the conditional 
probability of each node value for a conditioning 
case (possible combinations of values for parent 
nodes).
 In the example, the possible values for each node are 
true/false.
 The sum of the probabilities for each value of a node 
given a particular conditioning case  is 1.
31
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Example:
CPT for Alarm Node
Burglary Earthquake
P(Alarm|Burglary,Earthquake)
True False
True              True 0.950             0.050
True              False                       0.940             0.060
False             True 0.290             0.710
False             False 0.001             0.999        
1
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Complete Belief Network
Burglary Earthquake
Alarm
JohnCalls
MaryCalls
P(B)
0.001
P(E)
0.002
B       E    P(A|B,E)
T       T        0.95
T       F        0.94
F       T        0.29
F       F        0.001
A       P(J|A)
T         0.90
F         0.05
A       P(M|A)
T         0.70
F         0.01
33
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Network as representation of 
joint
 A generic entry in the joint probability distribution 
is the probability of a conjunction of particular 
assignments to each variable, such as:
P(x1,...,xn )  P(xi | Parents(Xi))
i1
n

 Each entry in the joint is represented by the 
product of appropriate elements of the CPTs 
in the belief network.
34
Data Mining (BGU) Prof. Mark Last
Lesson No. 8
Example Calculation
Calculate the probability of the event that the 
alarm has sounded but neither a burglary nor an 
earthquake has occurred, and both John and 
Mary call.
P(J ^ M ^ A ^ ~B ^ ~E) 
= P(J|A) P(M|A) P(A|~B,~E) P(~B) P(~E)
= 0.90 * 0.70 * 0.001 * 0.999 * 0.998 
= 0.00062
35
Data Mining (BGU) Prof. Mark Last
Inference Methods for Bayesian 
Networks
 We may want to infer the value of some target variable 
(Burglary) given observed values for other variables.
 What we generally want is the probability distribution
 Inference straightforward if all other values in network 
known
 More general case, if we know a subset of the values of 
variables, we can infer a probability distribution over other 
variables.
 NP-Hard problem
 But approximations work well
Lesson No. 8 36
Data Mining (BGU) Prof. Mark Last
37
How Are Bayesian Networks Constructed?
 Subjective construction: Identification of (direct) causal structure
 People are quite good at identifying direct causes from a given set of 
variables & whether the set contains all relevant direct causes
 Markovian assumption: Each variable becomes independent of its non-
effects once its direct causes are known
 E.g., S ‹— F —› A ‹— T, path S—›A is blocked once we know F—›A 
 HMM (Hidden Markov Model): often used to model dynamic systems 
whose states are not observable, yet their outputs are
 Synthesis from other specifications
 E.g., from a formal system design: block diagrams & info flow
 Learning from data
 E.g., from medical records or student admission record
 Learn parameters give its structure or learn both structure and parms
 Maximum likelihood principle: favors Bayesian networks that maximize the 
probability of observing the given data set
Data Mining (BGU) Prof. Mark Last
Lesson No. 8 38
Training Bayesian Networks
 Several cases
 Given both the network structure and all variables 
observable: learn only the CPTs (similar to NBC)
 Network structure known, some hidden variables: method 
of gradient descent, analogous to neural network learning
 Network structure unknown, all variables observable: 
search through the model space to reconstruct graph 
topology 
 Unknown structure, all hidden variables: no good 
algorithms known for this purpose
 D. Heckerman.  A Tutorial on Learning with Bayesian 
Networks.  In Learning in Graphical Models, M. 
Jordan, ed. MIT Press, 1999.
Variables believed to influence but 
not observable
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\הרצאות\הרצאה-9\kdd09.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 1
Lecture No. 9 – Instance-Based 
Learning and SVM 
 Overview of Instance-based Learning 
 K-nearest Neighbours
 Case-Based Reasoning (CBR)
 Kernel-Based Methods
 Support Vector Machines (SVM)
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 2
Instance-Based Methods
 Model-based (“eager”) learning
 Process training examples and store the model for classification of 
future instances
 Instance-based (“lazy”) learning 
 Store training examples and delay the processing (“lazy evaluation”) 
until a new instance must be classified
 Typical approaches of instance-based learning
 k-nearest neighbor approach
 Instances represented as points in a Euclidean space.
 Kernel-based methods / Locally weighted regression
 Construct local approximation
 Case-based reasoning
 Uses symbolic representations and knowledge-based inference
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 3
Lecture No. 9 – Instance-Based 
Learning and SVM
 Overview of Instance-based Learning 
 K-nearest Neighbours
 Case-Based Reasoning (CBR)
 Kernel-Based Methods
 Overview of Support Vector Machines (SVM)
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 4
The k-Nearest Neighbor Algorithm
 All instances correspond to points in the n-D space.
 The nearest neighbor are defined in terms of Euclidean 
distance.
 The target function could be discrete- or real- valued.
 For discrete-valued, the k-NN returns the most common 
value among the k training examples nearest to xq. 
 For continuous-valued target functions, calculate the mean 
values of the k nearest neighbors
 Voronoi diagram: the decision surface induced by 1-NN 
for a typical set of training examples.
. 
_
+
_ xq
+
_ _
+
_
_
+
.
.
.
. .
1-NN vs. 5-
NN
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 5
K-nearest neighbor for discrete 
classes
Algorithm (parameter k)
Training
For each training example (X,C(X))
add the example to our training list.
Testing
When a new example Xq arrives, assign class:
C(Xq) = majority voting on the k nearest neighbors of Xq
where δ(a,b) = 1 if a = b and 0 otherwise 
Source: www2.cs.uh.edu/~vilalta/courses/ machinelearning/instancelearning1.ppt

i
i
v
q XCvXC ))(,(maxarg)( 
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 6
K-nearest neighbor for real-valued 
functions
Algorithm (parameter k)
Training
For each training example (X,C(X))
add the example to our training list.
Testing
When a new example Xq arrives, assign class:
C(Xq) = average value among k nearest neighbors of Xq
Source: www2.cs.uh.edu/~vilalta/courses/ machinelearning/instancelearning1.ppt
k
XC
XC i
i
q


)(
)(
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 7
The Distance Between Examples 
 We need a measure of distance in 
order to know who are the 
neighbours
 Assume that we have T attributes for 
the learning problem. Then one 
example point x has elements xt 
, t=1,…T.
 The distance between two points xi xj
is often defined as the Euclidean 
distance:



T
t
tjtiji xxd
1
2][),( xx
Source; www.cs.bham.ac.uk/~axk/KNN_CBR.ppt
Data Mining (BGU) Prof. Mark Last
K-NN Example: Iris Dataset
Source: Fisher (1936)
Lecture No. 9 9
 Number of records: 150
 Number of attributes: 4
 Number of classes: 3 (iris setosa, iris 
versicolor, and iris virginica)
sepal length in 
cm
sepal width 
in cm
petal length 
in cm
petal width 
in cm
Class
1 4.6 3.6 1 0.2 1
2 4.3 3 1.1 0.1 1
3 5 3.2 1.2 0.2 1
4 5.8 4 1.2 0.2 1
5 4.4 3 1.3 0.2 1
6 4.4 3.2 1.3 0.2 1
7 4.5 2.3 1.3 0.3 1
8 4.7 3.2 1.3 0.2 1
9 5 3.5 1.3 0.3 1
10 5.4 3.9 1.3 0.4 1
Petal - כותרת -עלה
Sepal - גביע -עלה
Are the classes 
linearly separable?
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 10
K-NN Example: Iris Dataset (cont.)
 Full size: 150 observations
 Testing set (query examples): q = 50, 100, 150
k
sepal 
length in 
cm
sepal 
width in 
cm
petal 
length in 
cm
petal 
width in 
cm
Actual 
Class
Dist (k,50)
Predicted 
Class
8 5 3.4 1.5 0.2 1
50 5 3.3 1.4 0.2 0.1414 1
k
sepal 
length in 
cm
sepal 
width in 
cm
petal 
length in 
cm
petal 
width in 
cm
Actual 
Class
Dist 
(k,100) Predicted 
Class
97 5.7 2.9 4.2 1.3 2
100 5.7 2.8 4.1 1.3 0.1414 2
k
sepal 
length in 
cm
sepal 
width in 
cm
petal 
length in 
cm
petal 
width in 
cm
Actual 
Class
Dist 
(k,150) Predicted 
Class
128 6.1 3 4.9 1.8 3
150 5.9 3 5.1 1.8 0.2828 3
Nearest 
Neighbor
Nearest 
Neighbor
Nearest 
Neighbor
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 11
Distance-weighted k-NN
 Replace by: 


k
i
i
Vv
xfvqf
1
))(,(maxarg)(ˆ 



k
i
ii
Vv
xfvwqf
1
))(,(maxarg)(ˆ 
2
)(
1
iq
i
xxd
w


 Where δ(a,b) = 1 if a = b and 0 otherwise
Source: faculty.cs.byu.edu/~cgc/Teaching/CS_478/ CS%20478%20-
%20Instance%20Based%20Learning.ppt
Data Mining (BGU) Prof. Mark Last
Distance-weighted k-NN Example
Lecture No. 9 12
i X Y Class DistX DistY Dist^2 Weight w*delta(0) w*delta(1)
1 4 6 0 -1 1 2 0.500 0.500 0.000
2 2 7 1 -3 2 13 0.077 0.000 0.077
3 9 10 1 4 5 41 0.024 0.000 0.024
Query 5 5 ? 0.500 0.101
k = 3 (number of nearest neighbors)
Predicted class
k-NN: ?
Distance-weighted k-NN:
2
)(
1
iq
i
xxd
w





k
i
ii
Vv
xfvwqf
1
))(,(maxarg)(ˆ 
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 13
When To Consider Nearest Neighbor
 Instances map to points in n
 Less than  20 attributes per instance
 Lots of training data
 Advantages
 Training is very fast
 Learn complex target functions
 Don’t lose information
 Disadvantages
 Slow at query time
 Limited interpretability
 Curse of dimensionality
 Distance between neighbors could be dominated by irrelevant 
attributes
 Solutions?
Source: Lecture Slides for Machine Learning by T Mitchell, 1997
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 19
Lecture No. 9 – Instance-Based 
Learning and SVM
 Overview of Instance-based Learning 
 K-nearest Neighbours
 Case-Based Reasoning (CBR)
 Kernel-Based Methods
 Support Vector Machines (SVM)
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 20
One-dimensional Kernel Smoothers
(Based on Hastie et al.)
 k-nearest-neighbor average for real-valued functions

 Nk(x) – the set of k points nearest to x
 assigns equal weight to all points in neighborhood
 changes in a discrete way => discontinuous
 Solution: points should have smooth decrease in weight 
according to their distance from the target point
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 21
One-dimensional Kernel Smoothers (cont.)
Figure 6.1: In each panel 100 pairs xi, yi are generated at random from the blue curve with 
Gaussian errors: Y = sin(4X) + ε, X ~ U[0, 1], ε ~ N(0, 1/3). In the left panel the green curve 
is the result of a 30-nearest-neighbor running-mean smoother. The red point is the fitted 
constant ˆ f(x0), and the red circles indicate those observations contributing to the fit at x0. 
The solid yellow region indicates the weights assigned to observations. In the right panel, the 
green curve is the kernel-weighted average, using an Epanechnikov kernel with (half)window 
width λ = 0.2.
f(x0)=?
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 22
Nadaraya-Watson Kernel-weighted Average
• Using the Epanechnikov quadratic kernel:
•  is a smoothing parameter and determines width of kernel
• increase in  means lower variance, higher bias (why?)
• more generally, it can be a function hm(x) (adaptive neighborhood)
• e.g. for k-nearest neighbors: hk(x) = |x – x[k]| where x[k] is k-th
nearest neighbor of x.
`







otherwise 0
1|t| if )1(
4
3
)(
2t
tD
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 23
Other Kernels
• Other popular kernels:
 Tri-cube function:
 Differentiable
 Flatter on top
 Gaussian density function: 
 s.d. = window size
• Epanechnikov & tri-cube have compact support
D (t) = z (t)


 

otherwise              0
1;|t| if )||1(
)(
33t
tD
Data Mining (BGU) Prof. Mark Last
Epanechnikov Quadratic Kernel 
Example ( = 50)
Lecture No. 9 24


 

otherwise 0
1|t| if )1(75.0
)(
2t
tD
X Y |X-X_0|
|X-X_0|/ 
lambda K f_est
1280.29 17.67 18.19 0.3638 0.6507
1288.38 17.67 10.10 0.2021 0.7194
1298.48 12.00 0.00 0.0000 0.7500 14.01
1299.73 11.33 1.25 0.0250 0.7495
1301.44 12.00 2.96 0.0592 0.7474
Total 3.6170
00
00
00
00
00
00
00
00
00
00
1275.00 1280.00 1285.00 1290.00 1295.00 1300.00 1305.00
X
X0
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 25
Remarks on Lazy vs. Eager 
Learning
 Instance-based learning: lazy evaluation 
 Decision-tree and Bayesian classification:  eager 
evaluation
 Key differences
 Lazy method uses a “local model” when querying instance xq
 Eager method uses a global approximation
 Efficiency: Lazy - less time training but more time 
predicting
 Accuracy
 Lazy method effectively uses a richer hypothesis space 
 Eager: must commit to a single hypothesis (model)
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 26
Lecture No. 9 – Instance-Based 
Learning and SVM
 Overview of Instance-based Learning 
 K-nearest Neighbours
 Case-Based Reasoning (CBR)
 Kernel-Based Methods
 Support Vector Machines (SVM)
Data Mining (BGU) Prof. Mark Last
27
Classification: A Mathematical Mapping
 Classification: predicts categorical class labels
 E.g., Personal homepage classification
 xi = (x1, x2, x3, …), yi = +1 or –1
 x1 : # of words “homepage”
 x2 : # of words “welcome”
 Mathematically, x  X = n, y  Y = {+1, –1}
 We want to derive a function f: X  Y
 Linear Classification
 Binary Classification problem
 Data above the red line belongs to class ‘x’
 Data below red line belongs to class ‘o’
 Examples: SVM, Perceptron, Probabilistic Classifiers
x
xx
x
xx
x
x
x
x oo
o
o
o
o
o
o
o o
o
o
o
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 28
SVM—Support Vector Machines
 A relatively new classification method for both linear and 
nonlinear data
 It uses a nonlinear mapping to transform the original training 
data into a higher dimension
 With the new dimension, it searches for the linear optimal 
separating hyperplane (i.e., “decision boundary”)
 With an appropriate nonlinear mapping to a sufficiently high 
dimension, data from two classes can always be separated by a 
hyperplane
 SVM finds this hyperplane using support vectors (“essential” 
training tuples) and margins (defined by the support vectors)
Data Mining (BGU) Prof. Mark Last
SVM—History and Applications
 Vapnik and colleagues (1992)—groundwork 
from Vapnik & Chervonenkis’ statistical 
learning theory in 1960s
 Features: training can be slow but accuracy is 
high owing to their ability to model complex 
nonlinear decision boundaries (margin 
maximization)
 Used both for classification and numeric 
prediction
 Applications: 
 handwritten digit recognition, object recognition, 
speaker identification, benchmarking time-series 
prediction tests, text categorization
Lecture No. 9 29
Vladimir Naumovich Vapnik
Владимир Наумович Вапник
Data Mining (BGU) Prof. Mark Last
A Discriminant Function
 It can be arbitrary functions of x, such as:
Nearest 
Neighbor
Decision 
Tree
Linear
Functions
( ) Tg b x w x
Nonlinear
Functions
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
30Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Linear Discriminant Function
 g(x) is a linear function:
( ) Tg b x w x
x1
x2
wT x + b < 0
wT x + b > 0
 A hyper-plane in the 
feature space (a straight 
line in 2-D)
n
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
bias - 
 vector training- },...,,(
attributes ofnumber  - 
torweight vec},...,,(
21
21
b
xxxx
m
wwww
m
m


31Lecture No. 9
Data Mining (BGU) Prof. Mark Last
 How would you classify 
these points using a linear 
discriminant function in 
order to minimize the error 
rate?
Linear Discriminant Function denotes +1
denotes -1
x1
x2
 Infinite number of 
answers!
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
32Lecture No. 9
Data Mining (BGU) Prof. Mark Last
x1
x2 How would you classify 
these points using a linear 
discriminant function in 
order to minimize the error 
rate?
Linear Discriminant Function denotes +1
denotes -1
 Infinite number of 
answers!
 Which one is the best?
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
33Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Large Margin Linear Classifier 
“safe zone”
 The linear discriminant 
function (classifier) with 
the maximum margin is the 
best
 Margin is defined as the 
width that the boundary 
could be increased by 
before hitting a data point
 Why it is the best?
 Robust to outliers and thus 
strong generalization ability 
Margin
x1
x2
denotes +1
denotes -1
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
34Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Large Margin Linear Classifier 
 Given a set of data points:
 With a scale transformation 
on both w and b, the above 
is equivalent to 
x1
x2
denotes +1
denotes -1
For 1,   0
For 1,   0
T
i i
T
i i
y b
y b
   
   
w x
w x
{( , )},  1,2, ,i iy i nx , where
For 1,   1
For 1,   1
T
i i
T
i i
y b
y b
   
    
w x
w x
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
H1
H2
H1:
H2:
35Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Large Margin Linear Classifier 
 We know that
 The margin width is:
x1
x2
denotes +1
denotes -1
 1
 1
T
T
b
b


 
  
w x
w x
Margin
x+
x+
x-( )
2
    ( )
M  
 
  
   
x x n
w
x x
w w
n
Support Vectors
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
H1
H2
22
2
2
1 ... nwwww 
36Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Reminder: 
Distance between Two Parallel Lines
d
Ax + By + C1 = 0
x
y
0
Ax + By + C2 = 0
22
12
BA
CC
d



37Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Large Margin Linear Classifier 
 Formulation: 
x1
x2
denotes +1
denotes -1
Margin
x+
x+
x-
n
21
minimize  
2
w
such that
For 1,   1
For 1,   1
T
i i
T
i i
y b
y b
   
    
w x
w x
This slide is courtesy of www1.cs.columbia.edu/~belhumeur/courses/biometrics/2009/svm.ppt
( ) 1Ti iy b w x
Which is the same as
H1
H2
38Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 39
The Dual Problem
 n – number of training records
 This is a quadratic programming (QP) problem
 A global maximum of ai can always be found
 w can be recovered by
get  from  ( ) 1 0,    
where  is support vector
T
i i
i
b y b  w x
x
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 40
Characteristics of the Solution
 Many of the ai are zero
 w is a linear combination of a small number of data points
 This “sparse” representation can be viewed as data 
compression as in the construction of knn classifier
 xi with non-zero ai are called support vectors (SV)
 The decision boundary is determined only by the SV
 Let tj (j=1, ..., s) be the indices of the s support vectors. 
We can write
 For testing with a new data z
 Compute                                                      and 
classify z as class 1 if the sum is positive, and class -1 
otherwise.  Note: w need not be formed explicitly
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 41
a6=1.4
A Geometrical Interpretation
Class 1
Class -1
a1=0.8
a2=0
a3=0
a4=0
a5=0
a7=0
a8=0.6
a9=0
a10=0
W1=0.8*(-1)*X11
+1.4*1*X16
+ 0.6*(-1)*X18 
W2=0.8*(-1)*X21
+1.4*1*X26
+ 0.6*(-1)*X28 
X2
X1
Data Mining (BGU) Prof. Mark Last
Linear SVM Example
Source : Dan Ventura, Brigham Young University, March 12, 2009
Lecture No. 9 42
i X1 X2 Y
1 3 1 1
2 3 -1 1
3 6 1 1
4 6 -1 1
5 1 0 -1
6 0 1 -1
7 0 -1 -1
8 -1 0 -1
-1.5
-1
-0.5
0
0.5
1
1.5
-2 -1 0 1 2 3 4 5 6 7
X
2
X1
1 -1
Which line is the best discriminating 
function for this data?
How many 
unknown variables 
do we need to find?
Data Mining (BGU) Prof. Mark Last
Linear SVM Example (cont.)
Lecture No. 9 43

 

n
i
n
i
j
T
iji xxyyQ
1 1
i / j 1 2 3 4 5 6 7 8
1 10 8 19 17 -3 -1 1 3
2 8 10 17 19 -3 1 -1 3
3 19 17 37 35 -6 -1 1 6
4 17 19 35 37 -6 1 -1 6
5 -3 -3 -6 -6 1 0 0 -1
6 -1 1 -1 1 0 1 -1 0
7 1 -1 1 -1 0 -1 1 0
8 3 3 6 6 -1 0 0 1
i X1 X2 Y
1 3 1 1
2 3 -1 1
3 6 1 1
4 6 -1 1
5 1 0 -1
6 0 1 -1
7 0 -1 -1
8 -1 0 -1
Example:
i = 1, j = 2: 1*1*(3*3 + 1*(-1)) = 8
Data Mining (BGU) Prof. Mark Last
Linear SVM Example: The Optimal 
Solution
Lecture No. 9 44
i X1 X2 Y alpha alpha*Y
1 3 1 1 0.25 0.25 s2
2 3 -1 1 0.25 0.25 s3
3 6 1 1 0 0
4 6 -1 1 0 0
5 1 0 -1 0.5 -0.5 s1
6 0 1 -1 0 0
7 0 -1 -1 0 0
8 -1 0 -1 0 0
Total 1.00 0.00
W(alpha) 0.5
w1 = 1
w2 = 0
b = -2
1)(  bxwy j
T
j
Data Mining (BGU) Prof. Mark Last
Lecture No. 9 45
Why Is SVM Effective on High Dimensional Data?
 The complexity of trained classifier is characterized by the # of 
support vectors rather than the dimensionality of the data
 The support vectors are the essential or critical training examples —
they lie closest to the decision boundary (MMH)
 If all other training examples are removed and the training is 
repeated, the same separating hyperplane would be found
 The number of support vectors found can be used to compute an 
(upper) bound on the expected error rate of the SVM classifier, which 
is independent of the data dimensionality
 Thus, an SVM with a small number of support vectors can have good 
generalization, even when the dimensionality of the data is high
Data Mining (BGU) Prof. Mark Last
Non-linear SVMs
 Datasets that are linearly separable with noise work out 
great:
0 x
0 x
x2
0 x
 But what are we going to do if the dataset is just too hard? 
 How about… mapping data to a higher-dimensional space:
This slide is courtesy of www.iro.umontreal.ca/~pift6080/documents/papers/svm_tutorial.ppt
),( 2kk xx
46Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Non-linear SVMs:  Feature Space
 General idea:  the original input space can be mapped to 
some higher-dimensional feature space where the training 
set is separable:
Φ:  x→ φ(x)
This slide is courtesy of www.iro.umontreal.ca/~pift6080/documents/papers/svm_tutorial.ppt
47Lecture No. 9
Data Mining (BGU) Prof. Mark Last
48Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Linear Classifiers in High-
Dimensional Spaces
Var1
Var2 Constructed 
Feature 1
Find function (x) to map to 
a different space
Constructed 
Feature 2
49Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Advantages of Non-Linear Surfaces
Var1
Var2
50Lecture No. 9
Data Mining (BGU) Prof. Mark Last
bxxybxwxg j
T
ii
SVi
ij
T
j  

)()()()( a
Nonlinear SVMs: The Kernel Trick
 With this mapping, our discriminant function is now:
 No need to know this mapping explicitly, because we only 
use the dot product of feature vectors in both the training 
and test.
 A kernel function is defined as a function that corresponds 
to a dot product of two feature vectors in some expanded 
feature space:
( , ) ( ) ( )Ti j i jK  x x x x
51Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Nonlinear SVMs: The Kernel Trick
2-dimensional vectors x=[x1   x2];  
let K(xi,xj)=(1 + xi
Txj)
2
, (Polynomial kernel of degree 2)
Need to show that K(xi,xj) = φ(xi)
Tφ(xj):
K(xi,xj)=(1 + xi
Txj)
2
,
= 1+ xi1
2xj1
2 + 2 xi1xj1 xi2xj2+ xi2
2xj2
2 + 2xi1xj1 + 2xi2xj2
= [1  xi1
2  √2 xi1xi2  xi2
2  √2xi1  √2xi2]
T [1  xj1
2  √2 xj1xj2  xj2
2  √2xj1  
√2xj2] 
= φ(xi)
Tφ(xj),    where φ(x) = [1  x1
2  √2 x1x2  x2
2   √2x1  √2x2]
 An example:
This slide is courtesy of www.iro.umontreal.ca/~pift6080/documents/papers/svm_tutorial.ppt
52Lecture No. 9
What is the number of new dimensions?
Data Mining (BGU) Prof. Mark Last
Nonlinear SVMs: The Kernel Trick
 Linear kernel:
2
2
( , ) exp( )
2
i j
i jK


 
x x
x x
( , ) Ti j i jK x x x x
( , ) (1 )T pi j i jK  x x x x
0 1( , ) tanh( )
T
i j i jK   x x x x
 Examples of commonly-used kernel functions:
 Polynomial kernel of degree P:
 Gaussian (Radial-Basis Function (RBF) ) kernel:
 Sigmoid:
 In general, functions that satisfy Mercer’s condition can be 
kernel functions.
53Lecture No. 9
Data Mining (BGU) Prof. Mark Last
What Functions are Kernels?
 For some functions K(xi,xj) checking that K(xi,xj)= φ(xi)
Tφ(xj) can 
be cumbersome. 
 Mercer’s theorem:  
Every semi-positive definite symmetric function is a kernel
 Semi-positive definite symmetric functions correspond to a semi-
positive definite symmetric Gram matrix:
K(x1,x1) K(x1,x2) K(x1,x3) … K(x1,xn)
K(x2,x1) K(x2,x2) K(x2,x3) K(x2,xn)
… … … … … 
K(xn,x1) K(xn,x2) K(xn,x3) … K(xn,xn)
K=
54Lecture No. 9
For more details, see HTF, section 5.8.1
1) K(x; y) = K(y; x)
2) ∀𝑐 ∈ 𝑅𝑛, 𝑐𝑇𝐾𝑐 ≥ 0
Data Mining (BGU) Prof. Mark Last
Nonlinear SVM: Optimization
 Original SVM formulation
 n inequality constraints
 n positivity constraints
 n number of  variables
 The solution of the discriminant 
function is
 The (Wolfe) dual of this problem
 one equality constraint
 n positivity constraints
 n inequality constraints
 n number of a variables (Lagrange 
multipliers)
 Objective function more 
complicated
 NOTICE: Data only appear as (xi) 
(xj)
0
 ,1))((  ..


i
iii xbxwyts



i
i
bw
Cw 
2
, 2
1
min
 

i
i
i iyts
0
 ,0C  .. i
a
a
 
ji i
iijijiji
a
yxxyy
i ,
))()((
2
1
min aaa
SV
( ) ( , )i i
i
g K ba

 x x x
55Lecture No. 9
Data Mining (BGU) Prof. Mark Last
Support Vector Machine: 
Algorithm
 1. Choose a kernel function
 2. Choose a value for C
 SVM complexity constant which sets the tolerance for 
misclassification, where higher C values allow for 'softer' 
boundaries and lower values create 'harder' boundaries. A 
complexity constant that is too large can lead to over-fitting, while 
values that are too small may result in over-generalization (RM)
 3. Solve the quadratic programming problem (many 
software packages available)
 4. Construct the discriminant function from the support 
vectors 
Lecture No. 9 56
Data Mining (BGU) Prof. Mark Last
Non-linear SVM Example
Lecture No. 9 57
i X1 X2 Y
1 2 2 1
2 2 -2 1
3 -2 -2 1
4 -2 2 1
5 1 1 -1
6 1 -1 -1
7 -1 -1 -1
8 -1 1 -1
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5
X
2
X1
1 -1
Which kernel should we use?
Data Mining (BGU) Prof. Mark Last
58
SVM vs. Neural Network
 SVM
 Deterministic algorithm
 Nice Generalization 
properties
 Hard to learn – learned in 
batch mode using 
quadratic programming 
techniques
 Using kernels can learn 
very complex functions
 Neural Network
 Nondeterministic algorithm
 Generalizes well but doesn’t 
have strong mathematical 
foundation
 Can easily be learned in 
incremental fashion
 To learn complex functions—
use multilayer perceptron (not 
that trivial)
 Linear regression
 Linear output neuron
 Logistic regression
 Sigmoid output neuron
Lecture No. 9
Data Mining (BGU) Prof. Mark Last
SVM Related Links
 SVM Website
 http://www.kernel-machines.org/
 Representative implementations
 LIBSVM: an efficient implementation of SVM, multi-
class classifications, nu-SVM, one-class SVM, 
including also various interfaces with java, python, etc.
 SVM-light: simpler but performance is not better than 
LIBSVM, support only binary classification and only C 
language
 SVM-torch: another recent implementation also written 
in C.
Lecture No. 9 59
Data Mining (BGU) Prof. Mark Last
60
SVM—Introduction Literature
 “Statistical Learning Theory” by Vapnik: extremely hard to 
understand, containing many errors too.
 C. J. C. Burges. A Tutorial on Support Vector Machines for 
Pattern Recognition. Knowledge Discovery and Data Mining, 
2(2), 1998.
 Nello Cristianini and John Shawe-Taylor. An Introduction to 
Support Vector Machines and Other Kernel-Based Learning 
Methods. Cambridge University Press, New York, NY, USA, 
1999.
 Hastie, T., Tibshirani, R., and Friedman, J., The Elements of 
Statistical Learning: Data Mining, Inference, and Prediction, 
2nd Edition, Springer Verlag, 2009 (Chapters 4, 12)
Lecture No. 9
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מבוא\סילבוס-הקורס\מדעי-הנתונים-2019-מ.-לסט.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
 
    
 
 .Fax   08-6479346פקס.   Tel.    08-6479347  טל.
 Israel  ישראל Beer-Sheva    84105 שבע -באר  P.O.B  653ת.ד. 
http://www.ise.bgu.ac.il 
 הפקולטה למדעי ההנדסה
 מידעו תוכנה המחלקה להנדסת מערכות
Faculty of Engineering Sciences 
Department of Software and Information 
Systems Engineering 
 
 
 מדעי הנתונים ובינה עסקית  שם הקורס בעברית:
 Data Science and Business Intelligence שם הקורס באנגלית:
  3105-1-372 מספר הקורס:
 ש"ש 5, סה"כ: 1, מעבדה 1, תרגיל 3 הרצאה:  הקורס:מבנה 
 4.0 זכות:נקודות 
 מרצה: פרופ' מרק לסט 
 מתרגלים: גב' ניבה חזון, מר גיל אברהמי
 סמסטר ב', תשע"ט
 
  תאור הקורס:
נמשיך עם סקירה של  לאחר מכן. (Data Science) נתוניםה מדעי של תחוםל מבואעם  נפתחהקורס 
תורת האינפורמציה. בחלק באת הרקע הדרוש  נספק, נתונים. כמו כן הנדסתהכנת נתונים ולטכניקות 
, הכולל עצי החלטולמידה חישובית כריית נתונים לשיטות הנפוצות ביותר האת  נכסהשל קורס,  המרכזי
של , מכונות תצפיות מבוססת הבייס, למיד , למידתרשתות נוירונים מלאכותיות ,עמומות-אינפו רשתות
בינה עסקית בניתוח אשכולות. נסיים את הקורס עם מושגי היסוד ו, קשר, חוקי (SVMה )תמיכ יווקטור
(Business Intelligence) מקוון עיבוד אנליטיומחסני נתונים  כגון (OLAP.) 
 למדו בכיתה.שבאמצעות טכניקות ואלגוריתמים  מעשיותתרגול, התלמידים יפתרו בעיות בשיעורי 
 .Python-ו R: מידעכריית ל ביותר פופולרייםה םכליבשימוש בהמעבדה, התלמידים יתנסו  בשיעורי
 
 מטרות הקורס:
יישומן לפתרון  בתחומים של מדעי הנתונים ובינה עסקית כוללהשיטות המובילות  ללמוד ולתרגל את
 בעולם האמיתי.בעיות 
 
 תנאי קדם:
 מבוא להסתברות וסטטיסטיקה 37211021
 שערותהבדיקת ו אמידה 37212021
 נתוניםבסיסי  37213305
 
    
 
 .Fax   08-6479346פקס.   Tel.    08-6479347  טל.
 Israel  ישראל Beer-Sheva    84105 שבע -באר  P.O.B  653ת.ד. 
http://www.ise.bgu.ac.il 
 הפקולטה למדעי ההנדסה
 מידעו תוכנה המחלקה להנדסת מערכות
Faculty of Engineering Sciences 
Department of Software and Information 
Systems Engineering 
 
 דרישות הקורס והרכב הציון:
  המצגות וחומר הקריאה.   , שיעורי תרגול, שיעורי מעבדהבסס על הרצאות בכיתהמתהקורס
 המפורסמות באתר הקורס אינן מכסות בהכרח את כל החומר הנלמד בכיתה.
 מכסה את הבוחן מקוון  יפורסם באתר הקורס פעמים במהלך הסמסטר, 5-בחנים מקוונים: כ
 ימועד עדמקוונים הבחנים האת  לבצע. התלמידים חייבים בשיעורים הקודמיםנלמד ש החומר
האישית של כל תלמיד לבדוק באתר הקורס על בסיס קבוע  ואחריות זו. שיפורסמו באתר ההגשה
 .10%-כ: של כל הבחנים משקל כולל .חדשיםבחנים  את ההודעות על פרסום
 משקל כל הסופיציון המ 20%-ככולל של בית במשקל  יפורסמו כחמש עבודותבית:  עבודות .
בזוגות במועדים שייקבעו. ביחידים או יש להגיש  את עבודות הביתיפורסם באתר קורס.  עבודה
 .יותהם יכללו מטלות חישוביות ותכנות
  בצורת קוד או פלט תוכנה. דרישות  יהיה כרוך בגשת דו"ח קצרמפגשי מעבדה: כל מפגש מעבדה
 .10%-כ –משימות אלה המשקל הכולל של פורסמו באתר הקורס. מפורטות י
 60%-. משקל: כתתקיים בחינה סופית: בסוף הסמסטר, ה מסכמתבחינ. 
 ( מחלה או מילואים( תקבל ציון של אפס.  כגון כל מטלה שלא תוגש בזמן ללא סיבה מוצדקת 
 הקורס למעברחינה הסופית נדרשים בבוהצלחה  השתתפות בכל מפגשי המעבדה.  
 
 (:הביבליוגרפירשימת קריאה )
 קריאות חובה
T1 Han, J., Kamber, M., and Pei, J. Data Mining: Concepts and Techniques, 3rd Edition, 
Morgan Kaufmann, 2011. 
T2 Kelleher, J. D., Mac Namee, B., & D'Arcy, A., Fundamentals of Machine Learning for 
Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies. MIT Press, 
2015. 
T3 Hastie, T., Tibshirani, R., and Friedman, J., The Elements of Statistical Learning: Data 
Mining, Inference, and Prediction, Second Edition, Springer Verlag, 2009. 
 
 קריאות רשות
R1 Mitchell, T.M., Machine Learning, McGraw-Hill, 1997. 
R2 Maimon, O. and Last, M., Knowledge Discovery and Data Mining – The Info-Fuzzy 
Network (IFN) Methodology, Kluwer Academic Publishers, Massive Computing 
Series, 2000. 
R3 E. Sperley, Enterprise Data Warehouse: Planning, Building, and Implementation , 
Volume 1, Prentice-Hall, 1999. 
R4 Pyle, D., Data Preparation for Data Mining, Morgan Kaufmann, San Francisco, CA, 
1999 
 
 
    
 
 .Fax   08-6479346פקס.   Tel.    08-6479347  טל.
 Israel  ישראל Beer-Sheva    84105 שבע -באר  P.O.B  653ת.ד. 
http://www.ise.bgu.ac.il 
 הפקולטה למדעי ההנדסה
 מידעו תוכנה המחלקה להנדסת מערכות
Faculty of Engineering Sciences 
Department of Software and Information 
Systems Engineering 
 
 ית המפגשים:תכנ
 קריאה נדרשת נושא השיעור שיעור מס'
מבוא למדעי הנתונים וגילוי ידע בבסיסי  1
 נתונים
T1, Ch. 1 
T2, Ch. 1 
 והנדסת נתונים הכנת הנתונים 2
 
T1, Ch. 3 
T2, Ch. 3 
 T2, Ch. 4 מבוא לתורת האינפורמציה 3
R2, Sec. 1.7 + 
Appendix A 
  1בוחן מקוון מס'  
 T1, Sec. 8.2 באמצעות עצי החלטה למידה 4-5
 R2, Ch. 3 עמומות-רשתות אינפו 6
  2בוחן מקוון מס'  
 T1, Sec. 9.2 רשתות נוירונים מלאכותיות 7
T3, Ch. 11 
 T1, Sec. 8.3, 9.1 שיטות למידה בייסיאניות 8
מכונת וקטורים למידה מבוססת תצפיות ו 9
 (SVM) תומכים
T1, Sec. 9.3, 9.5 
R1, Ch. 8  
T3, Ch. 12 
  3בוחן מקוון מס'  
 T1, Ch. 6 גילוי של חוקי קשר 10
 T1, Ch. 10 ניתוח אשכולות 11
T3, Ch. 14.3 
  4בוחן מקוון מס'  
(BIבינה עסקית )מבוא ל 12 , מתודולוגיה של ׂ 
 מחסני נתונים
T1, Sec. 4.1 
T2, Ch. 2 
, OLAP)ניתוח אנליטי מקוון ) 13  ׂ  ׂBig Data  
Predictive Data Analytics 
T1, Sec. 4.2 
T2, Ch. 11 
  5בוחן מקוון מס'  
 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\2018-Lab-Requirements\דרישות-ונהלים_2018.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות
מדעי הנתונים
מעבדה 1
היכרות ונהלים
28 פברואר 19
1

2


1
היכרות
גיל אברהמי
gilav@post.bgu.ac.il
שעות קבלה:  
ימי חמישי 14:10-15:00, בניין 96 חדר 123
בתיאום מראש במייל
28 פברואר 19
2

3


2
תכני ונהלי המעבדה
תכני המעבדה
מעבדות 1-5:     R
מעבדות 6-10:   Python Data Mining
מעבדות 11-13: RapidMiner 

נהלים
בכל מעבדה יינתן תרגיל בנוגע לחומר הנלמד בה.
את התרגילים יש להגיש לתיבת ההגשה המתאימה ב-moodle עד למועד המעבדה הבאה.
הנוכחות במעבדות וכן הגשת כל התרגילים הם תנאי הכרחי לסיום הקורס.
תוכן מפגשי המעבדה כלול במבחן!
28 פברואר 19
3

4


3
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-1א---R---Introduction\Lab1_Introduction_R_1.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות
מדעי הנתונים
מעבדה 1a

R
Introduction
28 פברואר 19
1
Written by Noa Eyal

2


1
About R
R is a free software environment for statistical computing and graphics. 
Interpreted language.
Support matrix arithmetic.

28 פברואר 19
2

3

R היא שפת תכנות וסביבת עבודה שמשמשת בעיקר לניתוחים סטטיסטיים .
2
R installation
28 פברואר 19
3
https://cran.r-project.org/bin/windows/base/



https://cran.r-project.org/bin/windows/base/



https://cran.r-project.org/bin/windows/base/
R-studio installation
https://www.rstudio.com/



4


3
R Help
?function_name (for example: ?sum) opens a help window about the function
Google
str(function_name) – returns summary about objects or functions. 
	


28 פברואר 19
4

5
“Hello World” example
Go to File -> New script
Type: print("hello world")
run the script by ctrl+A and ctrl+R
> print("hello world")
[1] "hello world"

28 פברואר 19
5

6
Datasets in R
> library(“datasets”)   # loads the package
> data()  # list of the datasets in loaded packages
> data(AirPassengers) # load the dataset “AirPassengers” into   memory 
> AirPassengers
Jan  Feb Mar Apr May Jun Jul   Aug Sep Oct Nov Dec
1949 112 118 132 129 121 135 148 148 136 119 104 118
1950 115 126 141 135 125 149 170 170 158 133 114 140
1951 145 150 178 163 172 178 199 199 184 162 146 166
1952 171 180 193 181 183 218 230 242 209 191 172 194
1953 196 196 236 235 229 243 264 272 237 211 180 201
1954 204 188 235 227 234 264 302 293 259 229 203 229
1955 242 233 267 269 270 315 364 347 312 274 237 278
1956 284 277 317 313 318 374 413 405 355 306 271 306
1957 315 301 356 348 355 422 465 467 404 347 305 336
28 פברואר 19
6

7

ניתן להשתמש ב-datasets שקיימים בחבילת datasets בסיסית של R
6
Data types in R
28 פברואר 19
7

8
Vectors
How to create a vector
	> x<-c(1,2,3,4,5)
	> x
	[1] 1 2 3 4 5
	> x[3]   #specific element in the vector
	[1] 3
Another possibility to create a vector:
      vector()       # Produces a vector of given length and 		        mode
	> x<-vector("numeric",length=5)
	> x
	[1] 0 0 0 0 0

28 פברואר 19
8

9

וקטור מכיל סוג אחד של משתנה (מאותה מחלקה). 
קיימות שתי דרכים ליצור וקטור, כפי שמופיע בשקף.

8
List
An ordered collection of objects (of mixed types)

> x<-list("a",1,TRUE,1+4i)
> x 
[[1]] 
[1] "a" 

[[2]] 
[1] 1 

[[3]] 
[1] TRUE 

[[4]] 
[1] 1+4i
28 פברואר 19
9
> a<-list(names=c("Dan","Tom","Ron"),			age=c(21,12,32))
> a
$names
[1] "Dan" "Tom" "Ron"
$age
[1] 21 12 32

> a$names
[1] "Dan" "Tom" "Ron"
> a$age
[1] 21 12 32
> a$names[2]
[1] "Tom"


Each element in the list is a vector of a certain type

10

רשימה היא וקטור שיכול להכיל סוגים שונים של משתנים.
ניתן לגשת לרשימה גם בעזרת $ (ואז לא צריך מרכאות) או ע"י סוגריים מרובעים, שם הפיצ'ר צריך להיות במרכאות).
9
List (Cont.)
> n = c(2, 3, 5) 
> s = c("aa", "bb", "cc", "dd", "ee") 
> b = c(TRUE, FALSE, TRUE, FALSE, FALSE) 
> x = list(n, s, b, 3)   # x contains copies of n, s, b
> x[2] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee”
> x[[2]] 
[1] "aa" "bb" "cc" "dd" "ee"
> x[[2]][1] = "ta" 
> x[[2]] 
[1] "ta" "bb" "cc" "dd" "ee" 
28 פברואר 19
10

11
Factors
Type of vectors for categorical data
> data(PlantGrowth)
> head(PlantGrowth)
  weight   group
1   4.17     ctrl
2   5.58     ctrl
3   5.18     ctrl
4   6.11     ctrl
5   4.50     ctrl
6   4.61     ctrl

> class(PlantGrowth$group)
[1] "factor"
28 פברואר 19
11
> PlantGrowth$group
 [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1 trt1 trt1 trt1 trt1 trt1 trt2 trt2
[23] trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2
Levels: ctrl trt1 trt2
> unique(PlantGrowth$group) [1] ctrl trt1 trt2 Levels: ctrl trt1 trt2


12

פקטור הוא וקטור עבור משתנים קטגוריילים. 
נייבא את PlantGroth ואם נבדוק את ה-class של הפיצ'ר group נראה שהוא Factor
כאשר נפנה ישירות לעמודה זו ע"י PlantGrowth$group נקבל בנוסף לוקטור, גם את ה-levels כלומר אילו קטגוריות קיימות.

11
Matrices
A matrix is a collection of data elements arranged in a two-dimensional rectangular layout. 
> M=matrix(1:20,nrow=4,ncol=5)
       [,1] [,2]  [,3]   [,4]   [,5]
[1,]    1     5     9     13     17
[2,]    2     6    10    14     18
[3,]    3     7    11    15     19
[4,]    4     8    12    16     20

> M[2,3]   # element in row 2 and col 3
 [1] 10

28 פברואר 19
12

13

ניתן ליצור מטריצה ע"י ציון וקטור ולאחר מכן פרוט מס' השורות ומספר העמודות, איכלוס המטריצה יעשה לפי הטורים (קודם הטור הראשון יתמלא, אח"כ השני וכו')


12
Matrices (cont.)
We can create matrix by binding vectors:
Binding columns with cbind
Binding rows with rbind
> a<-c(1,2,3,4)
> b<-c(5,6,7,8)
> c<-c(9,10,11,12)

> rbind(a,b,c)

    [,1]  [,2] [,3]   [,4]
a     1     2     3       4
b     5     6     7       8
c     9    10    11    12

28 פברואר 19
13
cbind(a,b,c)

         a    b     c
[1,]   1    5     9
[2,]   2    6    10
[3,]   3    7    11
[4,]   4    8    12


14

ניתן ליצור מטריצה ע"י חיבור וקטורים... להדגיש כי כאשר משתמשים ב-rbind, שמות השורות במטריצה הם שמות הוקטורים המקוריים, באמצעות cbind, שמות הטורים הם שמות הוקטורים המקוריים. 

13
Data Frames
Tightly coupled collections of variables which share many of the properties of matrices and of lists, used as the fundamental data structure by most of R's modeling software.
Create dataframe:
 df<-data.frame(name=c("dor","tom","tal"),age=c(21,22,30),city=factor(c("tel-aviv","tel-aviv","beer-sheva")))
> df
     name   age        city
1    dor      21      tel-aviv
2    tom     22      tel-aviv
3    tal       30       beer-sheva
28 פברואר 19
14

15

R מאוד שימושית ונוחה לעבודה של ניתוח נתונים מכיוון שיש מבנה נתונים מיוחד לטבלאות. 
DataFrame הוא בעצם רשימה שלכל אלמנט בה יש את אותו אורך. כל טור בטבלה מייצג אלמנט ברשימה והטורים לא חייבים להיות מאותו סוג (בניגוד למטריצות).
14
Data Frames (cont.)
Read data frame from CSV file:
	DF<-read.csv(file, header=TRUE, 			sep=“,” …)
Get the working directory:
	> getwd()
Set working directory: 
	> setwd()
28 פברואר 19
15

16

ניתן לייבא קובץ של אקסל ישירות לתוך dataFrame ע"י read.csv . 
ניתן לבדוק את מיקום ה-working directory ע"י הפקודה getwd() ולשנות את המיקום ע"י setwd()
15
Data Frames (cont.)
Subseting
Selecting column:
	> PlantGrowth[,”weight”] 
	> PlantGrowth$weight
Selecting rows:
	> PlantGrowth[2:4,]
	     weight  group
	2     5.58    ctrl
	3     5.18    ctrl
	4     6.11    ctrl
Information about the dataFrame:
nrow/ncol  # returns the num of rows/col in the D
>summary(PlandtGrowth)
weight   	       group   
 Min.   :3.590 	       ctrl:10  
 1st Qu.:4.550        trt1:10  
 Median :5.155      trt2:10  
 Mean   :5.073            
 3rd Qu.:5.530            
 Max.   :6.310 
28 פברואר 19
16

Equivalent

17

כאשר משתמשים ב-$ אין צורך במרכאות, ניתן ללחוץ על tab כדי לא לכתוב את כל השם של האובייקט...

16
Additional functions
names(x)   # returns the names in x (if exist)
length(x)   # returns the length of x
append()   # Add elements to a vector
ls()         # List objects in current environment
as.class(x) # convert the class of x 
28 פברואר 19
17

18
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-1ב---R---Introduction\Lab1_Introduction_R_2.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות
מדעי הנתונים
מעבדה 1b

R
Introduction
27 פברואר 18
1
Written by Noa Eyal

2


1
Control Flow
If-Then
Loop operators:
For
While
27 פברואר 18
2

3
Control Flow
If-Then
> x <- 5
> if (x > 0){ print (“Positive”)}
If-Then-Else
> x <- 5
> y <- if (x > 0) 5 else 6
Nested if-Then-Else
> x <- 0
> if (x > 0){ print (“Positive”) }
> else if (x < 0){ print (“Negative”) }
> else print (“Zero”) 





27 פברואר 18
3

4
Control Flow
For loop
> for(i in 1:5) print(1:i)
> for(n in c(2,5,10,20,50)) {
> x <- n^2
> print(x)}
While lopp
> x <- 1
> while(x < 5) {x <- x+1; print(x);}




27 פברואר 18
4

5
Functions
bubbleSort <- function(v) {
	n <- length(v)
	for(i in 1:(n-1)){
		for(k in 1:(n-i)){
			if(v[k] > v[k+1]){
				temp <- v[k]
				v[k] <- v[k+1]
				v[k+1] <- temp
			}
		}
	}
	return(v)		# you can write just v instead of)
}
> v <- c(3,2,1)
> bubbleSort(v)
[1] 1 2 3

27 פברואר 18
5

6
Functions
kelvin_to_celsius <- function(temp_K) {
  temp_C <- temp_K - 273.15
  return(temp_C)
}

fahrenheit_to_kelvin <- function(temp_F) {
  temp_K <- ((temp_F - 32) * (5 / 9)) + 273.15
  return(temp_K)
}

fahrenheit_to_celsius <- function(temp_F) {
  temp_K <- fahrenheit_to_kelvin(temp_F)
  temp_C <- kelvin_to_celsius(temp_K)
  return(temp_C)
}
27 פברואר 18
6

7
Functions
Invoking the functions:
# boiling point of water
> fahrenheit_to_kelvin(212)
[1] 373.15

# absolute zero in Celsius
> kelvin_to_Celsius(0)
[1] -273.15

# freezing point of water in Celsius
> fahrenheit_to_celsius(32.0)
[1] 0



27 פברואר 18
7

8
Functions
Match function calls:

func <- function (x,y, ...) {
  whatWasCalled <- match.call(expand.dots = TRUE) 	
  print(whatWasCalled)
  firstArgValue = whatWasCalled[[2]] 			
  print(firstArgValue)
  Xname <- names(whatWasCalled)[2]			
  print (Xname)
}
# First argument is the name of the function



27 פברואר 18
8

9
Functions
Invoking the function:
func(x = “a”, y = 10, z = 20, t=30)
[1] func(x = "a", y = 10, z = 20, t = 30)
[1] a
[1] "x”




27 פברואר 18
9

10
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-2---R---Data-preparation\Lab2_Data-preparation_R.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות
מדעי הנתונים
מעבדה 2

R
Data preparation
06 מרץ 19
1
Written by Noa Eyal

2
Handling Missing Data
06 מרץ 19
2

3

על מנת לטעון קבצי נתונים מסוגים שונים: http://www.r-tutor.com/r-introduction/data-frame/data-import
2
Handling Missing Data
NaN – “not a number”, Impossible values (e.g., dividing by zero) 
Na – “not available”
	, missing values

NaN is necessarily Na 
Na is not necessarily NaN 
is.na(x)  # returns TRUE if
	       x is missing
06 מרץ 19
3
> x<-0/0
> x
[1] NaN

> is.na(x)
[1] TRUE
> is.nan(x)
[1] TRUE

v<-c(1,2,NA,3,NA,5)
> is.na(v)
[1] FALSE FALSE  TRUE FALSE  TRUE FALSE
> a<-c(1,2) 
> a[2]
[1] 2
> a[3]
[1] NA

4

לא כל האלגוריתמים יודעים להתמודד עם missing data, לכן יש צורך לטפל בערכים חסרים לפני שמתחילים לבנות את המודל....
3
Handling Missing Data (cont.)
06 מרץ 19
4
> data(airquality)
> dim(airquality)
[1] 153 6
> head(airquality) 
    Ozone  Solar.R Wind Temp Month Day
1      41       190    7.4        67       5      1
2      36       118    8.0        72       5      2
3      12       149    12.6      74       5      3
4      18       313    11.5      62       5      4
5      NA      NA     14.3      56       5      5
6      28       NA     14.9      66       5      6




5

כך נראים הנתונים שלנו כרגע.
4
Handling Missing Data (cont.)
06 מרץ 19
5
Missing values in column Solar.R:
> bad<-is.na(airquality$Solar.R)
Create a vector of Solar.R without missing values:
> airquality$Solar.R[!bad]
 [1] 190 118 149 313 299  99  19 194 256 290 274  65 334 307  78 322  44   8 320  25  92  66 266  13 252 223 279 286 287 242….

6

נניח שאנחנו בוחרים טור מסויים ב-dataframe, אנחנו יכולים ליצור וקטור שיכיל TRUE אם ערך מסוים בטור חסר באמצעות הפנוקציה is.na(), לאחר מכן ניתן להשאיר את הערכים באותו טור שאינם חסרים באמצעות subsetting בעזרת: airquality$Solar.R[!bad]


5
Handling Missing Data (cont.)
If we want to keep only the rows with no missing values:
> NoMissing<-complete.cases(airquality)
> NoMissing
[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE…

> airquality[NoMissing,  ]
  Ozone Solar.R Wind Temp Month Day
1    41     190     7.4      67        5         1
2    36     118     8.0      72        5         2
3    12     149     12.6    74        5         3
 4    18     313     11.5    62        5         4
 7     23    299      8.6     65        5         7
…
An alternative method: The function na.omit()
> na.omit(airquality)

06 מרץ 19
6

7

ניתן להשתמש בפונקציה complete.cases() על מנת להשאיר ב-dataframe רק את השורות שאין בהן ערכים חסרים.
ניתן לקבל תוצאה זהה ע"י השימוש בפונקציה na.omit(). הפונקציה אינה מסירה ערכים חסרים מתוך ה-dataFrame אלא מדפיסה אותו ללא ערכים חסרים (את התוצאה ניתן לשמור ב-dataFrame חדש.

6
Handling Missing Data (cont.)
Some functions like mean() returns NA if the object has missing values… we can use the na.rm argument to ignore missing values:
> mean(airquality$Solar.R)
[1] NA
> mean(airquality$Solar.R, na.rm=TRUE)
[1] 185.9315
06 מרץ 19
7

8
Handling Missing Data (cont.)
Replacing missing data with column mean:
Let’s calculate the average of all columns with the function colMeans and save it into the variable aqMeans:
>aqMeans<-colMeans(airquality,na.rm=TRUE)
> aqMeans
     Ozone        Solar.R       Wind       Temp      Month        Day 
      42.129      185.931     9.957     77.882     6.993       15.803 
The indexes of the missing values:
> indx<-which(is.na(airquality), arr.ind=TRUE)
now let’s replace the missing values:
> airquality[indx]<-aqMeans[indx[,2]]
06 מרץ 19
8

9

חשוב להדגיש שניתן להשתמש בכל מיני שיטות על-מנת להחליף ערכים חסרים אך לא נמחיש את כולן.
8
Handling Missing Data (cont.)
The result:
> head(airquality)
 Ozone    Solar.R   Wind   Temp   Month   Day
1 41.00    190.00     7.4      67          5          1
2 36.00    118.00     8.0      72          5          2
3 12.00    149.00    12.6     74          5          3
4 18.00    313.00    11.5     62          5          4
5 42.12    185.93    14.3     56          5          5
6 28.00    185.93    14.9     66          5          6

Alternative method:
Use the function na.aggregate (zoo package)
> library(zoo)
> na.aggregate(airquality) 

06 מרץ 19
9

10

If zoo isn’t installed yet, first type
>install.packages("zoo")
And only then
>library(zoo)
9
Normalization
06 מרץ 19
10

11
*apply functions
*apply – loop functions
apply
lapply
sapply
vapply
…
Split-Apply-Combine - Each of the *apply functions will SPLIT up some data into smaller pieces, APPLY a function to each piece.
06 מרץ 19
11

12
lapply, sapply - examples
> class(airquality)
[1] "data.frame" 
>lapply(airquality, class) 
$Ozone [1] "integer" $Solar.R [1] "integer" $Wind [1] "numeric" 
$Temp [1] "integer" $Month [1] "integer" $Day [1] "integer“
Since the returned value is a list where every element is of similar length, we cam Simplify it by using sapply
>sapply(airquality, class)
  Ozone     Solar.R     Wind        Temp     Month     Day 
"integer" "integer" "numeric" "integer" "integer" "integer"
06 מרץ 19
12

13

Notice apply will simplify the result to a vector if the result of the corresponding lapply is a list of vectors of length 1. If the result would be a list of vectors of similar lengths of length >1, the result will be a Matrix.
We can try this using:
>lapply(airquality, range)
Vs.
>sapply(airquality, range)
12
z-score normalization
We can use the function scale() in order to normalize the airquality dataset
In order to apply the scale function for each column in the airquality dataset we can use the apply function
	> str(apply)
	function (X, MARGIN, FUN, ...) 
	X – DataFrame
	MARGIN – 1 for rows, 2 for columns
	FUN – the function we want to apply
	… - arguments of the function we want to apply
Let’s apply the scale function on the airquality dataset:
	> ZAQ<-apply(airquality, 2, scale, center=TRUE, scale=TRUE) 

	


06 מרץ 19
13
Reduce mean
Divide by std. 
Perform for columns

14

ניתן לעשות נורמליזציה של Z-score, נשתמש בפונקציה scale(),  לפונקציה יש מספר ארגומנטים:
X – הווקטור עליו אנו רוצים לבצע את הנורמליזציה
center=TRUE – מחסיר מכל ערך בווקטור את ממוצע הווקטור
scale=TRUE – במידה ו-center=TRUE, מחלק כל ערך שממנו הוחסר הממוצע בסטיית התקן
על-מנת שנוכל להחיל את הפונקציה על כל ה-dataset בו-זמנית, יש צורך ללמד את הפונקציה apply(), הפונקציה מקבלת dataFrame, בנוסף יש צורך לציין את ה-MARGIN, את הפונקציה שאותה אנחנו רוצים להחיל ואת הארגומנטים של הפונקציה. ניתן כמובן גם להשתמש בלופ רגיל...
הפונקציה apply מחזירה matrix ויש צורך להמיר ל-dataFrame (בשקופית הבאה)

13
z-score normalization (cont.)
The apply function returns a matrix, we can use the as.data.frame() in order to convert it back to dataFrame format:
     > zaq<-as.data.frame(ZAQ)

06 מרץ 19
14
       Ozone     Solar.R      Wind     Temp      Month          Day
1    -0.034     0.045       -0.726     -1.150    -1.407       -1.670
2    -0.186     -0.754      -0.556     -0.621    -1.407       -1.557
3    -0.913     -0.410       0.750     -0.410    -1.407       -1.444
4    -0.731     1.411        0.438      -1.678    -1.407       -1.332
5        NA           NA         1.233     -2.312    -1.407       -1.219
-0.428         NA         1.403     -1.255    -1.407       -1.106
…

15
z-score normalization (cont.)
06 מרץ 19
15
Z-score normalization
> hist(zaq$Ozone, col=70, breaks=15)
> hist(airquality$Ozone, col=70, breaks=15)


16


15
Discretization
06 מרץ 19
16

17
Discretization by prior knowledge
Suppose we want to convert the continuous variable “Ozone” to nominal 
> summary(airquality$Ozone)
   Min. 	1st Qu. 	 Median    Mean 	3rd Qu.    Max.    NA's 
   1.00   	18.00  	 31.50  	 42.13 	  63.25  168.00      37 
 We can decide according to prior knowledge that:
1-60 -> Low
60-120 -> Medium
120-170 -> High
06 מרץ 19
17

18

לפעמים נרצה לעשות דיסקריטיזציה למשתנה רציף, ניתן לעשות זאת באמצעות "רוחב שווה" של אינטרוולים או תדירות שווה במספר K של אינטרוולים... כאן נעשה דיסקריטיזציה ע"פ בחירת המשתמש.

17
Discretization by prior knowledge (cont.)
Let’s write a loop in the script window:
i=0
for(x in airquality$Ozone){
	i=i+1
	if(is.na(x)==TRUE){
		next
	}else if(x<=60){
		airquality[i,"Ozone"]="Low"
	} else if(x>60 & x<=120){
		airquality[i,"Ozone"]="Medium"
	} else{
		airquality[i,"Ozone"]="High"
	}
}
airquality$Ozone<-as.factor(airquality$Ozone)

06 מרץ 19
18
If the value is NA we will skip to the next iteration
We need to convert Ozone from character to factor

19

יש להדגיש שניתן להמיר את המשתנה בדרכים פשוטות יותר או באמצעות פונקציות מחבילות אבל רציתי להדגים כאן לולאה... 

18
Discretization by prior knowledge (cont.)
>head(airquality)




> str(airquality)
'data.frame':   153 obs. of  6 variables:
 $ Ozone  : Factor w/ 3 levels "High","Low","Medium": 2 2 2 2 NA 2 2 2 2 NA ...
 $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
 $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
 $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
 $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
 $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...




06 מרץ 19
19
	Ozone	Solar.R	Wind	Temp	Month	Day
1	Low	190	7.4	67	5	1
2	Low	118	8	72	5	2
3	Low	149	12.6	74	5	3
4	Low	313	11.5	62	5	4
5	<NA>	NA	14.3	56	5	5
6	Low	NA	14.9	66	5	6

20
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-3---R---Visualization\Lab3_Visualization_R.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Written by Noa Eyal
 מעבדה 3

Visualization
מעבדות    
מדעי הנתונים
14 מרץ 19
1

2
ggplot2 library
14 מרץ 19
2

3

install.packages("ggplot2")
library(ggplot2)

2
Qplot()
qplot(x, y, data=, color=, shape=, size=, alpha=, geom=, method=, formula=, facets=, xlim=, ylim= xlab=, ylab=, main=)
x,y - Specifies the variables placed on the horizontal and vertical axis. For univariate plots (e.g., histograms), omit y.
Data – the dataframe.
main, xlab, ylab – title and axis's labels.
xlim, ylim - Two-element numeric vectors giving the minimum and maximum values for the horizontal and vertical axes, respectively.
Geom – i.e. "point", "smooth", "boxplot", "line", "histogram", "density", "bar", and "jitter".
color, shape, size, fill
alpha - Alpha transparency for overlapping elements (0-1).
…
14 מרץ 19
3

4

# color, shape, size, fill - Associates the levels of variable with symbol color, shape, or size. For line plots, color associates levels of a variable with line color. For density and box plots, fill associates fill colors with a variable. Legends are drawn automatically.

# Facets - Creates a trellis graph by specifying conditioning variables. Its value is expressed as rowvar ~ colvar. To create trellis graphs based on a single conditioning variable, use rowvar~. or .~colvar)

# method, formula - If geom="smooth", a loess fit line (Locally Weighted Scatterplot Smoothing) and confidence limits are added by default. When the number of observations is greater than 1,000, a more efficient smoothing algorithm is employed. Methods include "lm" for regression, "gam" for generalized additive models, and "rlm" for robust regression. The formula parameter gives the form of the fit. For example, to add simple linear regression lines, you'd specify geom="smooth", method="lm", formula=y~x. Changing the formula to y~poly(x,2) would produce a quadratic fit. Note that the formula uses the letters x and y, not the names of the variables.  For method="gam", be sure to load the mgcv package. For method="rml", load the MASS package.
3
mtcars datafreme
> head(mtcars)




create factors with value labels
> library(ggplot2)
> mtcars$gear <- factor(mtcars$gear,levels=c(3,4,5), labels=c("3gears","4gears","5gears"))
> mtcars$am <- factor(mtcars$am,levels=c(0,1), labels=c("Automatic","Manual"))
> mtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8), labels=c("4cyl","6cyl","8cyl")) 
14 מרץ 19
4

5


4
Kernel Density Plot
Kernel density plots for mpg grouped by number of gears (indicated by color)
> qplot(mpg, data=mtcars, geom="density", fill=gear, alpha=I(.5), main="Distribution of Gas Milage", xlab="Miles Per Gallon", ylab="Density")
14 מרץ 19
5

6

kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.
5
Scatterplot of mpg vs. hp
Scatterplot of mpg vs. hp for each combination of gears and cylinders in each facet, transmittion type is represented by shape and color.
> qplot(hp, mpg, data=mtcars, shape=am, color=am, facets=gear~cyl, size=I(3), xlab="Horsepower", ylab="Miles per Gallon")
14 מרץ 19
6

7
Regression
Separate regressions of mpg on weight for each number of cylinders.
> qplot(wt, mpg, data=mtcars, geom=c("point", "smooth"), method="lm", formula=y~x, color=cyl, main="Regression of MPG on Weight", xlab="Weight", ylab="Miles per Gallon")
14 מרץ 19
7

8
Boxplots
Boxplots of mpg by number of gears observations (points) are overlayed and jittered.
> qplot(gear, mpg, data=mtcars, geom=c("boxplot", "jitter"), fill=gear,  main="Mileage by Gear Number", xlab="", ylab="Miles per Gallon")
14 מרץ 19
8

9
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-4---R---Classification\Lab4_Classification_R.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מעבדות
מדעי הנתונים
מעבדה 4

R
Classification
19 פברואר 18
1
Written by Noa Eyal

2


1
19 פברואר 18
2
Iris dataset
> data(iris)
> head(iris)
  Sepal.Length    Sepal.Width    Petal.Length    Petal.Width    Species
1          5.1                 3.5                  1.4                        0.2            setosa
2          4.9                 3.0                  1.4                        0.2            setosa
3          4.7                 3.2                  1.3                        0.2            setosa
4          4.6                 3.1                  1.5                        0.2            setosa
5          5.0                 3.6                  1.4                        0.2            setosa
6          5.4                 3.9                  1.7                        0.4            setosa

3


2
Iris dataset
> str(iris)
'data.frame':   150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4…..
 $ Sepal.Width : num  3.5 3 3.2 …..
 $ Petal.Length: num  1.4 1.4 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2…..
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1….

> pie(table(iris$Species))
19 פברואר 18
3

4


3
Data partitioning
We can use the CARET
(short for Classification And REgression Training) package to split the dataset into training and testing sets
> library(caret)
> inTrain<-createDataPartition(y=iris$Species,p=0.7 ,list=FALSE)
> training<-iris[inTrain,]
> testing<-iris[-inTrain,]
 > nrow(training)
[1] 105
> nrow(testing)
[1] 45



dataset
class
fraction for train
should the results be in a list (TRUE) or matrix (FALSE)
19 פברואר 18
4

5

להתקנת החבילה (וכל חבילה נחוצה אחרת):
utils:::menuInstallPkgs()
יש לבחור מרשימת ה-pakages את הרלוונטיות – במקרה שלנו caret ו-e1071.

נראה כאן דרך לפצל את ה-dataset ל-train ו-test, נשתמש ב-70% train
בהתחלה נטען את הספרייה CARET, 
לאחר מכן ניצור אינדקס שיבחר לנו את השורות בשביל ה-training לפי ה-p הנבחר (במקרה זה 0.7). 
כעת נעשה subsetting, בהתחלה ל-train, נבחר את השורות לפי inTrain, לאחר מכן נבנה את ה-test ע"י subsetting עם –inTrain”", כלומר הבחירה ההופכית. 
ניתן להראות באמצעות summary(training) ו-summary(testing) שהפרופורציות של הקטגוריות ב-class נשמרות
נשתמש ב-train וב-test בהמשך... (שקופית 7)
4
Data partitioning
If we want to use 10-fold cross validation we need to use the function trainControl in order to specify the type of resampling:
> fitControl <-trainControl(method ="repeatedcv", number=10, repeats = 10)
19 פברואר 18
5

6

על-מנת להשתמש ב-cross validation יש צורך להשתמש בפונקציה trainControl שמאפשרת להגדיר פרמטרים עבור הפונקציה train שנשתמש בה בשקופית הבאה כדי לבנות את המודל
5
  Creating the model – decision tree
We can use CART in order to classify the dataset:
> CARTFit<-train(Species~., data = training, trControl = fitControl,method = "rpart")
> CARTFit
CART 

105 samples
  4 predictor
  3 classes: 'setosa', 'versicolor', 'virginica' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 

Summary of sample sizes: 94, 95, 94, 94, 93, 96, ... 

Resampling results across tuning parameters:

      cp      	   Accuracy        Kappa     Accuracy SD  Kappa SD 
  0.0000000    0.9548030     0.9315680      0.0713571    0.1083715
  0.4571429    0.7423182     0.6211156      0.1380386    0.1993878
  0.5000000    0.3747576     0.1114286      0.1361979    0.1889331

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was cp = 0. 
~ – use all other attributes for classification
Defined in slide 4
19 פברואר 18
6

7

כעת נבנה את המודל, לאחר הרצת הפונקציה train באמצעות האלגוריתם CART, נתבקש להתקין חבילה, נבחר ב-yes כדי להתקין אותה... 
ניתן להסביר כי Species הוא ה-class, לאחר מכן יש טילדה ונקודה (~.) שהמשמעות שלהם. היא שאנחנו משתמשים בכל שאר הפיצ'רים כדי לחזות את ה-class).

אם היינו רוצים לייצר מודל מהשילוב של חלק מהתכונות בלבד, היינו מציינים זאת כך:
> CARTFit<-train(Species~Sepal.Length+Sepal.Width+Petal.Length, data = training, trControl = fitControl,method = "rpart")
6
Observing the model
> CARTFit$finalModel
n= 105 
node), split, n, loss, yval, (yprob)
      * denotes terminal node
1) root 105 70 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 35  0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 70 35 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 38  3 versicolor (0.00000000 0.92105263 0.07894737) 
    7) Petal.Width>=1.75 32  0 virginica (0.00000000 0.0000
19 פברואר 18
7

8


7
Plotting the model
We can plot the tree:
> plot(CARTFit$finalMode, uniform=TRUE, main="CART")
> text(CARTFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)

 
19 פברואר 18
8

9

הפונקציה Plot משמשת לציור העץ והפונקציה text משמשת להוספת הטקסט... ניתן להדגיש כי יש חבילות ב-R שמייצרות עצים יפים וברורים יותר. 
8
Creating the model – Naïve Bayes
We can use Naïve Bayes in order to classify the dataset, this time we will boiled the model on 70% train set and predict on the test set
> nbFit<-train(Species~ ., data = training,method = "nb")
> nbFit
Naive Bayes 

105 samples
  4 predictor
  3 classes: 'setosa', 'versicolor', 'virginica' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 105, 105, 105, 105, 105, 105, ... 

Resampling results across tuning parameters:

  usekernel  Accuracy  	 Kappa     	 Accuracy SD  Kappa SD  
  FALSE         0.9560067  	0.9327271  	0.03361559   0.05160664
   TRUE         0.9534428  	0.9289742  	0.03198437   0.04874448

Tuning parameter 'fL' was held constant at a value of 0
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were fL = 0 and usekernel = FALSE. 


19 פברואר 18
9

10

כעת נבנה מודל באמצעות naïve bayes, הפעם לא נשתמש ב-CV, אלא בחלוקה ל-70% train ו-30% test שהכנו בשקופית 3
9
Prediction on test set
> NbPredict<-predict(nbFit, newdata=testing)
 [1] setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa    
[12] setosa     setosa     setosa     setosa     versicolor versicolor versicolor virginica  versicolor virginica  versicolor
[23] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor virginica  virginica  versicolor
[34] virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica 
[45] virginica 
Levels: setosa versicolor virginica
19 פברואר 18
10

11

הפונקציה מחזירה וקטור של הערכים החזויים 
10
Confusion Matrix
> CM<-confusionMatrix(NbPredict, reference=testing$Species)
Confusion Matrix and Statistics

            Reference
Prediction   setosa    versicolor 	virginica
  setosa            15             0         	   0
  versicolor       0            13         	   1
  virginica          0              2        	  14

Overall Statistics
                                         
               Accuracy : 0.9333         
                 95% CI : (0.8173, 0.986)
    No Information Rate : 0.3333         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9            
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                 	           Class: setosa    Class: versicolor Class: virginica
Sensitivity           	           1.0000               0.8667       	    0.9333
Specificity            	           1.0000               0.9667         	    0.9333
Pos Pred Value                1.0000               0.9286       	    0.8750
Neg Pred Value               1.0000               0.9355     	    0.9655
Prevalence                       0.3333               0.3333       	    0.3333
Detection Rate                0.3333               0.2889       	    0.3111
Detection Prevalence     0.3333               0.3111      	     0.3556
Balanced Accuracy         1.0000               0.9167       	    0.9333



19 פברואר 18
11

12

ניתן ליצור confusion matrix ע"י שימוש בוקטורים הערכים החזויים ובפקטור הערכים האמיתיים (נמצא בתוך הטבלה של testing)

אתם מוזמנים לנסות גם פעולות כמו:
plot(density(iris$Sepal.Length))
plot(iris$Sepal.Length, iris$Sepal.Width)
pairs(iris)

11
Ctree
> library(party)
> iris_ctree <- ctree(Species ~ ., data=training)
> plot(iris_ctree)
19 פברואר 18
12

13


12
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-5---R---Unsupervised-Learning\Lab5_Unsupervised_Learning_R.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Written by Noa Eyal
 מעבדה 5

Unsupervised Learning
מעבדות
 מדעי הנתונים
03 אפריל 19
1

2
Unsupervised learning
What happens if we don’t know the actual label of the record? 
We need to discover the label in advance. 
We can do that by using cluster analysis.
03 אפריל 19
2

3
K-means
kmeans(x, centers, iter.max = 10, nstart = 1, algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"), trace=FALSE)

x - numeric matrix of data
centers – the number of clusters (k)
iter.max - the maximum number of iterations allowed.
nstart - how many random sets should be chosen
03 אפריל 19
3

4

תיאור הפונקציה kmeans
3
K-means
>data(iris)
>iris.features<-iris
>iris.features$Species<-NULL    #delets the class column
>results<-kmeans(iris.features, 3)    #builds kmeans model with iris dataset and k=3
K-means clustering with 3 clusters of sizes 38, 50, 62
 
Cluster means:
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     6.850000    3.073684     5.742105    2.071053
2     5.006000    3.428000     1.462000    0.246000
3     5.901613    2.748387     4.393548    1.433871
 
Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3
 [61] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 1 1 3 1 1 1 1 1 1 3 3 1 1 1 1 3
[121] 1 3 1 3 1 1 3 3 1 1 1 1 1 3 1 1 1 1 3 1 1 1 3 1 1 1 3 1 1 3
 
Within cluster sum of squares by cluster:
[1] 23.87947 15.15100 39.82097
 (between_SS / total_SS =  88.4 %)
 
Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"      

Cluster assignment for each record
The size of each cluster
03 אפריל 19
4

5

האלגוריתם מסתמך על החלוקה הראשונית לקלסטרים ולכן בכל הרצה נקבל תוצאות קצת שונות... 
ניתן לראות את גודל הקלסטרים ע"י  results$size ולווקטור השתייכות הרשומות לקלסטרים באמצעות results$cluster
4
K-means
We can attach the vector of the clusters to the iris table by: 
> c_iris<-cbind(iris.features, results$cluster)
And than continue with supervised learning
> head(c_iris)
     Sepal.Length Sepal.Width Petal.Length Petal.Width results$cluster
1          5.1      	   3.5          1.4              0.2         	      3
2          4.9     	   3.0          1.4              0.2         	      3
3          4.7      	   3.2          1.3              0.2         	      3
4          4.6      	   3.1      	    1.5             0.2        	      3
5          5.0      	   3.6      	    1.4             0.2        	      3
6          5.4      	   3.9      	    1.7             0.4        	      3
03 אפריל 19
5

6
K-means performance evaluation
Let’s create a table containing the affiliation of each record to the real and kmeans clusters:

> table(iris$Species, results$cluster)




Real cluster
Predicted cluster
3	2	1	
0	0	50	Setosa
2	48	0	Versicolor
36	14	0	virginica 
>plot(iris$Petal.Length,iris$Petal.Width,col=results$cluster)
>plot(iris$Petal.Length,iris$Petal.Width,col=iris$Species)
03 אפריל 19
6

7

כפי שניתן לראות, קלסטר 1 כנראה מייצג את Setosa, קלסטר 2 כנראה מייצג את Versicolor ו-קלסטר 3 כנראה מייצג את virginica .
** המספרים בטבלה יכולים טיפה להשתנות.
ניתן לראות ש-14 רשומות נכנסו לקלסטר 3 למרות שבמציאות הן מסווגות כ-virginica .
כלומר החלוקה שהאלגוריתם עשה קרובה לאמת אך לא מדוייקת ב-100%.
הגרף מצד שמאל מראה את ההפרדה בין הקלסטרים תוך שימוש בקלסטרים שיצרנו (כל צבע מייצג קלסטר), הגרף מצד ימין מראה את אותו פיזור אך הצבעים מייצגים את הסוג האמיתי של האירוס.
6
Hierarchical clustering
Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters.
03 אפריל 19
7

8
Distance matrix
We need to find the distance matrix between the records in the dataset.
Let’s look on the first 5 rows in iris:
     > irisSample <- iris[1:5,]





We can use the function “dist” to calculate the distance between the records
     iris_distance<-dist(irisSample)
 Sepal.Length   Sepal.Width   Petal.Length   Petal.Width   Species
1          5.1            3.5                       1.4                   0.2            setosa
2          4.9            3.0                       1.4                   0.2            setosa
3          4.7            3.2                       1.3                   0.2            setosa
4          4.6            3.1                       1.5                   0.2            setosa
5          5.0            3.6                       1.4                   0.2             setosa
4	3	2	1	
			 0.5385165	2
		0.3000000	0.5099020	3
	0.2449490	0.3316625	0.6480741	4
0.6480741	0.5099020	0.6082763	0.1414214	5
03 אפריל 19
8

9

על מנת להשתמש ב-hclust יש צורך לחשב את מטריצת המרחקים של כל זוג רשומות ב-dataset. 
טבלת המרחקים נראית כמשולש מכיוון שאין צורך במשולש המשלים (המרחק בין רשומה 1 ל-2 הוא אותו מרחק כמו מרשומה 2 ל-1)

8
hclust
hclust (d, method = "complete", members = NULL) 
d – the distance matrix
method – options: 
Complete - the longest distance from any member of A to any member of B
Average -  the average distance from any member of A to any member of B
Single - the shortest distance from any member of A to any member of B. 

03 אפריל 19
9

10
Additional Reading

Association rules
03 אפריל 19
10

11
Titanic.raw dataset
> load("titanic.raw.rdata")
> str(titanic.raw)
'data.frame': 2201 obs. of 4 variables: $ Class : Factor w/ 4 levels "1st","2nd","3rd",..: 3 3 3 3 3 3 3 3 3 3 ... $ Sex : Factor w/ 2 levels "Female","Male": 2 2 2 2 2 2 2 2 2 2 ... $ Age : Factor w/ 2 levels "Adult","Child": 2 2 2 2 2 2 2 2 2 2 ... $ Survived: Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 1 1 1 1 ...
03 אפריל 19
11

12

In order to use the following functions an installation of arules and arulesViz packages is necessary using:
utils:::menuInstallPkgs()
11
arules
We will first find all association rules:
> library(arules)
> rules <- apriori(titanic.raw)
> inspect(rules)
lhs                                           rhs              support    confidence         lift
1 {}                         => {Age=Adult} 0.9504771 0.9504771  1.0000000
2 {Class=2nd}      => {Age=Adult} 0.1185825 0.9157895  0.9635051
3 {Class=1st}        => {Age=Adult} 0.1449341 0.9815385  1.0326798
4 {Sex=Female}   => {Age=Adult} 0.1930940 0.9042553  0.9513700
5 {Class=3rd}       => {Age=Adult} 0.2848705 0.8881020  0.9343750
6 {Survived=Yes} => {Age=Adult} 0.2971377 0.9198312  0.9677574
7 {Class=Crew}   => {Sex=Male}  0.3916402 0.9740113  1.2384742
…
03 אפריל 19
12

13
arules
We can also choose to generate only rules with rhs containing “Survived” only:
> rules <- apriori(titanic.raw, parameter = list(minlen=2, supp=0.005, conf=0.8),  appearance = list(rhs=c("Survived=No", "Survived=Yes"),  default="lhs"),  control = list(verbose=F))
> rules.sorted <- sort(rules, by="lift")
> inspect(rules.sorted)


lhs  rhs support confidence lift 
1 {Class=2nd, Age=Child} => {Survived=Yes} 0.010904134 1.0000000 3.095640 
2 {Class=2nd, Sex=Female, Age=Child} => {Survived=Yes} 0.005906406 1.0000000 3.095640
3 {Class=1st, Sex=Female} => {Survived=Yes} 0.064061790 0.9724138 3.010243
4 {Class=1st, Sex=Female, Age=Adult} => {Survived=Yes} 0.063607451 0.9722222 3.009650
5 {Class=2nd, Sex=Female} => {Survived=Yes} 0.042253521 0.8773585 2.715986
6 {Class=Crew, Sex=Female} => {Survived=Yes} 0.009086779 0.8695652 2.691861

03 אפריל 19
13

14
Pruning Redundant Rules
Notice rule no. 2 provides no extra knowledge, given rule no. 1.
Loosing redundant rules is required:
> subset.matrix <- is.subset(rules.sorted, rules.sorted)
> subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
> redundant <- colSums(subset.matrix, na.rm=T) >= 1
> which(redundant)
[1] 2 4 7 8
> # remove redundant rules
> rules.pruned <- rules.sorted[!redundant]
> inspect(rules.pruned)
03 אפריל 19
14

15

** The lhs of rule no.1 in the previous slide is contained in rule no. 2.
14
Visualizing Association Rules
> library(arulesViz)
> plot(rules)
03 אפריל 19
15

16
Visualizing Association Rules
> plot(rules, method="graph", control=list(type="items"))
03 אפריל 19
16

17
Visualizing Association Rules
> plot(rules, method="paracoord", control=list(reorder=TRUE))
03 אפריל 19
17

18
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-6---Python---Introduction\Lab6_Intoduction_Python-(1).pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Python Programming
INTRODUCTION
Python Language
 Python is a simple and minimalistic High-Level Language.
 Python supports procedure-oriented programming as well as object-oriented programming.
 Python is interpreted. The program is run directly from the source code. 
 Python converts the source code into an intermediate form called bytecodes and then translates this into the native language of your computer and then runs it.
 Python is developed under an OSI-approved open source license.
 The Python Package Index (PyPI) hosts thousands of third-party modules for Python.
 Both Python's standard library and the community-contributed modules allow for endless possibilities.
Data Types
Python has five standard data types −
 Numbers
 String
 List
 Tuple
 Dictionary
Note:
 Python variables do not need explicit declaration to reserve memory space.
 The declaration happens automatically when you assign a value to a variable. The equal sign (=) is used to assign values to variables.
 Python allows you to assign a single value to several variables simultaneously (a = b = c = 2)

Numeric Types 
Operation	Result
x + y	sum of x and y
x - y	difference of x and y
x * y	product of x and y
x / y	quotient of x and y
x // y	(floored) quotient of x and y
x % y	remainder of x / y
-x	x negated
+x	x unchanged
abs(x)	absolute value or magnitude of x
int(x)	x converted to integer
long(x)	x converted to long integer
float(x)	x converted to floating point
complex(re,im)	a complex number with real part re, imaginary part im. im defaults to zero.
c.conjugate()	conjugate of the complex number c
divmod(x, y)	the pair (x // y, x % y)
pow(x, y)	x to the power y
x ** y	x to the power y
https://docs.python.org/2.4/lib/typesnumeric.html
Python has four different numeric types :
 int (signed integers) - 100
 long (long integers) - 51924361L
 float (floating point real) – 3.23+e18
 complex (complex numbers) - 3.14j

Strings
 String is contiguous set of characters represented in the quotation marks ( “” or ‘’).
 Basic string operators:
 +  Concatenation ( “Hello” + “World”)
 *  Repeatition (“Hello”*2= “HelloHello”)
 [] Slice specific index (“Hello”[2]= ‘l’)
 [ : ] Slice Range (“Hello”[1:4] = “ello”)
str = "Hello World!"

print str          # Prints complete string
print str[0]       # Prints first character of the string
print str[2:5]     # Prints characters starting from 3rd to 5th
print str[2:]      # Prints string starting from 3rd character
print str * 2      # Prints string two times
print str + “Prog" # Prints concatenated string
https://www.tutorialspoint.com/python/python_strings.htm
Lists
 Items in a list need not be of the same type.
 Basic Operations:
Creating a list, comma-separated values.
Accessing Values in Lists using [ ] or [ : ] operators.
Updating Lists with append().
Delete List Elements
Built-in List functions (cmp, len, max, min, list)



list1 = ['a', 'b', 2000, 100, "Python"]
print "list1[0]: ", list1[0]        # list1[0]:  a
print "list1[1:2]: ", list1[1:2]    # list1[1:2]:  ['b']
list1[2] = 2001                     # updating the second value in list1
print list1[2]                      # prints 2001
del list1[2]                        # deletes the second item in list1
print list1                         # ['a', 'b', 100, 'Python']
print len(list1)                    # prints 4
list1.reverse()
print list1                         # ['Python', 100, 'b', 'a']
Tuples
 A tuple is a sequence of immutable Python objects, enclosed by parentheses.
 As in Lists, tuples can be manipulated similarly, using similar operators.



tup = (1, 2, 3, 4)
print "tup[0]: ", tup[0]            # tup[0]:  1
print "tup[1:2]: ", tup[1:2]        # tup[1:2]:  (2,)
tup1= ('a', 'b')
tup2= tup1+tup
print tup2                          # prints ('a', 'b', 1, 2, 3, 4)
print 3 in tup                      # prints True
print len(tup)
del tup                             # deletes the entire tuple
Dictionary
 The key is separated from its value by a colon (:), items are separated by commas, enclosed with {}.
 Basic Operations:
Creating a dictionary using {}
Accessing Values in Dictionary
Updating Dictionaries
Delete Dictionary Elements
Key constrains:
no duplicate key is allowed.
Keys must be immutable



dict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'}    # create a dictionary
dict['Age'] = 8                                        # update existing entry
dict['School'] = "DPS School“                          # Add new entry
print dict
dict = {'Name': 'Zara', 'Age': 7, 'Name': 'Manni'}
print "dict['Name']: ", dict['Name']                   # When duplicate keys encountered during     				    assignment, the last assignment wins
                                                       # prints "dict['Name']:  Manni"
print dict.keys()                                      # prints ['Age', 'Name']
dict.clear()                                           # remove all entries in dict
del dict                                               # delete entire dictionary
https://www.tutorialspoint.com/python/python_strings.htm
Functions
Python uses carriage returns to separate statements and a colon and indentation to separate code blocks. 
C++ and Java use semicolons to separate statements and curly braces to separate code blocks.
Python functions have no explicit begin or end, only delimiter is a colon (:) and the indentation of the code itself.
def changeme( mylist ):
    mylist.append([1,2,3,4])
    print "Values inside the function: ", mylist
    return

# Function definition is here
def printinfo1( name, age ):
   print "Name: ", name
   print "Age: ", age
   return

# Function definition is here
def printinfo2( *vartuple ):
    print "Output is: "
    for var in vartuple:
      print var
    return

printinfo1( age=50, name="miki" )   # prints Name:  miki, Age: 50
changeme(['a'])                     # Values inside the function:  ['a', [1, 2, 3, 4]]
printinfo2( 70, 60, 50 )            # Output is: 70, 60, 50
https://www.tutorialspoint.com/python/python_strings.htm
Simple GUI
 tkinter is a module in the Python standard library which serves as an interface to Tk, a simple toolkit. 
Anything that happens in a user interface is an event. We say that an event is fired whenever the user does something.
http://python-textbok.readthedocs.io/en/1.0/Introduction_to_GUI_Programming.html#custom-events
Simple GUI - Calculator
from Tkinter import Tk, Label, Button, Entry, IntVar, END, W, E

class Calculator:

    def __init__(self, master):
        self.master = master
        master.title("Calculator")

        self.total = 0
        self.entered_number = 0

        self.total_label_text = IntVar()
        self.total_label_text.set(self.total)
        self.total_label = Label(master, textvariable=self.total_label_text)

        self.label = Label(master, text="Total:")

        vcmd = master.register(self.validate) # we have to wrap the command
        self.entry = Entry(master, validate="key", validatecommand=(vcmd, '%P'))

        self.add_button = Button(master, text="+", command=lambda: self.update("add"))
        self.subtract_button = Button(master, text="-", command=lambda: self.update("subtract"))
        self.reset_button = Button(master, text="Reset", command=lambda: self.update("reset"))
        # LAYOUT
        self.label.grid(row=0, column=0, sticky=W)
        self.total_label.grid(row=0, column=1, columnspan=2, sticky=E)

        self.entry.grid(row=1, column=0, columnspan=3, sticky=W+E)

        self.add_button.grid(row=2, column=0)
        self.subtract_button.grid(row=2, column=1)
        self.reset_button.grid(row=2, column=2, sticky=W+E)


    def validate(self, new_text):
        if not new_text: # the field is being cleared
            self.entered_number = 0
            return True

        try:
            self.entered_number = int(new_text)
            return True
        except ValueError:
            return False

    def update(self, method):
        if method == "add":
            self.total += self.entered_number
        elif method == "subtract":
            self.total -= self.entered_number
        else:  # reset
            self.total = 0

        self.total_label_text.set(self.total)
        self.entry.delete(0, END)

root = Tk()
my_gui = Calculator(root)
root.mainloop()
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-7---Python---Pre-Processing1\Pre-Processing_Python.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Analysis in Python
PRE PROCESSING 1
Anaconda
 Python distribution for scientific data analysis tasks:
Developed by Continuum Analytics, Inc.
Cross-Platform, available on Windows, Linux, Mac OS X.
Includes many data analysis packages, such as NumPy, SciPy, Pandas, IPython and other.
Includes the Python development environment : Spyder
Includes the Python platform-independent package manager: Conda



Pandas
 Pandas is an open source project, providing high-performance, easy-to-use data structures and data analysis tools for Python.
 handling of missing data (represented as NaN)
 columns can be inserted and deleted from DataFrame
 slicing, fancy indexing, and subsetting of large data sets
 Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases
 Categorical variable analysis
 Predictive Modeling
 Plots


http://pandas.pydata.org/
Wikipedia


3
Data Exploration
 Import the required packages and libraries: pandas, numpy
 read the training data file (in csv format) using the read.csv(“path”)
 For the non-numerical values, use value_counts() for frequency analysis
Access particular column in data set using dfname[‘column_name’]
import pandas as pd
import numpy as np
import matplotlib as plt

df = pd.read_csv("C:\Users\user\Desktop\\train_Loan.csv")             # Reading the dataset in a dataframe using Pandas

print(df.head(10))                                               # Print the first 10 rows of the data set
print("\n Summary of numeircal variables:")
print(df.describe())                                             # Get summary of numerical variables
print("\n Frequency Distribution of Property_Area attribute:")
print(df['Property_Area'].value_counts())                        # Frequency distribution for non-numerical attributes
print("\n Frequency Distribution of Credit_History attribute:")
print(df['Credit_History'].value_counts())
https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/

Import pandas+numpy through PieCharm:
File -> Settings -> Project Interpreter -> “+” button -> search “pandas” -> install package

Import pandas+numpy through Terminal commands:
conda install pandas
conda install numpy

https://docs.anaconda.com/anaconda/navigator/tutorials/pandas/

4
Data Exploration
 Iterating over rows of a dataframe requires sometimes to know the column types.
 ‘object’ type is used for representing nominal variables in Pandas.
 Column can be modified to ‘object’ type using the astype(np.object) function


# print the data types of the attributes in the DataFrame
print(df.dtypes)
# Modify the Credit_History column to ‘object’ type
df['Credit_History'] = df['Credit_History'].astype(np.object)
print(df.dtypes)
Distribution analysis
 Plotting the histogram of some numeric attribute using hist(‘#number of bins’)
 Plotting boxplot using boxplot(column=‘name of column’, [by=‘name of column’])
df['ApplicantIncome'].hist(bins=50)
df.boxplot(column='ApplicantIncome')
df.boxplot(column='ApplicantIncome', by='Education')
Categorical variable analysis
 Generate pivot table on categorical analysis like in Excel using 
pivot_table(values, index, aggFunc)
Can be plotted using the “matplotlib” library.

temp1 = df['Credit_History'].value_counts(ascending=True)
temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())
print 'Frequency Table for Credit History:'
print temp1

print '\n Probability of getting loan for each Credit History class:'
print temp2
temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])
temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)
Pre-Processing : missing values
 Output number of NaN/ null values in data set using dt.apply(lambda x: sum(x.isnull()),axis=0) 
 Fill missing values in some attribute : fillna(‘new value’, inplace=True)
 Use mean value, or specific value, or build a supervised learning model 
 Use the mode of the column for filling missing values (mode(data[‘column Name’])
 Treat for extreme values in distribution using log or other math operators



print(df.apply(lambda x: sum(x.isnull()),axis=0))
df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)
df['Self_Employed'].fillna('No', inplace=True)

df['LoanAmount_log'] = np.log(df['LoanAmount'])
df['LoanAmount_log'].hist(bins=20)

df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']
df['TotalIncome_log'] = np.log(df['TotalIncome'])

df['TotalIncome_log'].hist(bins=20)
print(mode(df['Gender']).mode[0])
df['Gender'].fillna(mode(df['Gender']).mode[0], inplace=True)
Pre-Processing : Discretization
 Discretize continuous values into bins for further processing, using the cut function.
 Define the bins and cut point
 cut(DataFrame[‘Attribute name’], cut_points, labels)



def binning(col, cut_points, labels=None):
    # Define min and max values:
    minval = col.min()
    maxval = col.max()

    # create list by adding min and max to cut_points
    break_points = [minval] + cut_points + [maxval]

    # if no labels provided, use default labels 0 ... (n-1)
    if not labels:
        labels = range(len(cut_points)+1)

    # Binning using cut function of pandas
    colBin = pd.cut(col, bins=break_points, labels=labels, include_lowest=True)
    return colBin
Pre-Processing : Discretization
 Discretize continuous values into bins for further processing, using the cut function.
 Define the bins and cut point
 cut(DataFrame[‘Attribute name’], cut_points, labels)



# Define bins as 0<=x<100, 100<=x<200, 200<=x<300, x>=300
bins = [100, 200, 300]
group_names = ['Low', 'Medium', 'High', 'Extreme']
# Discretize the values in LoanAmount attribute
df["LoanAmount_Bin"] = binning(df["LoanAmount"], bins, group_names)

# Count the number of observations which each value
print pd.value_counts(df["LoanAmount_Bin"], sort=False)
print(df)
Pre-Processing : Outlier Detection
 Outlier is an observation that appears far away and diverges from an overall pattern
 Data Entry Errors
 Measurement Error
 Experimental Error
 Data Processing Error
 Natural Outlier

# Keep only the ones that are within +3 to -3 standard deviations in the column 'LoanAmount'.
print(df[(np.abs(df.LoanAmount-df.LoanAmount.mean()) <= (3*df.LoanAmount.std()))])
Indexing and Selecting Data
 Filter values of a column based on conditions from another set of columns.
 Use the loc(conditions, column to display)


# filter values of a column based on conditions from another set of columns
# for example, females who are not graduate and got a loan
print(df.loc[(df["Gender"] == "Female") & (df["Education"] == "Not Graduate") & (df["Loan_Status"] == "Y"),
             ["Gender", "Education", "Loan_Status"]])
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\מעבדה-8----Python---Pre-Processing2\Lab8_Pre-Processing2_Python.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Analysis in Python
PRE-PROCESSING 2
https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
Pre-Processing : Feature Engineering
 Feature Engineering can refer to:
 Variable transformation  -  replacement of a variable by a function. 
 Variable / Feature creation  - generating new variables / features based on existing ones.


 We want to transform our variable because:
We want to change their scale, without changing their distribution.
We want to transform complex non-linear relationships into linear relationships.
 We want to handle skewed distributions.


Variable transformation
 Methods for Variable Transformation:
 Logarithm
 Square / Cube root
 Binning
square_root = lambda x: np.sqrt(x)
df['ApplicantIncome']= df['ApplicantIncome'].apply(square_root)
# create a function called times100
def multiply_Loan_Amount(x):
    # that, if x is a string,
    if type(x) is str:
        # just returns it untouched
        return x
    # but, if not, return it multiplied by 1.5
    elif x:
        return 1.5 * x
    # and leave everything else
    else:
        return

df.applymap(multiply_Loan_Amount)
Variable / Feature creation
 Common methods for variable creation:
 Creating new variables
 Creating derived variables
 Creating dummy variables

Variable / Feature creation
 Creating new variables:
 Adding new column to existing data frame.
 Can be executed using the assign(‘New Column’= values) function
  Can contain lambda function inside its creation (e.g, lambda x: 42 if x > 1 else 55)

df['Random_Factor'] = np.random.randn(len(df))

newCol = np.log(df['ApplicantIncome'])
df = df.assign(ln_Income=newCol)

print(df)
Variable / Feature creation
 Creating derived variables:
 Manipulation of existing column values to create new variable
 For example, in Titanic dataset, create new ‘Salutation’ variable out of ‘Name’

# Create new variable salutation (Master, Mr, Miss, Mrs) from 'Name' variable
# create a function called multiply_Loan_Amount
def extract_salutation(x):
    # that, if x contains string, split the salutation after comma ','
    if type(x) is str:
        return x.split(',')[1].split('.')[0]
    # and leave everything else
    else:
        return
df1['Salutation'] = df1.apply(lambda row: extract_salutation(row['Name']), axis=1)
Variable / Feature creation
 Creating dummy variables:
 Create Indicator Variables, replacing one value by ‘0’ and the second by ‘1’.
 Dummy variables can also be created for more than two classes of a categorical variable.	
df_Gender = pd.get_dummies(df['Gender'])
df = df.join(df_Gender)
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-1\200878627_204736961.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מדעי הנתונים ובינה עסקית – מעבדה 1
1.  numeric
	Command:	class(iris$Sepal.Length)
2. 5X150
	Command:	length(iris);	length(iris$Sepal.Length)
3. setosa[50], versicolor[50], virginica[50]
	Command:	unique(iris$Species);	sum(iris$Species == "setosa");	sum(iris$Species == "versicolor");	sum(iris$Species == "virginica")
4. 

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-1\תרגיל_מעבדה-1.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
תרגיל מעבדה 1
טען את iris מתוך הספרייה dataset
מהו ה-class של הטור Sepal.Length? 
כמה שורות וכמה עמודות יש ב-iris?
אילו קטגוריות קיימות בטור Species? כמה פעמים מופיעה כל קטגוריה?
שמור את הטור Sepal.Length  בתור וקטור a ואת הטור Sepal.Width  בתור וקטור b, וקשור את שני הווקטורים כך שכל וקטור יהיה שורה במטריצה חדשה m. הצג את המטריצה.

הוראות הגשה
הגשה בזוגות בלבד.
עליכם להגיש קובץ word ובו תשובות לשאלות הנ"ל.
שם קובץ ה- word  אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.



בהצלחה!
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-2\200878627_204736961.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מדעי הנתונים ובינה עסקית – מעבדה 2
1. Commands: 
wisconsinOG<-read.csv("C:\\Users\\Dan\\Desktop\\wisconsin.csv")
wisconsin<-na.omit(wisconsinOG)
summary(Wisconsin$BareNuclei)
Results:




2. Commands: 
 length(wisconsin$BareNuclei)
Results:
682

3.
3.1 Commands: (chosen column: BareNuclei)
hist(wisconsin$BareNuclei)
Results:









3.2 + 3.3 . נבצע Equal-Width discretization:
nb = wisconsin$BareNuclei
intervalWidth = (max(nb)-min(nb))/3
cutPoints = c(0,0)
i=1
while (i <= length(cutPoints)){
    cutPoints[i] = min(nb) + i*intervalWidth
    i = i+1
}
i=1
for (x in nb){
    if (x <= cutPoints[1]){
        nb[i] = "LOW"
    }else if (x > cutPoints[1] & x <= cutPoints[2]){
        nb[i] = "MED"
    }else{
        nb[i] = "HIGH"
    }
    i = i+1
}
nb <- as.factor(nb)
wisconsin$BareNuclei = nb
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-2\Lab-2_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
תרגיל מעבדה 2
טען את הקובץ Wisconsin.csv המצורף
**הקובץ נלקח מהאתר: http://sci2s.ugr.es/keel/dataset.php?cod=73
היעזרו בקישור המופיע בהערות לשקף הראשון במצגת של מעבדה מספר 2 לטעינת המודל.
This dataset contains 699 cases from a study that was conducted at the University of Wisconsin Hospitals, Madison, about patients who had undergone surgery for breast cancer. The task is to determine if the detected tumor is benign (2) or malignant [4].
attribute ClumpThickness integer [1, 10]                                                                                         
attribute CellSize integer [1, 10]                                                                                               attribute CellShape integer [1, 10]                                                                                              attribute MarginalAdhesion integer [1, 10]                                                                                       attribute EpithelialSize integer [1, 10]                                                                                         attribute BareNuclei integer [1, 10]                                                                                             attribute BlandChromatin integer [1, 10]                                                                                         attribute NormalNucleoli integer [1, 10]                                                                                        attribute Mitoses integer [1, 10]                                                                                              attribute Class {2,4}  

שמרו את הטבלה במשתנה חדש כך שלא תכיל שורות עם ערכים חסרים, כתבו את הפקודות בהן השתמשתם. יש להציג נתונים סיכומיים של התכונה שעבורה אתם מתכוונים לבצע דיסקרטיזציה ועפ"י נתונים אלו לקבוע ערכי סף ולבצע את הסעיף הנוכחי.
כמה שורות נותרו בטבלה?
בחרו באחת העמודות בבסיס הנתונים לטובת המשימה הבאה:
הציגו את התפלגות הנתונים בעמודה בה בחרתם.
בחרו באופן מושכל גבולות לביצוע דיסקרטיזציה וציינו מהם הגבולות בהם השתמשתם?
בצעו דיסקרטיזציה ע"י חלוקה לשלושה אינטרוולים לפי בחירתכם.

הוראות הגשה
הגשה בזוגות בלבד.
עליכם להגיש קובץ word ובו תשובות לשאלות הנ"ל, הכוללות את הפקודות בהם השתמשתם ואת התוצאות שנתקבלו בכל סעיף.
שם קובץ ה- word  אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
רק אחד מבני הזוג יגיש את העבודה באתר הקורס.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.

בהצלחה!


 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-3\200878627_204736961.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מדעי הנתונים ובינה עסקית – מעבדה 3
1. Commands:
table <- read.csv("C:\\Users\\Dan\\Desktop\\lab.csv")
plot(x=table$X1, y= table$X2, xlab = "day", ylab = "n of cases", main = "Real Class", data = table, color=factor(table$Class,levels = c(0,1), labels = c("firstclass","secondclass")))
Results:

2. Commands:
newTable <- table
newTable <- subset(newTable, select = c("X1", "X2"))
Results:

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-3\Lab-3_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
תרגיל מעבדה 3

יש לטעון את הקובץ lab.csv
1. יש לצייר גרף לפי ההנחיות הבאות:
	ציר X הוא המשתנה X1, ציר Y הוא המשתנה X2
	צבע הנקודות נקבע ע"פ המשתנה Class
	כותרת הגרף תהיה Real class
	כותרת ציר X תהיה "day" , כותרת ציר Y תהיה "n of cases"
** שימו לב ש-Class הוא נומרי. כדי שהצבעים יוצגו יש לבקש מהפונק' Plot להתייחס לשדה זה כ-factor (אין צורך לשנות את סוג השדה ב-dataframe, אלא רק עבור יצירת התרשים).

2. יש לשמור את הטבלה בתור משתנה חדש שלא מכיל את המשתנה ה-Class.
 

עליך להגיש קובץ word ובו תשובות לשאלות הנ"ל 
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.

בהצלחה!



 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-4\200878627_204736961.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
מדעי הנתונים ובינה עסקית – מעבדה 4
1. Commands:
wisconsinFile <- read.csv("C:\\Users\\Dan\\Desktop\\wisconsin.csv")
wisconsin<-na.aggregate(wisconsinFile)
wisconsin$Class<-factor(wisconsin$Class)
2. Commands:
utils:::menuInstallPkgs() ##for caret and zoo
library(caret)
inTrain<-createDataPartition(y=wisconsin$Class,p=0.75 ,list=FALSE)
training<-wisconsin[inTrain,]
testing<-wisconsin[-inTrain,]
3. Commands:
NROW(training)
Results: 	
[1] 524
4. We chose Naïve Bayes: 
Commands:
nbFit<-train(Class~ ., data = training,method = "nb")
nbFit
Results:
Naive Bayes 

524 samples
  9 predictor
  2 classes: '2', '4' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 524, 524, 524, 524, 524, 524, ... 
Resampling results across tuning parameters:

  usekernel  Accuracy   Kappa    
  FALSE      0.9604024  0.9132298
   TRUE      0.9600419  0.9116284

Tuning parameter 'fL' was held constant at a value of 0
Tuning parameter 'adjust' was held constant at a value of 1
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were fL = 0, usekernel = FALSE and adjust = 1.




5. Commands:
NbPredict<-predict(nbFit, newdata=testing)
CM<-confusionMatrix(NbPredict, reference=testing$Class)
CM
Results:
Confusion Matrix and Statistics

          Reference
Prediction   2   4
         2 106   0
         4   8  60
                                          
               Accuracy : 0.954           
                 95% CI : (0.9114, 0.9799)
    No Information Rate : 0.6552          
    P-Value [Acc > NIR] : < 2e-16         
                                          
                  Kappa : 0.9014          
                                          
 Mcnemar's Test P-Value : 0.01333         
                                          
            Sensitivity : 0.9298          
            Specificity : 1.0000          
         Pos Pred Value : 1.0000          
         Neg Pred Value : 0.8824          
             Prevalence : 0.6552          
         Detection Rate : 0.6092          
   Detection Prevalence : 0.6092          
      Balanced Accuracy : 0.9649          
                                          
       'Positive' Class : 2     
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-4\Lab-4_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
תרגיל מעבדה 4
הערה: בסיס הנתונים מכיל ערכי NA .
חלק מאלגוריתמי הסיווג יצריכו הסרה של ערכים אלו. מומלץ להסיר NAs לפני שתתחילו בפתרון התרגיל (ניתן להשתמש בפונק' na.aggregate() מהספרייה zoo לביצוע פעולה זו. ראו מצגת מעבדה 2).
המירו את המשתנה Class לפקטור.
צרו טבלאות training ו-testing כך ש-p=0.75.
כמה שורות מכילה טבלת ה-training?
בחרו מודל מתאים מתוך הרשימה הבאה:
 https://topepo.github.io/caret/available-models.html  
והשתמשו בו על טבלת ה-training. באיזה מודל בחרתם? העתיקו את הפלט של הרצת המודל.
השתמשו במודל שבניתם על מנת לחזות את ה-Class עבור טבלת ה-testing. מהי רמת הדיוק שהתקבלה? צרו confusion matrix והעתיקו את הפלט.

הוראות הגשה
הגשה בזוגות בלבד.
עליכם להגיש קובץ word ובו תשובות לשאלות הנ"ל, הכוללות את הפקודות בהם השתמשתם ואת התוצאות שנתקבלו בכל סעיף.
שם קובץ ה- word  אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
רק אחד מבני הזוג יגיש את העבודה באתר הקורס.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.


בהצלחה!

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-5\Lab-5_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
תרגיל מעבדה 5

יש לטעון את הקובץ lab.csv
1. יש לשמור את הטבלה בתור משתנה חדש שלא מכיל את המשתנה ה-Class.
2. יש להשתמש ב-kmeans (k=2) על-מנת לחזות לאיזה קלסטר תשתייך כל רשומה.
מהם הגדלים של הקלסטרים שהתקבלו?
כמה רשומות שנכללו בקלסטר הלא נכון? (בהנחה שאנחנו יודעים את הסיווג הנכון). 
צרפו את וקטור ההשתייכות לקלסטר לטבלה המקורית (שלא מכילה את טור ה-Class) ושנו את שם העמודה שהתווספה ל-Cluster (היעזרו בגוגל כדי לגלות כיצד עושים זאת...).
יש לצייר גרף (הספריה ggplot2):
	ציר X הוא המשתנה X1, ציר Y הוא המשתנה X2
	צבע הנקודות נקבע ע"פ החלוקה לקלסטרים.
	כותרת הגרף תהיה Cluster.
	כותרת ציר X תהיה "day" , כותרת ציר Y תהיה "n of cases"
** שימו לב ש-Class הוא נומרי. כדי שהצבעים יוצגו יש לבקש מהפונק' Plot להתייחס לשדה זה כ-factor (אין צורך לשנות את סוג השדה ב-dataframe, אלא רק עבור יצירת התרשים).


הוראות הגשה
הגשה בזוגות בלבד.
עליכם להגיש קובץ word ובו תשובות לשאלות הנ"ל, הכוללות את הפקודות בהם השתמשתם ואת התוצאות שנתקבלו בכל סעיף.
שם קובץ ה- word  אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
רק אחד מבני הזוג יגיש את העבודה באתר הקורס.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.


בהצלחה!



 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-7\Lab-7_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>

תרגיל מעבדה 7

טען את הקובץ train_Loan.csv .
הדפס את התפלגות הערכים (Frequency Distribution) של המשתנים השונים בקובץ האימון.
הדפס את הטיפוס (Type) של כל אחד מהשתנים בקובץ האימון.
בצע תיקון Missing Values עבור המשתנים Gender, Married,  ו-Self_Employed.
בצע דיסקרטיזציה (מכל סוג) למשתנה LoanAmount. ציין את ה-bins השונים.
בצע Outlier Detection למשתנה LoanAmount, על פי 3 סטיות תקן בהתפלגות המשתנה.


הוראות הגשה
הגשה בזוגות בלבד.
עליך להגיש קובץ .py ובו כל הפעולות. יש לציין הערות בקוד המתארות את הפעולה.
שם הקובץ אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
רק אחד מבני הזוג יגיש את העבודה באתר הקורס.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.

 
בהצלחה! 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\מעבדות\תרגיל-מעבדה-8\Lab-8_exercise.docx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>

תרגיל מעבדה 8

טען את הקובץ את הקובץ train_Loan.csv .

הוסף עמודה חדשה בשם Normalized_Income לקובץ אימון אשר מיוצגת ע"י הנוסחה: 0.5*sqrt('Applicant_Income’).

צור Dummy Variable עבור המשתנה Education, עבורו הערך ‘Graduate’ יקבל את הקידוד ‘1’, והערך ‘Not Graduate’ יקבל את הקידוד ‘0’ . הוסף את תוצאות משתנה ה- Dummy Variable ל- Dataset  המכיל את נתוני הטבלה המקורית.



 
הוראות הגשה
הגשה בזוגות בלבד.
עליך להגיש קובץ .py ובו כל הפעולות. יש לציין הערות בקוד המתארות את הפעולה.
שם הקובץ אמור לכלול את מספרי תעודות הזהות של שתי המגישים וביניהם קו תחתון.
רק אחד מבני הזוג יגיש את העבודה באתר הקורס.
ניתן להגיש את התרגיל עד שבוע לאחר המעבדה בה פורסם לתיבת ההגשה הרלוונטית באתר הקורס.

 
בהצלחה! 

 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תכנה-וחומר-עזר\A-Mathematical-Theory-of-Communication\shannon1948.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Reprinted with corrections from The Bell System Technical Journal,
Vol. 27, pp. 379–423, 623–656, July, October, 1948.
A Mathematical Theory of Communication
By C. E. SHANNON
INTRODUCTION
T
HE recent development of various methods of modulation such as PCM and PPM which exchange
bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A
basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the
present paper we will extend the theory to include a number of new factors, in particular the effect of noise
in the channel, and the savings possible due to the statistical structure of the original message and due to the
nature of the final destination of the information.
The fundamental problem of communication is that of reproducing at one point either exactly or ap-
proximately a message selected at another point. Frequently the messages have meaning; that is they refer
to or are correlated according to some system with certain physical or conceptual entities. These semantic
aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual
message is one selected from a set of possible messages. The system must be designed to operate for each
possible selection, not just the one which will actually be chosen since this is unknown at the time of design.
If the number of messages in the set is finite then this number or any monotonic function of this number
can be regarded as a measure of the information produced when one message is chosen from the set, all
choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic
function. Although this definition must be generalized considerably when we consider the influence of the
statistics of the message and when we have a continuous range of messages, we will in all cases use an
essentially logarithmic measure.
The logarithmic measure is more convenient for various reasons:
1. It is practically more useful. Parameters of engineering importance such as time, bandwidth, number
of relays, etc., tend to vary linearly with the logarithm of the number of possibilities. For example,
adding one relay to a group doubles the number of possible states of the relays. It adds 1 to the base 2
logarithm of this number. Doubling the time roughly squares the number of possible messages, or
doubles the logarithm, etc.
2. It is nearer to our intuitive feeling as to the proper measure. This is closely related to (1) since we in-
tuitively measures entities by linear comparison with common standards. One feels, for example, that
two punched cards should have twice the capacity of one for information storage, and two identical
channels twice the capacity of one for transmitting information.
3. It is mathematically more suitable. Many of the limiting operations are simple in terms of the loga-
rithm but would require clumsy restatement in terms of the number of possibilities.
The choice of a logarithmic base corresponds to the choice of a unit for measuring information. If the
base 2 is used the resulting units may be called binary digits, or more briefly bits, a word suggested by
J. W. Tukey. A device with two stable positions, such as a relay or a flip-flop circuit, can store one bit of
information. N such devices can store N bits, since the total number of possible states is 2N and log2 2
N = N.
If the base 10 is used the units may be called decimal digits. Since
log2 M = log10 M= log10 2
= 3:32log10 M;
1Nyquist, H., “Certain Factors Affecting Telegraph Speed,” Bell System Technical Journal, April 1924, p. 324; “Certain Topics in
Telegraph Transmission Theory,” A.I.E.E. Trans., v. 47, April 1928, p. 617.
2Hartley, R. V. L., “Transmission of Information,” Bell System Technical Journal, July 1928, p. 535.
1
INFORMATION
SOURCE
MESSAGE
TRANSMITTER
SIGNAL RECEIVED
SIGNAL
RECEIVER
MESSAGE
DESTINATION
NOISE
SOURCE
Fig. 1—Schematic diagram of a general communication system.
a decimal digit is about 3 13 bits. A digit wheel on a desk computing machine has ten stable positions and
therefore has a storage capacity of one decimal digit. In analytical work where integration and differentiation
are involved the base e is sometimes useful. The resulting units of information will be called natural units.
Change from the base a to base b merely requires multiplication by logb a.
By a communication system we will mean a system of the type indicated schematically in Fig. 1. It
consists of essentially five parts:
1. An information source which produces a message or sequence of messages to be communicated to the
receiving terminal. The message may be of various types: (a) A sequence of letters as in a telegraph
of teletype system; (b) A single function of time f (t) as in radio or telephony; (c) A function of
time and other variables as in black and white television — here the message may be thought of as a
function f (x;y; t) of two space coordinates and time, the light intensity at point (x;y) and time t on a
pickup tube plate; (d) Two or more functions of time, say f (t), g(t), h(t) — this is the case in “three-
dimensional” sound transmission or if the system is intended to service several individual channels in
multiplex; (e) Several functions of several variables — in color television the message consists of three
functions f (x;y; t), g(x;y; t), h(x;y; t) defined in a three-dimensional continuum — we may also think
of these three functions as components of a vector field defined in the region — similarly, several
black and white television sources would produce “messages” consisting of a number of functions
of three variables; (f) Various combinations also occur, for example in television with an associated
audio channel.
2. A transmitter which operates on the message in some way to produce a signal suitable for trans-
mission over the channel. In telephony this operation consists merely of changing sound pressure
into a proportional electrical current. In telegraphy we have an encoding operation which produces
a sequence of dots, dashes and spaces on the channel corresponding to the message. In a multiplex
PCM system the different speech functions must be sampled, compressed, quantized and encoded,
and finally interleaved properly to construct the signal. Vocoder systems, television and frequency
modulation are other examples of complex operations applied to the message to obtain the signal.
3. The channel is merely the medium used to transmit the signal from transmitter to receiver. It may be
a pair of wires, a coaxial cable, a band of radio frequencies, a beam of light, etc.
4. The receiver ordinarily performs the inverse operation of that done by the transmitter, reconstructing
the message from the signal.
5. The destination is the person (or thing) for whom the message is intended.
We wish to consider certain general problems involving communication systems. To do this it is first
necessary to represent the various elements involved as mathematical entities, suitably idealized from their
2
physical counterparts. We may roughly classify communication systems into three main categories: discrete,
continuous and mixed. By a discrete system we will mean one in which both the message and the signal
are a sequence of discrete symbols. A typical case is telegraphy where the message is a sequence of letters
and the signal a sequence of dots, dashes and spaces. A continuous system is one in which the message and
signal are both treated as continuous functions, e.g., radio or television. A mixed system is one in which
both discrete and continuous variables appear, e.g., PCM transmission of speech.
We first consider the discrete case. This case has applications not only in communication theory, but
also in the theory of computing machines, the design of telephone exchanges and other fields. In addition
the discrete case forms a foundation for the continuous and mixed cases which will be treated in the second
half of the paper.
PART I: DISCRETE NOISELESS SYSTEMS
1. THE DISCRETE NOISELESS CHANNEL
Teletype and telegraphy are two simple examples of a discrete channel for transmitting information. Gen-
erally, a discrete channel will mean a system whereby a sequence of choices from a finite set of elementary
symbols S1; : : : ;Sn can be transmitted from one point to another. Each of the symbols Si is assumed to have
a certain duration in time ti seconds (not necessarily the same for different Si, for example the dots and
dashes in telegraphy). It is not required that all possible sequences of the Si be capable of transmission on
the system; certain sequences only may be allowed. These will be possible signals for the channel. Thus
in telegraphy suppose the symbols are: (1) A dot, consisting of line closure for a unit of time and then line
open for a unit of time; (2) A dash, consisting of three time units of closure and one unit open; (3) A letter
space consisting of, say, three units of line open; (4) A word space of six units of line open. We might place
the restriction on allowable sequences that no spaces follow each other (for if two letter spaces are adjacent,
it is identical with a word space). The question we now consider is how one can measure the capacity of
such a channel to transmit information.
In the teletype case where all symbols are of the same duration, and any sequence of the 32 symbols
is allowed the answer is easy. Each symbol represents five bits of information. If the system transmits n
symbols per second it is natural to say that the channel has a capacity of 5n bits per second. This does not
mean that the teletype channel will always be transmitting information at this rate — this is the maximum
possible rate and whether or not the actual rate reaches this maximum depends on the source of information
which feeds the channel, as will appear later.
In the more general case with different lengths of symbols and constraints on the allowed sequences, we
make the following definition:
Definition: The capacity C of a discrete channel is given by
C = Lim
T!∞
logN(T )
T
where N(T ) is the number of allowed signals of duration T .
It is easily seen that in the teletype case this reduces to the previous result. It can be shown that the limit
in question will exist as a finite number in most cases of interest. Suppose all sequences of the symbols
S1; : : : ;Sn are allowed and these symbols have durations t1; : : : ; tn. What is the channel capacity? If N(t)
represents the number of sequences of duration t we have
N(t) = N(t  t1)+N(t  t2)+   +N(t  tn):
The total number is equal to the sum of the numbers of sequences ending in S1;S2; : : : ;Sn and these are
N(t  t1);N(t  t2); : : : ;N(t  tn), respectively. According to a well-known result in finite differences, N(t)
is then asymptotic for large t to Xt0 where X0 is the largest real solution of the characteristic equation:
X t1 +X t2 +   +X tn = 1
3
and therefore
C = logX0:
In case there are restrictions on allowed sequences we may still often obtain a difference equation of this
type and find C from the characteristic equation. In the telegraphy case mentioned above
N(t) = N(t 2)+N(t 4)+N(t 5)+N(t 7)+N(t 8)+N(t 10)
as we see by counting sequences of symbols according to the last or next to the last symbol occurring.
Hence C is   log0 where 0 is the positive root of 1 = 2 +4 +5 +7 +8 +10. Solving this we find
C = 0:539.
A very general type of restriction which may be placed on allowed sequences is the following: We
imagine a number of possible states a1;a2; : : : ;am. For each state only certain symbols from the set S1; : : : ;Sn
can be transmitted (different subsets for the different states). When one of these has been transmitted the
state changes to a new state depending both on the old state and the particular symbol transmitted. The
telegraph case is a simple example of this. There are two states depending on whether or not a space was
the last symbol transmitted. If so, then only a dot or a dash can be sent next and the state always changes.
If not, any symbol can be transmitted and the state changes if a space is sent, otherwise it remains the same.
The conditions can be indicated in a linear graph as shown in Fig. 2. The junction points correspond to the
DASH
DOT
DASH
DOT
LETTER SPACE
WORD SPACE
Fig. 2—Graphical representation of the constraints on telegraph symbols.
states and the lines indicate the symbols possible in a state and the resulting state. In Appendix 1 it is shown
that if the conditions on allowed sequences can be described in this form C will exist and can be calculated
in accordance with the following result:
Theorem 1: Let b(s)i j be the duration of the s
th symbol which is allowable in state i and leads to state j.
Then the channel capacity C is equal to logW where W is the largest real root of the determinant equation:∑
s
W b
(s)
i j   i j
= 0
where i j = 1 if i = j and is zero otherwise.
For example, in the telegraph case (Fig. 2) the determinant is:  1 (W 2 +W 4)(W 3 +W 6) (W 2 +W 4 1)
= 0:
On expansion this leads to the equation given above for this case.
2. THE DISCRETE SOURCE OF INFORMATION
We have seen that under very general conditions the logarithm of the number of possible signals in a discrete
channel increases linearly with time. The capacity to transmit information can be specified by giving this
rate of increase, the number of bits per second required to specify the particular signal used.
We now consider the information source. How is an information source to be described mathematically,
and how much information in bits per second is produced in a given source? The main point at issue is the
effect of statistical knowledge about the source in reducing the required capacity of the channel, by the use
4
of proper encoding of the information. In telegraphy, for example, the messages to be transmitted consist of
sequences of letters. These sequences, however, are not completely random. In general, they form sentences
and have the statistical structure of, say, English. The letter E occurs more frequently than Q, the sequence
TH more frequently than XP, etc. The existence of this structure allows one to make a saving in time (or
channel capacity) by properly encoding the message sequences into signal sequences. This is already done
to a limited extent in telegraphy by using the shortest channel symbol, a dot, for the most common English
letter E; while the infrequent letters, Q, X, Z are represented by longer sequences of dots and dashes. This
idea is carried still further in certain commercial codes where common words and phrases are represented
by four- or five-letter code groups with a considerable saving in average time. The standardized greeting
and anniversary telegrams now in use extend this to the point of encoding a sentence or two into a relatively
short sequence of numbers.
We can think of a discrete source as generating the message, symbol by symbol. It will choose succes-
sive symbols according to certain probabilities depending, in general, on preceding choices as well as the
particular symbols in question. A physical system, or a mathematical model of a system which produces
such a sequence of symbols governed by a set of probabilities, is known as a stochastic process.3 We may
consider a discrete source, therefore, to be represented by a stochastic process. Conversely, any stochastic
process which produces a discrete sequence of symbols chosen from a finite set may be considered a discrete
source. This will include such cases as:
1. Natural written languages such as English, German, Chinese.
2. Continuous information sources that have been rendered discrete by some quantizing process. For
example, the quantized speech from a PCM transmitter, or a quantized television signal.
3. Mathematical cases where we merely define abstractly a stochastic process which generates a se-
quence of symbols. The following are examples of this last type of source.
(A) Suppose we have five letters A, B, C, D, E which are chosen each with probability .2, successive
choices being independent. This would lead to a sequence of which the following is a typical
example.
B D C B C E C C C A D C B D D A A E C E E A
A B B D A E E C A C E E B A E E C B C E A D.
This was constructed with the use of a table of random numbers.4
(B) Using the same five letters let the probabilities be .4, .1, .2, .2, .1, respectively, with successive
choices independent. A typical message from this source is then:
A A A C D C B D C E A A D A D A C E D A
E A D C A B E D A D D C E C A A A A A D.
(C) A more complicated structure is obtained if successive symbols are not chosen independently
but their probabilities depend on preceding letters. In the simplest case of this type a choice
depends only on the preceding letter and not on ones before that. The statistical structure can
then be described by a set of transition probabilities pi( j), the probability that letter i is followed
by letter j. The indices i and j range over all the possible symbols. A second equivalent way of
specifying the structure is to give the “digram” probabilities p(i; j), i.e., the relative frequency of
the digram i j. The letter frequencies p(i), (the probability of letter i), the transition probabilities
3See, for example, S. Chandrasekhar, “Stochastic Problems in Physics and Astronomy,” Reviews of Modern Physics, v. 15, No. 1,
January 1943, p. 1.
4Kendall and Smith, Tables of Random Sampling Numbers, Cambridge, 1939.
5
pi( j) and the digram probabilities p(i; j) are related by the following formulas:
p(i) = ∑
j
p(i; j) = ∑
j
p( j; i) = ∑
j
p( j)p j(i)
p(i; j) = p(i)pi( j)
∑
j
pi( j) = ∑
i
p(i) = ∑
i; j
p(i; j) = 1:
As a specific example suppose there are three letters A, B, C with the probability tables:
pi( j) j
A B C
A 0 45
1
5
i B 12
1
2 0
C 12
2
5
1
10
i p(i)
A 927
B 1627
C 227
p(i; j) j
A B C
A 0 415
1
15
i B 827
8
27 0
C 127
4
135
1
135
A typical message from this source is the following:
A B B A B A B A B A B A B A B B B A B B B B B A B A B A B A B A B B B A C A C A B
B A B B B B A B B A B A C B B B A B A.
The next increase in complexity would involve trigram frequencies but no more. The choice of
a letter would depend on the preceding two letters but not on the message before that point. A
set of trigram frequencies p(i; j;k) or equivalently a set of transition probabilities pi j(k) would
be required. Continuing in this way one obtains successively more complicated stochastic pro-
cesses. In the general n-gram case a set of n-gram probabilities p(i1; i2; : : : ; in) or of transition
probabilities pi1;i2;:::;in 1(in) is required to specify the statistical structure.
(D) Stochastic processes can also be defined which produce a text consisting of a sequence of
“words.” Suppose there are five letters A, B, C, D, E and 16 “words” in the language with
associated probabilities:
.10 A .16 BEBE .11 CABED .04 DEB
.04 ADEB .04 BED .05 CEED .15 DEED
.05 ADEE .02 BEED .08 DAB .01 EAB
.01 BADD .05 CA .04 DAD .05 EE
Suppose successive “words” are chosen independently and are separated by a space. A typical
message might be:
DAB EE A BEBE DEED DEB ADEE ADEE EE DEB BEBE BEBE BEBE ADEE BED DEED
DEED CEED ADEE A DEED DEED BEBE CABED BEBE BED DAB DEED ADEB.
If all the words are of finite length this process is equivalent to one of the preceding type, but
the description may be simpler in terms of the word structure and probabilities. We may also
generalize here and introduce transition probabilities between words, etc.
These artificial languages are useful in constructing simple problems and examples to illustrate vari-
ous possibilities. We can also approximate to a natural language by means of a series of simple artificial
languages. The zero-order approximation is obtained by choosing all letters with the same probability and
independently. The first-order approximation is obtained by choosing successive letters independently but
each letter having the same probability that it has in the natural language.5 Thus, in the first-order ap-
proximation to English, E is chosen with probability .12 (its frequency in normal English) and W with
probability .02, but there is no influence between adjacent letters and no tendency to form the preferred
5Letter, digram and trigram frequencies are given in Secret and Urgent by Fletcher Pratt, Blue Ribbon Books, 1939. Word frequen-
cies are tabulated in Relative Frequency of English Speech Sounds, G. Dewey, Harvard University Press, 1923.
6
digrams such as TH, ED, etc. In the second-order approximation, digram structure is introduced. After a
letter is chosen, the next one is chosen in accordance with the frequencies with which the various letters
follow the first one. This requires a table of digram frequencies pi( j). In the third-order approximation,
trigram structure is introduced. Each letter is chosen with probabilities which depend on the preceding two
letters.
3. THE SERIES OF APPROXIMATIONS TO ENGLISH
To give a visual idea of how this series of processes approaches a language, typical sequences in the approx-
imations to English have been constructed and are given below. In all cases we have assumed a 27-symbol
“alphabet,” the 26 letters and a space.
1. Zero-order approximation (symbols independent and equiprobable).
XFOML RXKHRJFFJUJ ZLPWCFWKCYJ FFJEYVKCQSGHYD QPAAMKBZAACIBZL-
HJQD.
2. First-order approximation (symbols independent but with frequencies of English text).
OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA
NAH BRL.
3. Second-order approximation (digram structure as in English).
ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY ACHIN D ILONASIVE TU-
COOWE AT TEASONARE FUSO TIZIN ANDY TOBE SEACE CTISBE.
4. Third-order approximation (trigram structure as in English).
IN NO IST LAT WHEY CRATICT FROURE BIRS GROCID PONDENOME OF DEMONS-
TURES OF THE REPTAGIN IS REGOACTIONA OF CRE.
5. First-order word approximation. Rather than continue with tetragram, : : : , n-gram structure it is easier
and better to jump at this point to word units. Here words are chosen independently but with their
appropriate frequencies.
REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN DIFFERENT NAT-
URAL HERE HE THE A IN CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES
THE LINE MESSAGE HAD BE THESE.
6. Second-order word approximation. The word transition probabilities are correct but no further struc-
ture is included.
THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHAR-
ACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT
THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.
The resemblance to ordinary English text increases quite noticeably at each of the above steps. Note that
these samples have reasonably good structure out to about twice the range that is taken into account in their
construction. Thus in (3) the statistical process insures reasonable text for two-letter sequences, but four-
letter sequences from the sample can usually be fitted into good sentences. In (6) sequences of four or more
words can easily be placed in sentences without unusual or strained constructions. The particular sequence
of ten words “attack on an English writer that the character of this” is not at all unreasonable. It appears then
that a sufficiently complex stochastic process will give a satisfactory representation of a discrete source.
The first two samples were constructed by the use of a book of random numbers in conjunction with
(for example 2) a table of letter frequencies. This method might have been continued for (3), (4) and (5),
since digram, trigram and word frequency tables are available, but a simpler equivalent method was used.
7
To construct (3) for example, one opens a book at random and selects a letter at random on the page. This
letter is recorded. The book is then opened to another page and one reads until this letter is encountered.
The succeeding letter is then recorded. Turning to another page this second letter is searched for and the
succeeding letter recorded, etc. A similar process was used for (4), (5) and (6). It would be interesting if
further approximations could be constructed, but the labor involved becomes enormous at the next stage.
4. GRAPHICAL REPRESENTATION OF A MARKOFF PROCESS
Stochastic processes of the type described above are known mathematically as discrete Markoff processes
and have been extensively studied in the literature.6 The general case can be described as follows: There
exist a finite number of possible “states” of a system; S1;S2; : : : ;Sn. In addition there is a set of transition
probabilities; pi( j) the probability that if the system is in state Si it will next go to state S j. To make this
Markoff process into an information source we need only assume that a letter is produced for each transition
from one state to another. The states will correspond to the “residue of influence” from preceding letters.
The situation can be represented graphically as shown in Figs. 3, 4 and 5. The “states” are the junction
A
B
C
D
E
.1
.1
.2
.2
.4
Fig. 3—A graph corresponding to the source in example B.
points in the graph and the probabilities and letters produced for a transition are given beside the correspond-
ing line. Figure 3 is for the example B in Section 2, while Fig. 4 corresponds to the example C. In Fig. 3
A
A
B
B
BC
C
.1
.5 .5
.5
.2
.8
.4
Fig. 4—A graph corresponding to the source in example C.
there is only one state since successive letters are independent. In Fig. 4 there are as many states as letters.
If a trigram example were constructed there would be at most n2 states corresponding to the possible pairs
of letters preceding the one being chosen. Figure 5 is a graph for the case of word structure in example D.
Here S corresponds to the “space” symbol.
5. ERGODIC AND MIXED SOURCES
As we have indicated above a discrete source for our purposes can be considered to be represented by a
Markoff process. Among the possible discrete Markoff processes there is a group with special properties
of significance in communication theory. This special class consists of the “ergodic” processes and we
shall call the corresponding sources ergodic sources. Although a rigorous definition of an ergodic process is
somewhat involved, the general idea is simple. In an ergodic process every sequence produced by the process
6For a detailed treatment see M. Fréchet, Méthode des fonctions arbitraires. Théorie des événements en chaı̂ne dans le cas d’un
nombre fini d’états possibles. Paris, Gauthier-Villars, 1938.
8
is the same in statistical properties. Thus the letter frequencies, digram frequencies, etc., obtained from
particular sequences, will, as the lengths of the sequences increase, approach definite limits independent
of the particular sequence. Actually this is not true of every sequence but the set for which it is false has
probability zero. Roughly the ergodic property means statistical homogeneity.
All the examples of artificial languages given above are ergodic. This property is related to the structure
of the corresponding graph. If the graph has the following two properties7 the corresponding process will
be ergodic:
1. The graph does not consist of two isolated parts A and B such that it is impossible to go from junction
points in part A to junction points in part B along lines of the graph in the direction of arrows and also
impossible to go from junctions in part B to junctions in part A.
2. A closed series of lines in the graph with all arrows on the lines pointing in the same orientation will
be called a “circuit.” The “length” of a circuit is the number of lines in it. Thus in Fig. 5 series BEBES
is a circuit of length 5. The second property required is that the greatest common divisor of the lengths
of all circuits in the graph be one.
S
S
S
A
A
A
A
A
B
B
B
B
B
B B
C
D
D
D
D
D
D
E
E
E
E
E
E
E
E
E
E
E
Fig. 5—A graph corresponding to the source in example D.
If the first condition is satisfied but the second one violated by having the greatest common divisor equal
to d > 1, the sequences have a certain type of periodic structure. The various sequences fall into d different
classes which are statistically the same apart from a shift of the origin (i.e., which letter in the sequence is
called letter 1). By a shift of from 0 up to d  1 any sequence can be made statistically equivalent to any
other. A simple example with d = 2 is the following: There are three possible letters a;b;c. Letter a is
followed with either b or c with probabilities 13 and
2
3 respectively. Either b or c is always followed by letter
a. Thus a typical sequence is
a b a c a c a c a b a c a b a b a c a c:
This type of situation is not of much importance for our work.
If the first condition is violated the graph may be separated into a set of subgraphs each of which satisfies
the first condition. We will assume that the second condition is also satisfied for each subgraph. We have in
this case what may be called a “mixed” source made up of a number of pure components. The components
correspond to the various subgraphs. If L1, L2, L3; : : : are the component sources we may write
L = p1L1 + p2L2 + p3L3 +   
7These are restatements in terms of the graph of conditions given in Fréchet.
9
where pi is the probability of the component source Li.
Physically the situation represented is this: There are several different sources L1, L2, L3; : : : which are
each of homogeneous statistical structure (i.e., they are ergodic). We do not know a priori which is to be
used, but once the sequence starts in a given pure component Li, it continues indefinitely according to the
statistical structure of that component.
As an example one may take two of the processes defined above and assume p1 = :2 and p2 = :8. A
sequence from the mixed source
L = :2L1 + :8L2
would be obtained by choosing first L1 or L2 with probabilities .2 and .8 and after this choice generating a
sequence from whichever was chosen.
Except when the contrary is stated we shall assume a source to be ergodic. This assumption enables one
to identify averages along a sequence with averages over the ensemble of possible sequences (the probability
of a discrepancy being zero). For example the relative frequency of the letter A in a particular infinite
sequence will be, with probability one, equal to its relative frequency in the ensemble of sequences.
If Pi is the probability of state i and pi( j) the transition probability to state j, then for the process to be
stationary it is clear that the Pi must satisfy equilibrium conditions:
Pj = ∑
i
Pi pi( j):
In the ergodic case it can be shown that with any starting conditions the probabilities Pj(N) of being in state
j after N symbols, approach the equilibrium values as N ! ∞.
6. CHOICE, UNCERTAINTY AND ENTROPY
We have represented a discrete information source as a Markoff process. Can we define a quantity which
will measure, in some sense, how much information is “produced” by such a process, or better, at what rate
information is produced?
Suppose we have a set of possible events whose probabilities of occurrence are p1; p2; : : : ; pn. These
probabilities are known but that is all we know concerning which event will occur. Can we find a measure
of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?
If there is such a measure, say H(p1; p2; : : : ; pn), it is reasonable to require of it the following properties:
1. H should be continuous in the pi.
2. If all the pi are equal, pi = 1n , then H should be a monotonic increasing function of n. With equally
likely events there is more choice, or uncertainty, when there are more possible events.
3. If a choice be broken down into two successive choices, the original H should be the weighted sum
of the individual values of H. The meaning of this is illustrated in Fig. 6. At the left we have three
1/2
1/3
1/6
1/2
1/2
2/3
1/3
1/2
1/3
1/6
Fig. 6—Decomposition of a choice from three possibilities.
possibilities p1 = 12 , p2 =
1
3 , p3 =
1
6 . On the right we first choose between two possibilities each with
probability 12 , and if the second occurs make another choice with probabilities
2
3 ,
1
3 . The final results
have the same probabilities as before. We require, in this special case, that
H( 12 ;
1
3 ;
1
6 ) = H(
1
2 ;
1
2 )+
1
2 H(
2
3 ;
1
3 ):
The coefficient 12 is because this second choice only occurs half the time.
10
In Appendix 2, the following result is established:
Theorem 2: The only H satisfying the three above assumptions is of the form:
H = K
n
∑
i=1
pi log pi
where K is a positive constant.
This theorem, and the assumptions required for its proof, are in no way necessary for the present theory.
It is given chiefly to lend a certain plausibility to some of our later definitions. The real justification of these
definitions, however, will reside in their implications.
Quantities of the form H= ∑ pi log pi (the constant K merely amounts to a choice of a unit of measure)
play a central role in information theory as measures of information, choice and uncertainty. The form of H
will be recognized as that of entropy as defined in certain formulations of statistical mechanics8 where pi is
the probability of a system being in cell i of its phase space. H is then, for example, the H in Boltzmann’s
famous H theorem. We shall call H = ∑ pi log pi the entropy of the set of probabilities p1; : : : ; pn. If x is a
chance variable we will write H(x) for its entropy; thus x is not an argument of a function but a label for a
number, to differentiate it from H(y) say, the entropy of the chance variable y.
The entropy in the case of two possibilities with probabilities p and q = 1  p, namely
H = (p log p+q logq)
is plotted in Fig. 7 as a function of p.
H
BITS
p
0
.1
.2
.3
.4
.5
.6
.7
.8
.9
1.0
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1.0
Fig. 7—Entropy in the case of two possibilities with probabilities p and (1  p).
The quantity H has a number of interesting properties which further substantiate it as a reasonable
measure of choice or information.
1. H = 0 if and only if all the pi but one are zero, this one having the value unity. Thus only when we
are certain of the outcome does H vanish. Otherwise H is positive.
2. For a given n, H is a maximum and equal to logn when all the pi are equal (i.e., 1n ). This is also
intuitively the most uncertain situation.
8See, for example, R. C. Tolman, Principles of Statistical Mechanics, Oxford, Clarendon, 1938.
11
3. Suppose there are two events, x and y, in question with m possibilities for the first and n for the second.
Let p(i; j) be the probability of the joint occurrence of i for the first and j for the second. The entropy of the
joint event is
H(x;y) = ∑
i; j
p(i; j) log p(i; j)
while
H(x) = ∑
i; j
p(i; j) log∑
j
p(i; j)
H(y) = ∑
i; j
p(i; j) log∑
i
p(i; j):
It is easily shown that
H(x;y)H(x)+H(y)
with equality only if the events are independent (i.e., p(i; j) = p(i)p( j)). The uncertainty of a joint event is
less than or equal to the sum of the individual uncertainties.
4. Any change toward equalization of the probabilities p1; p2; : : : ; pn increases H. Thus if p1 < p2 and
we increase p1, decreasing p2 an equal amount so that p1 and p2 are more nearly equal, then H increases.
More generally, if we perform any “averaging” operation on the pi of the form
p0i = ∑
j
ai j p j
where ∑i ai j = ∑ j ai j = 1, and all ai j  0, then H increases (except in the special case where this transfor-
mation amounts to no more than a permutation of the p j with H of course remaining the same).
5. Suppose there are two chance events x and y as in 3, not necessarily independent. For any particular
value i that x can assume there is a conditional probability pi( j) that y has the value j. This is given by
pi( j) =
p(i; j)
∑ j p(i; j)
:
We define the conditional entropy of y, Hx(y) as the average of the entropy of y for each value of x, weighted
according to the probability of getting that particular x. That is
Hx(y) = ∑
i; j
p(i; j) log pi( j) :
This quantity measures how uncertain we are of y on the average when we know x. Substituting the value of
pi( j) we obtain
Hx(y) = ∑
i; j
p(i; j) log p(i; j)+∑
i; j
p(i; j) log∑
j
p(i; j)
= H(x;y) H(x)
or
H(x;y) = H(x)+Hx(y):
The uncertainty (or entropy) of the joint event x;y is the uncertainty of x plus the uncertainty of y when x is
known.
6. From 3 and 5 we have
H(x)+H(y)H(x;y) = H(x)+Hx(y):
Hence
H(y)Hx(y):
The uncertainty of y is never increased by knowledge of x. It will be decreased unless x and y are independent
events, in which case it is not changed.
12
7. THE ENTROPY OF AN INFORMATION SOURCE
Consider a discrete source of the finite state type considered above. For each possible state i there will be a
set of probabilities pi( j) of producing the various possible symbols j. Thus there is an entropy Hi for each
state. The entropy of the source will be defined as the average of these Hi weighted in accordance with the
probability of occurrence of the states in question:
H = ∑
i
PiHi
= ∑
i; j
Pi pi( j) log pi( j) :
This is the entropy of the source per symbol of text. If the Markoff process is proceeding at a definite time
rate there is also an entropy per second
H 0 = ∑
i
fiHi
where fi is the average frequency (occurrences per second) of state i. Clearly
H 0 = mH
where m is the average number of symbols produced per second. H or H 0 measures the amount of informa-
tion generated by the source per symbol or per second. If the logarithmic base is 2, they will represent bits
per symbol or per second.
If successive symbols are independent then H is simply ∑ pi log pi where pi is the probability of sym-
bol i. Suppose in this case we consider a long message of N symbols. It will contain with high probability
about p1N occurrences of the first symbol, p2N occurrences of the second, etc. Hence the probability of this
particular message will be roughly
p = pp1N1 p
p2N
2    ppnNn
or
log p
:
= N ∑
i
pi log pi
log p
:
= NH
H
:
=
log1=p
N
:
H is thus approximately the logarithm of the reciprocal probability of a typical long sequence divided by the
number of symbols in the sequence. The same result holds for any source. Stated more precisely we have
(see Appendix 3):
Theorem 3: Given any  > 0 and  > 0, we can find an N0 such that the sequences of any length N  N0
fall into two classes:
1. A set whose total probability is less than .
2. The remainder, all of whose members have probabilities satisfying the inequality log p 1N  H
< :
In other words we are almost certain to have
log p 1
N
very close to H when N is large.
A closely related result deals with the number of sequences of various probabilities. Consider again the
sequences of length N and let them be arranged in order of decreasing probability. We define n(q) to be
the number we must take from this set starting with the most probable one in order to accumulate a total
probability q for those taken.
13
Theorem 4:
Lim
N!∞
logn(q)
N
= H
when q does not equal 0 or 1.
We may interpret logn(q) as the number of bits required to specify the sequence when we consider only
the most probable sequences with a total probability q. Then
logn(q)
N
is the number of bits per symbol for
the specification. The theorem says that for large N this will be independent of q and equal to H. The rate
of growth of the logarithm of the number of reasonably probable sequences is given by H, regardless of our
interpretation of “reasonably probable.” Due to these results, which are proved in Appendix 3, it is possible
for most purposes to treat the long sequences as though there were just 2HN of them, each with a probability
2 HN .
The next two theorems show that H and H 0 can be determined by limiting operations directly from
the statistics of the message sequences, without reference to the states and transition probabilities between
states.
Theorem 5: Let p(Bi) be the probability of a sequence Bi of symbols from the source. Let
GN =  1N ∑i
p(Bi) log p(Bi)
where the sum is over all sequences Bi containing N symbols. Then GN is a monotonic decreasing function
of N and
Lim
N!∞
GN = H:
Theorem 6: Let p(Bi;S j) be the probability of sequence Bi followed by symbol S j and pBi(S j) =
p(Bi;S j)=p(Bi) be the conditional probability of S j after Bi. Let
FN = ∑
i; j
p(Bi;S j) log pBi(S j)
where the sum is over all blocks Bi of N  1 symbols and over all symbols S j. Then FN is a monotonic
decreasing function of N,
FN = NGN   (N 1)GN 1;
GN =
1
N
N
∑
n=1
Fn;
FN GN ;
and LimN!∞ FN = H.
These results are derived in Appendix 3. They show that a series of approximations to H can be obtained
by considering only the statistical structure of the sequences extending over 1;2; : : : ;N symbols. FN is the
better approximation. In fact FN is the entropy of the Nth order approximation to the source of the type
discussed above. If there are no statistical influences extending over more than N symbols, that is if the
conditional probability of the next symbol knowing the preceding (N 1) is not changed by a knowledge of
any before that, then FN = H. FN of course is the conditional entropy of the next symbol when the (N 1)
preceding ones are known, while GN is the entropy per symbol of blocks of N symbols.
The ratio of the entropy of a source to the maximum value it could have while still restricted to the same
symbols will be called its relative entropy. This is the maximum compression possible when we encode into
the same alphabet. One minus the relative entropy is the redundancy. The redundancy of ordinary English,
not considering statistical structure over greater distances than about eight letters, is roughly 50%. This
means that when we write English half of what we write is determined by the structure of the language and
half is chosen freely. The figure 50% was found by several independent methods which all gave results in
14
this neighborhood. One is by calculation of the entropy of the approximations to English. A second method
is to delete a certain fraction of the letters from a sample of English text and then let someone attempt to
restore them. If they can be restored when 50% are deleted the redundancy must be greater than 50%. A
third method depends on certain known results in cryptography.
Two extremes of redundancy in English prose are represented by Basic English and by James Joyce’s
book “Finnegans Wake”. The Basic English vocabulary is limited to 850 words and the redundancy is very
high. This is reflected in the expansion that occurs when a passage is translated into Basic English. Joyce
on the other hand enlarges the vocabulary and is alleged to achieve a compression of semantic content.
The redundancy of a language is related to the existence of crossword puzzles. If the redundancy is
zero any sequence of letters is a reasonable text in the language and any two-dimensional array of letters
forms a crossword puzzle. If the redundancy is too high the language imposes too many constraints for large
crossword puzzles to be possible. A more detailed analysis shows that if we assume the constraints imposed
by the language are of a rather chaotic and random nature, large crossword puzzles are just possible when
the redundancy is 50%. If the redundancy is 33%, three-dimensional crossword puzzles should be possible,
etc.
8. REPRESENTATION OF THE ENCODING AND DECODING OPERATIONS
We have yet to represent mathematically the operations performed by the transmitter and receiver in en-
coding and decoding the information. Either of these will be called a discrete transducer. The input to the
transducer is a sequence of input symbols and its output a sequence of output symbols. The transducer may
have an internal memory so that its output depends not only on the present input symbol but also on the past
history. We assume that the internal memory is finite, i.e., there exist a finite number m of possible states of
the transducer and that its output is a function of the present state and the present input symbol. The next
state will be a second function of these two quantities. Thus a transducer can be described by two functions:
yn = f (xn;n)
n+1 = g(xn;n)
where
xn is the nth input symbol,
n is the state of the transducer when the nth input symbol is introduced,
yn is the output symbol (or sequence of output symbols) produced when xn is introduced if the state is n.
If the output symbols of one transducer can be identified with the input symbols of a second, they can be
connected in tandem and the result is also a transducer. If there exists a second transducer which operates
on the output of the first and recovers the original input, the first transducer will be called non-singular and
the second will be called its inverse.
Theorem 7: The output of a finite state transducer driven by a finite state statistical source is a finite
state statistical source, with entropy (per unit time) less than or equal to that of the input. If the transducer
is non-singular they are equal.
Let  represent the state of the source, which produces a sequence of symbols xi; and let  be the state of
the transducer, which produces, in its output, blocks of symbols y j. The combined system can be represented
by the “product state space” of pairs (;). Two points in the space (1;1) and (2;2), are connected by
a line if 1 can produce an x which changes 1 to 2, and this line is given the probability of that x in this
case. The line is labeled with the block of y j symbols produced by the transducer. The entropy of the output
can be calculated as the weighted sum over the states. If we sum first on  each resulting term is less than or
equal to the corresponding term for , hence the entropy is not increased. If the transducer is non-singular
let its output be connected to the inverse transducer. If H 01, H
0
2 and H
0
3 are the output entropies of the source,
the first and second transducers respectively, then H 01 H 02 H 03 = H 01 and therefore H 01 = H 02.
15
Suppose we have a system of constraints on possible sequences of the type which can be represented by
a linear graph as in Fig. 2. If probabilities p(s)i j were assigned to the various lines connecting state i to state j
this would become a source. There is one particular assignment which maximizes the resulting entropy (see
Appendix 4).
Theorem 8: Let the system of constraints considered as a channel have a capacity C = logW . If we
assign
p(s)i j =
B j
Bi
W `
(s)
i j
where `(s)i j is the duration of the s
th symbol leading from state i to state j and the Bi satisfy
Bi = ∑
s; j
B jW
 `
(s)
i j
then H is maximized and equal to C.
By proper assignment of the transition probabilities the entropy of symbols on a channel can be maxi-
mized at the channel capacity.
9. THE FUNDAMENTAL THEOREM FOR A NOISELESS CHANNEL
We will now justify our interpretation of H as the rate of generating information by proving that H deter-
mines the channel capacity required with most efficient coding.
Theorem 9: Let a source have entropy H (bits per symbol) and a channel have a capacity C (bits per
second). Then it is possible to encode the output of the source in such a way as to transmit at the average
rate
C
H
   symbols per second over the channel where  is arbitrarily small. It is not possible to transmit at
an average rate greater than
C
H
.
The converse part of the theorem, that
C
H
cannot be exceeded, may be proved by noting that the entropy
of the channel input per second is equal to that of the source, since the transmitter must be non-singular, and
also this entropy cannot exceed the channel capacity. Hence H 0 C and the number of symbols per second
= H 0=H C=H.
The first part of the theorem will be proved in two different ways. The first method is to consider the
set of all sequences of N symbols produced by the source. For N large we can divide these into two groups,
one containing less than 2(H+)N members and the second containing less than 2RN members (where R is
the logarithm of the number of different symbols) and having a total probability less than . As N increases
 and  approach zero. The number of signals of duration T in the channel is greater than 2(C )T with 
small when T is large. if we choose
T =

H
C
+

N
then there will be a sufficient number of sequences of channel symbols for the high probability group when
N and T are sufficiently large (however small ) and also some additional ones. The high probability group
is coded in an arbitrary one-to-one way into this set. The remaining sequences are represented by larger
sequences, starting and ending with one of the sequences not used for the high probability group. This
special sequence acts as a start and stop signal for a different code. In between a sufficient time is allowed
to give enough different sequences for all the low probability messages. This will require
T1 =

R
C
+'

N
where ' is small. The mean rate of transmission in message symbols per second will then be greater than
(1  )T
N
+ 
T1
N
# 1
=

(1  )
H
C
+

+ 
R
C
+'
 1
:
16
As N increases ,  and ' approach zero and the rate approaches
C
H
.
Another method of performing this coding and thereby proving the theorem can be described as follows:
Arrange the messages of length N in order of decreasing probability and suppose their probabilities are
p1  p2  p3     pn. Let Ps = ∑s 11 pi; that is Ps is the cumulative probability up to, but not including, ps.
We first encode into a binary system. The binary code for message s is obtained by expanding Ps as a binary
number. The expansion is carried out to ms places, where ms is the integer satisfying:
log2
1
ps
ms < 1+ log2
1
ps
:
Thus the messages of high probability are represented by short codes and those of low probability by long
codes. From these inequalities we have
1
2ms
 ps < 12ms 1 :
The code for Ps will differ from all succeeding ones in one or more of its ms places, since all the remaining
Pi are at least 12ms larger and their binary expansions therefore differ in the first ms places. Consequently all
the codes are different and it is possible to recover the message from its code. If the channel sequences are
not already sequences of binary digits, they can be ascribed binary numbers in an arbitrary fashion and the
binary code thus translated into signals suitable for the channel.
The average number H 0 of binary digits used per symbol of original message is easily estimated. We
have
H 0 =
1
N ∑ms ps:
But,
1
N ∑

log2
1
ps

ps  1N ∑ms ps <
1
N ∑

1+ log2
1
ps

ps
and therefore,
GN H 0 < GN + 1N
As N increases GN approaches H, the entropy of the source and H 0 approaches H.
We see from this that the inefficiency in coding, when only a finite delay of N symbols is used, need
not be greater than 1N plus the difference between the true entropy H and the entropy GN calculated for
sequences of length N. The per cent excess time needed over the ideal is therefore less than
GN
H
+
1
HN
 1:
This method of encoding is substantially the same as one found independently by R. M. Fano.9 His
method is to arrange the messages of length N in order of decreasing probability. Divide this series into two
groups of as nearly equal probability as possible. If the message is in the first group its first binary digit
will be 0, otherwise 1. The groups are similarly divided into subsets of nearly equal probability and the
particular subset determines the second binary digit. This process is continued until each subset contains
only one message. It is easily seen that apart from minor differences (generally in the last digit) this amounts
to the same thing as the arithmetic process described above.
10. DISCUSSION AND EXAMPLES
In order to obtain the maximum power transfer from a generator to a load, a transformer must in general be
introduced so that the generator as seen from the load has the load resistance. The situation here is roughly
analogous. The transducer which does the encoding should match the source to the channel in a statistical
sense. The source as seen from the channel through the transducer should have the same statistical structure
9Technical Report No. 65, The Research Laboratory of Electronics, M.I.T., March 17, 1949.
17
as the source which maximizes the entropy in the channel. The content of Theorem 9 is that, although an
exact match is not in general possible, we can approximate it as closely as desired. The ratio of the actual
rate of transmission to the capacity C may be called the efficiency of the coding system. This is of course
equal to the ratio of the actual entropy of the channel symbols to the maximum possible entropy.
In general, ideal or nearly ideal encoding requires a long delay in the transmitter and receiver. In the
noiseless case which we have been considering, the main function of this delay is to allow reasonably good
matching of probabilities to corresponding lengths of sequences. With a good code the logarithm of the
reciprocal probability of a long message must be proportional to the duration of the corresponding signal, in
fact  log p 1
T
 C

must be small for all but a small fraction of the long messages.
If a source can produce only one particular message its entropy is zero, and no channel is required. For
example, a computing machine set up to calculate the successive digits of  produces a definite sequence
with no chance element. No channel is required to “transmit” this to another point. One could construct a
second machine to compute the same sequence at the point. However, this may be impractical. In such a case
we can choose to ignore some or all of the statistical knowledge we have of the source. We might consider
the digits of  to be a random sequence in that we construct a system capable of sending any sequence of
digits. In a similar way we may choose to use some of our statistical knowledge of English in constructing
a code, but not all of it. In such a case we consider the source with the maximum entropy subject to the
statistical conditions we wish to retain. The entropy of this source determines the channel capacity which
is necessary and sufficient. In the  example the only information retained is that all the digits are chosen
from the set 0;1; : : : ;9. In the case of English one might wish to use the statistical saving possible due to
letter frequencies, but nothing else. The maximum entropy source is then the first approximation to English
and its entropy determines the required channel capacity.
As a simple example of some of these results consider a source which produces a sequence of letters
chosen from among A, B, C, D with probabilities 12 ,
1
4 ,
1
8 ,
1
8 , successive symbols being chosen independently.
We have
H =  12 log 12 + 14 log 14 + 28 log 18
= 74 bits per symbol:
Thus we can approximate a coding system to encode messages from this source into binary digits with an
average of 74 binary digit per symbol. In this case we can actually achieve the limiting value by the following
code (obtained by the method of the second proof of Theorem 9):
A 0
B 10
C 110
D 111
The average number of binary digits used in encoding a sequence of N symbols will be
N
 
1
2 1+ 14 2+
2
8
3= 74 N:
It is easily seen that the binary digits 0, 1 have probabilities 12 ,
1
2 so the H for the coded sequences is one
bit per symbol. Since, on the average, we have 74 binary symbols per original letter, the entropies on a time
basis are the same. The maximum possible entropy for the original set is log4 = 2, occurring when A, B, C,
D have probabilities 14 ,
1
4 ,
1
4 ,
1
4 . Hence the relative entropy is
7
8 . We can translate the binary sequences into
the original set of symbols on a two-to-one basis by the following table:
00 A0
01 B0
10 C0
11 D0
18
This double process then encodes the original message into the same symbols but with an average compres-
sion ratio 78 .
As a second example consider a source which produces a sequence of A’s and B’s with probability p for
A and q for B. If p q we have
H =  log pp(1  p)1 p
= p log p(1  p)(1 p)=p
:
= p log
e
p
:
In such a case one can construct a fairly good coding of the message on a 0, 1 channel by sending a special
sequence, say 0000, for the infrequent symbol A and then a sequence indicating the number of B’s following
it. This could be indicated by the binary representation with all numbers containing the special sequence
deleted. All numbers up to 16 are represented as usual; 16 is represented by the next binary number after 16
which does not contain four zeros, namely 17= 10001, etc.
It can be shown that as p! 0 the coding approaches ideal provided the length of the special sequence is
properly adjusted.
PART II: THE DISCRETE CHANNEL WITH NOISE
11. REPRESENTATION OF A NOISY DISCRETE CHANNEL
We now consider the case where the signal is perturbed by noise during transmission or at one or the other
of the terminals. This means that the received signal is not necessarily the same as that sent out by the
transmitter. Two cases may be distinguished. If a particular transmitted signal always produces the same
received signal, i.e., the received signal is a definite function of the transmitted signal, then the effect may be
called distortion. If this function has an inverse — no two transmitted signals producing the same received
signal — distortion may be corrected, at least in principle, by merely performing the inverse functional
operation on the received signal.
The case of interest here is that in which the signal does not always undergo the same change in trans-
mission. In this case we may assume the received signal E to be a function of the transmitted signal S and a
second variable, the noise N.
E = f (S;N)
The noise is considered to be a chance variable just as the message was above. In general it may be repre-
sented by a suitable stochastic process. The most general type of noisy discrete channel we shall consider
is a generalization of the finite state noise-free channel described previously. We assume a finite number of
states and a set of probabilities
p;i(; j):
This is the probability, if the channel is in state  and symbol i is transmitted, that symbol j will be received
and the channel left in state . Thus  and  range over the possible states, i over the possible transmitted
signals and j over the possible received signals. In the case where successive symbols are independently per-
turbed by the noise there is only one state, and the channel is described by the set of transition probabilities
pi( j), the probability of transmitted symbol i being received as j.
If a noisy channel is fed by a source there are two statistical processes at work: the source and the noise.
Thus there are a number of entropies that can be calculated. First there is the entropy H(x) of the source
or of the input to the channel (these will be equal if the transmitter is non-singular). The entropy of the
output of the channel, i.e., the received signal, will be denoted by H(y). In the noiseless case H(y) = H(x).
The joint entropy of input and output will be H(xy). Finally there are two conditional entropies Hx(y) and
Hy(x), the entropy of the output when the input is known and conversely. Among these quantities we have
the relations
H(x;y) = H(x)+Hx(y) = H(y)+Hy(x):
All of these entropies can be measured on a per-second or a per-symbol basis.
19
12. EQUIVOCATION AND CHANNEL CAPACITY
If the channel is noisy it is not in general possible to reconstruct the original message or the transmitted
signal with certainty by any operation on the received signal E. There are, however, ways of transmitting
the information which are optimal in combating noise. This is the problem which we now consider.
Suppose there are two possible symbols 0 and 1, and we are transmitting at a rate of 1000 symbols per
second with probabilities p0 = p1 = 12 . Thus our source is producing information at the rate of 1000 bits
per second. During transmission the noise introduces errors so that, on the average, 1 in 100 is received
incorrectly (a 0 as 1, or 1 as 0). What is the rate of transmission of information? Certainly less than 1000
bits per second since about 1% of the received symbols are incorrect. Our first impulse might be to say
the rate is 990 bits per second, merely subtracting the expected number of errors. This is not satisfactory
since it fails to take into account the recipient’s lack of knowledge of where the errors occur. We may carry
it to an extreme case and suppose the noise so great that the received symbols are entirely independent of
the transmitted symbols. The probability of receiving 1 is 12 whatever was transmitted and similarly for 0.
Then about half of the received symbols are correct due to chance alone, and we would be giving the system
credit for transmitting 500 bits per second while actually no information is being transmitted at all. Equally
“good” transmission would be obtained by dispensing with the channel entirely and flipping a coin at the
receiving point.
Evidently the proper correction to apply to the amount of information transmitted is the amount of this
information which is missing in the received signal, or alternatively the uncertainty when we have received
a signal of what was actually sent. From our previous discussion of entropy as a measure of uncertainty it
seems reasonable to use the conditional entropy of the message, knowing the received signal, as a measure
of this missing information. This is indeed the proper definition, as we shall see later. Following this idea
the rate of actual transmission, R, would be obtained by subtracting from the rate of production (i.e., the
entropy of the source) the average rate of conditional entropy.
R = H(x) Hy(x)
The conditional entropy Hy(x) will, for convenience, be called the equivocation. It measures the average
ambiguity of the received signal.
In the example considered above, if a 0 is received the a posteriori probability that a 0 was transmitted
is .99, and that a 1 was transmitted is .01. These figures are reversed if a 1 is received. Hence
Hy(x) = [:99log :99+0:01log0:01]
= :081 bits/symbol
or 81 bits per second. We may say that the system is transmitting at a rate 1000 81 = 919 bits per second.
In the extreme case where a 0 is equally likely to be received as a 0 or 1 and similarly for 1, the a posteriori
probabilities are 12 ,
1
2 and
Hy(x) = 
1
2 log
1
2 +
1
2 log
1
2

= 1 bit per symbol
or 1000 bits per second. The rate of transmission is then 0 as it should be.
The following theorem gives a direct intuitive interpretation of the equivocation and also serves to justify
it as the unique appropriate measure. We consider a communication system and an observer (or auxiliary
device) who can see both what is sent and what is recovered (with errors due to noise). This observer notes
the errors in the recovered message and transmits data to the receiving point over a “correction channel” to
enable the receiver to correct the errors. The situation is indicated schematically in Fig. 8.
Theorem 10: If the correction channel has a capacity equal to Hy(x) it is possible to so encode the
correction data as to send it over this channel and correct all but an arbitrarily small fraction  of the errors.
This is not possible if the channel capacity is less than Hy(x).
20
SOURCE
M
TRANSMITTER RECEIVER CORRECTING
DEVICE
OBSERVER
M0 M
CORRECTION DATA
Fig. 8—Schematic diagram of a correction system.
Roughly then, Hy(x) is the amount of additional information that must be supplied per second at the
receiving point to correct the received message.
To prove the first part, consider long sequences of received message M0 and corresponding original
message M. There will be logarithmically T Hy(x) of the M’s which could reasonably have produced each
M0. Thus we have THy(x) binary digits to send each T seconds. This can be done with  frequency of errors
on a channel of capacity Hy(x).
The second part can be proved by noting, first, that for any discrete chance variables x, y, z
Hy(x;z)Hy(x):
The left-hand side can be expanded to give
Hy(z)+Hyz(x)Hy(x)
Hyz(x)Hy(x) Hy(z)Hy(x) H(z):
If we identify x as the output of the source, y as the received signal and z as the signal sent over the correction
channel, then the right-hand side is the equivocation less the rate of transmission over the correction channel.
If the capacity of this channel is less than the equivocation the right-hand side will be greater than zero and
Hyz(x)> 0. But this is the uncertainty of what was sent, knowing both the received signal and the correction
signal. If this is greater than zero the frequency of errors cannot be arbitrarily small.
Example:
Suppose the errors occur at random in a sequence of binary digits: probability p that a digit is wrong
and q = 1  p that it is right. These errors can be corrected if their position is known. Thus the
correction channel need only send information as to these positions. This amounts to transmitting
from a source which produces binary digits with probability p for 1 (incorrect) and q for 0 (correct).
This requires a channel of capacity
 [p log p+q logq]
which is the equivocation of the original system.
The rate of transmission R can be written in two other forms due to the identities noted above. We have
R = H(x) Hy(x)
= H(y) Hx(y)
= H(x)+H(y) H(x;y):
21
The first defining expression has already been interpreted as the amount of information sent less the uncer-
tainty of what was sent. The second measures the amount received less the part of this which is due to noise.
The third is the sum of the two amounts less the joint entropy and therefore in a sense is the number of bits
per second common to the two. Thus all three expressions have a certain intuitive significance.
The capacity C of a noisy channel should be the maximum possible rate of transmission, i.e., the rate
when the source is properly matched to the channel. We therefore define the channel capacity by
C = Max
 
H(x) Hy(x)

where the maximum is with respect to all possible information sources used as input to the channel. If the
channel is noiseless, Hy(x) = 0. The definition is then equivalent to that already given for a noiseless channel
since the maximum entropy for the channel is its capacity.
13. THE FUNDAMENTAL THEOREM FOR A DISCRETE CHANNEL WITH NOISE
It may seem surprising that we should define a definite capacity C for a noisy channel since we can never
send certain information in such a case. It is clear, however, that by sending the information in a redundant
form the probability of errors can be reduced. For example, by repeating the message many times and by a
statistical study of the different received versions of the message the probability of errors could be made very
small. One would expect, however, that to make this probability of errors approach zero, the redundancy
of the encoding must increase indefinitely, and the rate of transmission therefore approach zero. This is by
no means true. If it were, there would not be a very well defined capacity, but only a capacity for a given
frequency of errors, or a given equivocation; the capacity going down as the error requirements are made
more stringent. Actually the capacity C defined above has a very definite significance. It is possible to send
information at the rate C through the channel with as small a frequency of errors or equivocation as desired
by proper encoding. This statement is not true for any rate greater than C. If an attempt is made to transmit
at a higher rate than C, say C+R1, then there will necessarily be an equivocation equal to or greater than the
excess R1. Nature takes payment by requiring just that much uncertainty, so that we are not actually getting
any more than C through correctly.
The situation is indicated in Fig. 9. The rate of information into the channel is plotted horizontally and
the equivocation vertically. Any point above the heavy line in the shaded region can be attained and those
below cannot. The points on the line cannot in general be attained, but there will usually be two points on
the line that can.
These results are the main justification for the definition of C and will now be proved.
Theorem 11: Let a discrete channel have the capacity C and a discrete source the entropy per second H.
If H C there exists a coding system such that the output of the source can be transmitted over the channel
with an arbitrarily small frequency of errors (or an arbitrarily small equivocation). If H > C it is possible
to encode the source so that the equivocation is less than H C+  where  is arbitrarily small. There is no
method of encoding which gives an equivocation less than H C.
The method of proving the first part of this theorem is not by exhibiting a coding method having the
desired properties, but by showing that such a code must exist in a certain group of codes. In fact we will
ATTAINABLE
REGION
C H(x)
Hy(x)
SL
OP
E
=
1.
0
Fig. 9—The equivocation possible for a given input entropy to a channel.
22
average the frequency of errors over this group and show that this average can be made less than . If the
average of a set of numbers is less than  there must exist at least one in the set which is less than . This
will establish the desired result.
The capacity C of a noisy channel has been defined as
C = Max
 
H(x) Hy(x)

where x is the input and y the output. The maximization is over all sources which might be used as input to
the channel.
Let S0 be a source which achieves the maximum capacity C. If this maximum is not actually achieved
by any source let S0 be a source which approximates to giving the maximum rate. Suppose S0 is used as
input to the channel. We consider the possible transmitted and received sequences of a long duration T . The
following will be true:
1. The transmitted sequences fall into two classes, a high probability group with about 2TH(x) members
and the remaining sequences of small total probability.
2. Similarly the received sequences have a high probability set of about 2TH(y) members and a low
probability set of remaining sequences.
3. Each high probability output could be produced by about 2THy(x) inputs. The probability of all other
cases has a small total probability.
All the ’s and ’s implied by the words “small” and “about” in these statements approach zero as we
allow T to increase and S0 to approach the maximizing source.
The situation is summarized in Fig. 10 where the input sequences are points on the left and output
sequences points on the right. The fan of cross lines represents the range of possible causes for a typical
output.
M
E
2H(x)T
HIGH PROBABILITY
MESSAGES
2H(y)T
HIGH PROBABILITY
RECEIVED SIGNALS
2Hy(x)T
REASONABLE CAUSES
FOR EACH E
2Hx(y)T
REASONABLE EFFECTS
FOR EACH M
Fig. 10—Schematic representation of the relations between inputs and outputs in a channel.
Now suppose we have another source producing information at rate R with R <C. In the period T this
source will have 2TR high probability messages. We wish to associate these with a selection of the possible
channel inputs in such a way as to get a small frequency of errors. We will set up this association in all
23
possible ways (using, however, only the high probability group of inputs as determined by the source S0)
and average the frequency of errors for this large class of possible coding systems. This is the same as
calculating the frequency of errors for a random association of the messages and channel inputs of duration
T . Suppose a particular output y1 is observed. What is the probability of more than one message in the set
of possible causes of y1? There are 2TR messages distributed at random in 2TH(x) points. The probability of
a particular point being a message is thus
2T (R H(x)):
The probability that none of the points in the fan is a message (apart from the actual originating message) is
P =

1 2T(R H(x))2THy(x) :
Now R < H(x) Hy(x) so R H(x) = Hy(x)  with  positive. Consequently
P =

1 2 THy(x) T2THy(x)
approaches (as T ! ∞)
1 2 T :
Hence the probability of an error approaches zero and the first part of the theorem is proved.
The second part of the theorem is easily shown by noting that we could merely send C bits per second
from the source, completely neglecting the remainder of the information generated. At the receiver the
neglected part gives an equivocation H(x) C and the part transmitted need only add . This limit can also
be attained in many other ways, as will be shown when we consider the continuous case.
The last statement of the theorem is a simple consequence of our definition of C. Suppose we can encode
a source with H(x) =C+a in such a way as to obtain an equivocation Hy(x) = a   with  positive. Then
R = H(x) =C+a and
H(x) Hy(x) =C+ 
with  positive. This contradicts the definition of C as the maximum of H(x) Hy(x).
Actually more has been proved than was stated in the theorem. If the average of a set of numbers is
within  of of their maximum, a fraction of at most
p
 can be more than
p
 below the maximum. Since  is
arbitrarily small we can say that almost all the systems are arbitrarily close to the ideal.
14. DISCUSSION
The demonstration of Theorem 11, while not a pure existence proof, has some of the deficiencies of such
proofs. An attempt to obtain a good approximation to ideal coding by following the method of the proof is
generally impractical. In fact, apart from some rather trivial cases and certain limiting situations, no explicit
description of a series of approximation to the ideal has been found. Probably this is no accident but is
related to the difficulty of giving an explicit construction for a good approximation to a random sequence.
An approximation to the ideal would have the property that if the signal is altered in a reasonable way
by the noise, the original can still be recovered. In other words the alteration will not in general bring it
closer to another reasonable signal than the original. This is accomplished at the cost of a certain amount of
redundancy in the coding. The redundancy must be introduced in the proper way to combat the particular
noise structure involved. However, any redundancy in the source will usually help if it is utilized at the
receiving point. In particular, if the source already has a certain redundancy and no attempt is made to
eliminate it in matching to the channel, this redundancy will help combat noise. For example, in a noiseless
telegraph channel one could save about 50% in time by proper encoding of the messages. This is not done
and most of the redundancy of English remains in the channel symbols. This has the advantage, however,
of allowing considerable noise in the channel. A sizable fraction of the letters can be received incorrectly
and still reconstructed by the context. In fact this is probably not a bad approximation to the ideal in many
cases, since the statistical structure of English is rather involved and the reasonable English sequences are
not too far (in the sense required for the theorem) from a random selection.
24
As in the noiseless case a delay is generally required to approach the ideal encoding. It now has the
additional function of allowing a large sample of noise to affect the signal before any judgment is made
at the receiving point as to the original message. Increasing the sample size always sharpens the possible
statistical assertions.
The content of Theorem 11 and its proof can be formulated in a somewhat different way which exhibits
the connection with the noiseless case more clearly. Consider the possible signals of duration T and suppose
a subset of them is selected to be used. Let those in the subset all be used with equal probability, and suppose
the receiver is constructed to select, as the original signal, the most probable cause from the subset, when a
perturbed signal is received. We define N(T;q) to be the maximum number of signals we can choose for the
subset such that the probability of an incorrect interpretation is less than or equal to q.
Theorem 12: Lim
T!∞
logN(T;q)
T
=C, where C is the channel capacity, provided that q does not equal 0 or
1.
In other words, no matter how we set out limits of reliability, we can distinguish reliably in time T
enough messages to correspond to about CT bits, when T is sufficiently large. Theorem 12 can be compared
with the definition of the capacity of a noiseless channel given in Section 1.
15. EXAMPLE OF A DISCRETE CHANNEL AND ITS CAPACITY
A simple example of a discrete channel is indicated in Fig. 11. There are three possible symbols. The first is
never affected by noise. The second and third each have probability p of coming through undisturbed, and
q of being changed into the other of the pair. We have (letting  = [p log p+ q logq] and P and Q be the
p
p
q
q
TRANSMITTED
SYMBOLS
RECEIVED
SYMBOLS
Fig. 11—Example of a discrete channel.
probabilities of using the first and second symbols)
H(x) = P logP 2Q logQ
Hy(x) = 2Q:
We wish to choose P and Q in such a way as to maximize H(x) Hy(x), subject to the constraint P+2Q= 1.
Hence we consider
U = P logP 2Q logQ 2Q+(P+2Q)
∂U
∂P
= 1  logP+= 0
∂U
∂Q
= 2 2logQ 2+2= 0:
Eliminating 
logP = logQ+
P = Qe = Q
25
P =

+2
Q =
1
+2
:
The channel capacity is then
C = log
+2

:
Note how this checks the obvious values in the cases p = 1 and p = 12 . In the first,  = 1 and C = log3,
which is correct since the channel is then noiseless with three possible symbols. If p = 12 ,  = 2 and
C = log2. Here the second and third symbols cannot be distinguished at all and act together like one
symbol. The first symbol is used with probability P = 12 and the second and third together with probability
1
2 . This may be distributed between them in any desired way and still achieve the maximum capacity.
For intermediate values of p the channel capacity will lie between log2 and log3. The distinction
between the second and third symbols conveys some information but not as much as in the noiseless case.
The first symbol is used somewhat more frequently than the other two because of its freedom from noise.
16. THE CHANNEL CAPACITY IN CERTAIN SPECIAL CASES
If the noise affects successive channel symbols independently it can be described by a set of transition
probabilities pi j. This is the probability, if symbol i is sent, that j will be received. The maximum channel
rate is then given by the maximum of
 ∑
i; j
Pi pi j log∑
i
Pipi j +∑
i; j
Pi pi j log pi j
where we vary the Pi subject to ∑Pi = 1. This leads by the method of Lagrange to the equations,
∑
j
ps j log
ps j
∑i Pipi j
=  s = 1;2; : : : :
Multiplying by Ps and summing on s shows that  = C. Let the inverse of ps j (if it exists) be hst so that
∑s hst ps j = t j. Then:
∑
s; j
hst ps j log ps j  log∑
i
Pi pit =C∑
s
hst :
Hence:
∑
i
Pi pit = exp
h
 C∑
s
hst +∑
s; j
hst ps j log ps j
i
or,
Pi = ∑
t
hit exp
h
 C∑
s
hst +∑
s; j
hst ps j log ps j
i
:
This is the system of equations for determining the maximizing values of Pi, with C to be determined so
that ∑Pi = 1. When this is done C will be the channel capacity, and the Pi the proper probabilities for the
channel symbols to achieve this capacity.
If each input symbol has the same set of probabilities on the lines emerging from it, and the same is true
of each output symbol, the capacity can be easily calculated. Examples are shown in Fig. 12. In such a case
Hx(y) is independent of the distribution of probabilities on the input symbols, and is given by  ∑ pi log pi
where the pi are the values of the transition probabilities from any input symbol. The channel capacity is
Max

H(y) Hx(y)

= MaxH(y)+∑ pi log pi:
The maximum of H(y) is clearly logm where m is the number of output symbols, since it is possible to make
them all equally probable by making the input symbols equally probable. The channel capacity is therefore
C = logm+∑ pi log pi:
26
a b c
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/3
1/3
1/3
1/3
1/6
1/6
1/6
1/6
1/6
1/6
1/6
1/3
1/3
1/3
1/2
1/2
1/2
Fig. 12—Examples of discrete channels with the same transition probabilities for each input and for each output.
In Fig. 12a it would be
C = log4  log2 = log2:
This could be achieved by using only the 1st and 3d symbols. In Fig. 12b
C = log4  23 log3  13 log6
= log4  log3  13 log2
= log 13 2
5
3 :
In Fig. 12c we have
C = log3  12 log2  13 log3  16 log6
= log
3
2
1
2 3
1
3 6
1
6
:
Suppose the symbols fall into several groups such that the noise never causes a symbol in one group to
be mistaken for a symbol in another group. Let the capacity for the nth group be Cn (in bits per second)
when we use only the symbols in this group. Then it is easily shown that, for best use of the entire set, the
total probability Pn of all symbols in the nth group should be
Pn =
2Cn
∑2Cn
:
Within a group the probability is distributed just as it would be if these were the only symbols being used.
The channel capacity is
C = log∑2Cn :
17. AN EXAMPLE OF EFFICIENT CODING
The following example, although somewhat unrealistic, is a case in which exact matching to a noisy channel
is possible. There are two channel symbols, 0 and 1, and the noise affects them in blocks of seven symbols.
A block of seven is either transmitted without error, or exactly one symbol of the seven is incorrect. These
eight possibilities are equally likely. We have
C = Max

H(y) Hx(y)

= 17

7+ 88 log
1
8

= 47 bits/symbol:
An efficient code, allowing complete correction of errors and transmitting at the rate C, is the following
(found by a method due to R. Hamming):
27
Let a block of seven symbols be X1;X2; : : : ;X7. Of these X3, X5, X6 and X7 are message symbols and
chosen arbitrarily by the source. The other three are redundant and calculated as follows:
X4 is chosen to make = X4 +X5 +X6 +X7 even
X2 “ “ “ “  = X2 +X3 +X6 +X7 “
X1 “ “ “ “  = X1 +X3 +X5 +X7 “
When a block of seven is received ; and  are calculated and if even called zero, if odd called one. The
binary number   then gives the subscript of the Xi that is incorrect (if 0 there was no error).
APPENDIX 1
THE GROWTH OF THE NUMBER OF BLOCKS OF SYMBOLS WITH A FINITE STATE CONDITION
Let Ni(L) be the number of blocks of symbols of length L ending in state i. Then we have
Nj(L) = ∑
i;s
Ni
 
L b(s)i j

where b1i j;b
2
i j; : : : ;b
m
i j are the length of the symbols which may be chosen in state i and lead to state j. These
are linear difference equations and the behavior as L! ∞ must be of the type
Nj = A jW
L:
Substituting in the difference equation
A jW
L = ∑
i;s
AiW
L b
(s)
i j
or
A j = ∑
i;s
AiW
 b
(s)
i j
∑
i

∑
s
W b
(s)
i j   i j

Ai = 0:
For this to be possible the determinant
D(W) = jai jj=
∑
s
W b
(s)
i j   i j

must vanish and this determines W , which is, of course, the largest real root of D = 0.
The quantity C is then given by
C = Lim
L!∞
log∑A jW L
L
= logW
and we also note that the same growth properties result if we require that all blocks start in the same (arbi-
trarily chosen) state.
APPENDIX 2
DERIVATION OF H = ∑ pi log pi
Let H
1
n
;
1
n
; : : : ;
1
n

= A(n). From condition (3) we can decompose a choice from sm equally likely possi-
bilities into a series of m choices from s equally likely possibilities and obtain
A(sm) = mA(s):
28
Similarly
A(tn) = nA(t):
We can choose n arbitrarily large and find an m to satisfy
sm  tn < s(m+1):
Thus, taking logarithms and dividing by n logs,
m
n
 log t
log s
 m
n
+
1
n
or
m
n
  log t
log s
< 
where  is arbitrarily small. Now from the monotonic property of A(n),
A(sm) A(tn) A(sm+1)
mA(s) nA(t) (m+1)A(s):
Hence, dividing by nA(s),
m
n
 A(t)
A(s)
 m
n
+
1
n
or
m
n
  A(t)
A(s)
< 
A(t)
A(s)
  logt
logs
< 2 A(t) = K log t
where K must be positive to satisfy (2).
Now suppose we have a choice from n possibilities with commeasurable probabilities pi =
ni
∑ni
where
the ni are integers. We can break down a choice from ∑ni possibilities into a choice from n possibilities
with probabilities p1; : : : ; pn and then, if the ith was chosen, a choice from ni with equal probabilities. Using
condition (3) again, we equate the total choice from ∑ni as computed by two methods
K log∑ni = H(p1; : : : ; pn)+K∑ pi logni:
Hence
H = K
h
∑ pi log∑ni ∑ pi logni
i
= K∑ pi log ni∑ni = K∑ pi log pi:
If the pi are incommeasurable, they may be approximated by rationals and the same expression must hold
by our continuity assumption. Thus the expression holds in general. The choice of coefficient K is a matter
of convenience and amounts to the choice of a unit of measure.
APPENDIX 3
THEOREMS ON ERGODIC SOURCES
If it is possible to go from any state with P > 0 to any other along a path of probability p > 0, the system is
ergodic and the strong law of large numbers can be applied. Thus the number of times a given path pi j in
the network is traversed in a long sequence of length N is about proportional to the probability of being at
i, say Pi, and then choosing this path, Pi pi jN. If N is large enough the probability of percentage error in
this is less than  so that for all but a set of small probability the actual numbers lie within the limits
(Pi pi j )N:
Hence nearly all sequences have a probability p given by
p = ∏ p(Pi pi j)Ni j
29
and
log p
N
is limited by
log p
N
= ∑(Pi pi j ) log pi j
or  log p
N
 ∑Pi pi j log pi j
< :
This proves Theorem 3.
Theorem 4 follows immediately from this on calculating upper and lower bounds for n(q) based on the
possible range of values of p in Theorem 3.
In the mixed (not ergodic) case if
L = ∑ piLi
and the entropies of the components are H1 H2    Hn we have the
Theorem: Lim
N!∞
logn(q)
N = '(q) is a decreasing step function,
'(q) = Hs in the interval
s 1
∑
1
i < q <
s
∑
1
i:
To prove Theorems 5 and 6 first note that FN is monotonic decreasing because increasing N adds a
subscript to a conditional entropy. A simple substitution for pBi(S j) in the definition of FN shows that
FN = NGN   (N 1)GN 1
and summing this for all N gives GN =
1
N ∑Fn. Hence GN  FN and GN monotonic decreasing. Also they
must approach the same limit. By using Theorem 3 we see that Lim
N!∞
GN = H.
APPENDIX 4
MAXIMIZING THE RATE FOR A SYSTEM OF CONSTRAINTS
Suppose we have a set of constraints on sequences of symbols that is of the finite state type and can be
represented therefore by a linear graph. Let `(s)i j be the lengths of the various symbols that can occur in
passing from state i to state j. What distribution of probabilities Pi for the different states and p
(s)
i j for
choosing symbol s in state i and going to state j maximizes the rate of generating information under these
constraints? The constraints define a discrete channel and the maximum rate must be less than or equal to
the capacity C of this channel, since if all blocks of large length were equally likely, this rate would result,
and if possible this would be best. We will show that this rate can be achieved by proper choice of the Pi and
p(s)i j .
The rate in question is
 ∑Pi p(s)i j log p(s)i j
∑Pi p
(s)
i j `
(s)
i j
=
N
M
:
Let `i j = ∑s `
(s)
i j . Evidently for a maximum p
(s)
i j = k exp`
(s)
i j . The constraints on maximization are ∑Pi =
1, ∑ j pi j = 1, ∑Pi(pi j  i j) = 0. Hence we maximize
U =
 ∑Pi pi j log pi j
∑Pi pi j`i j
+∑
i
Pi +∑i pi j +∑ jPi(pi j  i j)
∂U
∂pi j
= MPi(1+ log pi j)+NPi`i j
M2
++i +iPi = 0:
30
Solving for pi j
pi j = AiB jD
 `i j :
Since
∑
j
pi j = 1; A
 1
i = ∑
j
B jD
 `i j
pi j =
B jD `i j
∑s BsD `is
:
The correct value of D is the capacity C and the B j are solutions of
Bi =∑B jC `i j
for then
pi j =
B j
Bi
C `i j
∑Pi B jBi C
 `i j = Pj
or
∑ PiBi C
 `i j =
Pj
B j
:
So that if i satisfy
∑iC `i j =  j
Pi = Bii:
Both the sets of equations for Bi and i can be satisfied since C is such that
jC `i j   i jj= 0:
In this case the rate is
 ∑
Pi pi j log
B j
Bi
C `i j
∑Pipi j`i j
=C  ∑
Pi pi j log
B j
Bi
∑Pipi j`i j
but
∑Pi pi j(logB j  logBi) = ∑
j
Pj logB j ∑Pi logBi = 0
Hence the rate is C and as this could never be exceeded this is the maximum, justifying the assumed solution.
31
PART III: MATHEMATICAL PRELIMINARIES
In this final installment of the paper we consider the case where the signals or the messages or both are
continuously variable, in contrast with the discrete nature assumed heretofore. To a considerable extent the
continuous case can be obtained through a limiting process from the discrete case by dividing the continuum
of messages and signals into a large but finite number of small regions and calculating the various parameters
involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as
limits the proper values for the continuous case. There are, however, a few new effects that appear and also
a general change of emphasis in the direction of specialization of the general results to particular cases.
We will not attempt, in the continuous case, to obtain our results with the greatest generality, or with
the extreme rigor of pure mathematics, since this would involve a great deal of abstract measure theory
and would obscure the main thread of the analysis. A preliminary study, however, indicates that the theory
can be formulated in a completely axiomatic and rigorous manner which includes both the continuous and
discrete cases and many others. The occasional liberties taken with limiting processes in the present analysis
can be justified in all cases of practical interest.
18. SETS AND ENSEMBLES OF FUNCTIONS
We shall have to deal in the continuous case with sets of functions and ensembles of functions. A set of
functions, as the name implies, is merely a class or collection of functions, generally of one variable, time.
It can be specified by giving an explicit representation of the various functions in the set, or implicitly by
giving a property which functions in the set possess and others do not. Some examples are:
1. The set of functions:
f(t) = sin(t + ):
Each particular value of  determines a particular function in the set.
2. The set of all functions of time containing no frequencies over W cycles per second.
3. The set of all functions limited in band to W and in amplitude to A.
4. The set of all English speech signals as functions of time.
An ensemble of functions is a set of functions together with a probability measure whereby we may
determine the probability of a function in the set having certain properties.1 For example with the set,
f(t) = sin(t + );
we may give a probability distribution for , P(). The set then becomes an ensemble.
Some further examples of ensembles of functions are:
1. A finite set of functions fk(t) (k = 1;2; : : : ;n) with the probability of fk being pk.
2. A finite dimensional family of functions
f (1;2; : : : ;n;t)
with a probability distribution on the parameters i:
p(1; : : : ;n):
For example we could consider the ensemble defined by
f (a1; : : : ;an;1; : : : ;n;t) =
n
∑
i=1
ai sin i(!t + i)
with the amplitudes ai distributed normally and independently, and the phases i distributed uniformly
(from 0 to 2) and independently.
1In mathematical terminology the functions belong to a measure space whose total measure is unity.
32
3. The ensemble
f (ai; t) =
+∞
∑
n= ∞
an
sin(2Wt n)
(2Wt n)
with the ai normal and independent all with the same standard deviation
p
N. This is a representation
of “white” noise, band limited to the band from 0 to W cycles per second and with average power N.2
4. Let points be distributed on the t axis according to a Poisson distribution. At each selected point the
function f (t) is placed and the different functions added, giving the ensemble
∞
∑
k= ∞
f (t + tk)
where the tk are the points of the Poisson distribution. This ensemble can be considered as a type of
impulse or shot noise where all the impulses are identical.
5. The set of English speech functions with the probability measure given by the frequency of occurrence
in ordinary use.
An ensemble of functions f(t) is stationary if the same ensemble results when all functions are shifted
any fixed amount in time. The ensemble
f(t) = sin(t + )
is stationary if  is distributed uniformly from 0 to 2. If we shift each function by t1 we obtain
f(t + t1) = sin(t + t1 + )
= sin(t +')
with ' distributed uniformly from 0 to 2. Each function has changed but the ensemble as a whole is
invariant under the translation. The other examples given above are also stationary.
An ensemble is ergodic if it is stationary, and there is no subset of the functions in the set with a
probability different from 0 and 1 which is stationary. The ensemble
sin(t + )
is ergodic. No subset of these functions of probability 6= 0;1 is transformed into itself under all time trans-
lations. On the other hand the ensemble
asin(t + )
with a distributed normally and  uniform is stationary but not ergodic. The subset of these functions with
a between 0 and 1 for example is stationary.
Of the examples given, 3 and 4 are ergodic, and 5 may perhaps be considered so. If an ensemble is
ergodic we may say roughly that each function in the set is typical of the ensemble. More precisely it is
known that with an ergodic ensemble an average of any statistic over the ensemble is equal (with probability
1) to an average over the time translations of a particular function of the set.3 Roughly speaking, each
function can be expected, as time progresses, to go through, with the proper frequency, all the convolutions
of any of the functions in the set.
2This representation can be used as a definition of band limited white noise. It has certain advantages in that it involves fewer
limiting operations than do definitions that have been used in the past. The name “white noise,” already firmly entrenched in the
literature, is perhaps somewhat unfortunate. In optics white light means either any continuous spectrum as contrasted with a point
spectrum, or a spectrum which is flat with wavelength (which is not the same as a spectrum flat with frequency).
3This is the famous ergodic theorem or rather one aspect of this theorem which was proved in somewhat different formulations
by Birkoff, von Neumann, and Koopman, and subsequently generalized by Wiener, Hopf, Hurewicz and others. The literature on
ergodic theory is quite extensive and the reader is referred to the papers of these writers for precise and general formulations; e.g.,
E. Hopf, “Ergodentheorie,” Ergebnisse der Mathematik und ihrer Grenzgebiete, v. 5; “On Causality Statistics and Probability,” Journal
of Mathematics and Physics, v. XIII, No. 1, 1934; N. Wiener, “The Ergodic Theorem,” Duke Mathematical Journal, v. 5, 1939.
33
Just as we may perform various operations on numbers or functions to obtain new numbers or functions,
we can perform operations on ensembles to obtain new ensembles. Suppose, for example, we have an
ensemble of functions f(t) and an operator T which gives for each function f(t) a resulting function
g(t):
g(t) = T f(t):
Probability measure is defined for the set g(t) by means of that for the set f(t). The probability of a certain
subset of the g(t) functions is equal to that of the subset of the f(t) functions which produce members of
the given subset of g functions under the operation T . Physically this corresponds to passing the ensemble
through some device, for example, a filter, a rectifier or a modulator. The output functions of the device
form the ensemble g(t).
A device or operator T will be called invariant if shifting the input merely shifts the output, i.e., if
g(t) = T f(t)
implies
g(t + t1) = T f(t + t1)
for all f(t) and all t1. It is easily shown (see Appendix 5 that if T is invariant and the input ensemble is
stationary then the output ensemble is stationary. Likewise if the input is ergodic the output will also be
ergodic.
A filter or a rectifier is invariant under all time translations. The operation of modulation is not since the
carrier phase gives a certain time structure. However, modulation is invariant under all translations which
are multiples of the period of the carrier.
Wiener has pointed out the intimate relation between the invariance of physical devices under time
translations and Fourier theory.4 He has shown, in fact, that if a device is linear as well as invariant Fourier
analysis is then the appropriate mathematical tool for dealing with the problem.
An ensemble of functions is the appropriate mathematical representation of the messages produced by
a continuous source (for example, speech), of the signals produced by a transmitter, and of the perturbing
noise. Communication theory is properly concerned, as has been emphasized by Wiener, not with operations
on particular functions, but with operations on ensembles of functions. A communication system is designed
not for a particular speech function and still less for a sine wave, but for the ensemble of speech functions.
19. BAND LIMITED ENSEMBLES OF FUNCTIONS
If a function of time f (t) is limited to the band from 0 to W cycles per second it is completely determined
by giving its ordinates at a series of discrete points spaced 12W seconds apart in the manner indicated by the
following result.5
Theorem 13: Let f (t) contain no frequencies over W . Then
f (t) =
∞
∑
 ∞
Xn
sin(2Wt n)
(2Wt n)
where
Xn = f
 n
2W

:
4Communication theory is heavily indebted to Wiener for much of its basic philosophy and theory. His classic NDRC report,
The Interpolation, Extrapolation and Smoothing of Stationary Time Series (Wiley, 1949), contains the first clear-cut formulation of
communication theory as a statistical problem, the study of operations on time series. This work, although chiefly concerned with the
linear prediction and filtering problem, is an important collateral reference in connection with the present paper. We may also refer
here to Wiener’s Cybernetics (Wiley, 1948), dealing with the general problems of communication and control.
5For a proof of this theorem and further discussion see the author’s paper “Communication in the Presence of Noise” published in
the Proceedings of the Institute of Radio Engineers, v. 37, No. 1, Jan., 1949, pp. 10–21.
34
In this expansion f (t) is represented as a sum of orthogonal functions. The coefficients Xn of the various
terms can be considered as coordinates in an infinite dimensional “function space.” In this space each
function corresponds to precisely one point and each point to one function.
A function can be considered to be substantially limited to a time T if all the ordinates Xn outside this
interval of time are zero. In this case all but 2TW of the coordinates will be zero. Thus functions limited to
a band W and duration T correspond to points in a space of 2TW dimensions.
A subset of the functions of band W and duration T corresponds to a region in this space. For example,
the functions whose total energy is less than or equal to E correspond to points in a 2TW dimensional sphere
with radius r =
p
2WE.
An ensemble of functions of limited duration and band will be represented by a probability distribution
p(x1; : : : ;xn) in the corresponding n dimensional space. If the ensemble is not limited in time we can consider
the 2TW coordinates in a given interval T to represent substantially the part of the function in the interval T
and the probability distribution p(x1; : : : ;xn) to give the statistical structure of the ensemble for intervals of
that duration.
20. ENTROPY OF A CONTINUOUS DISTRIBUTION
The entropy of a discrete set of probabilities p1; : : : ; pn has been defined as:
H = ∑ pi log pi:
In an analogous manner we define the entropy of a continuous distribution with the density distribution
function p(x) by:
H = 
Z ∞
 ∞
p(x) log p(x)dx:
With an n dimensional distribution p(x1; : : : ;xn) we have
H = 
Z
  
Z
p(x1; : : : ;xn) log p(x1; : : : ;xn)dx1   dxn:
If we have two arguments x and y (which may themselves be multidimensional) the joint and conditional
entropies of p(x;y) are given by
H(x;y) = 
ZZ
p(x;y) log p(x;y)dxdy
and
Hx(y) = 
ZZ
p(x;y) log
p(x;y)
p(x)
dxdy
Hy(x) = 
ZZ
p(x;y) log
p(x;y)
p(y)
dxdy
where
p(x) =
Z
p(x;y)dy
p(y) =
Z
p(x;y)dx:
The entropies of continuous distributions have most (but not all) of the properties of the discrete case.
In particular we have the following:
1. If x is limited to a certain volume v in its space, then H(x) is a maximum and equal to logv when p(x)
is constant (1=v) in the volume.
35
2. With any two variables x, y we have
H(x;y)H(x)+H(y)
with equality if (and only if) x and y are independent, i.e., p(x;y) = p(x)p(y) (apart possibly from a
set of points of probability zero).
3. Consider a generalized averaging operation of the following type:
p0(y) =
Z
a(x;y)p(x)dx
with Z
a(x;y)dx =
Z
a(x;y)dy = 1; a(x;y) 0:
Then the entropy of the averaged distribution p0(y) is equal to or greater than that of the original
distribution p(x).
4. We have
H(x;y) = H(x)+Hx(y) = H(y)+Hy(x)
and
Hx(y)H(y):
5. Let p(x) be a one-dimensional distribution. The form of p(x) giving a maximum entropy subject to the
condition that the standard deviation of x be fixed at  is Gaussian. To show this we must maximize
H(x) = 
Z
p(x) log p(x)dx
with
2 =
Z
p(x)x2 dx and 1 =
Z
p(x)dx
as constraints. This requires, by the calculus of variations, maximizing
Z  p(x) log p(x)+p(x)x2 +p(x)dx:
The condition for this is
 1  log p(x)+x2 += 0
and consequently (adjusting the constants to satisfy the constraints)
p(x) =
1p
2
e (x
2=22):
Similarly in n dimensions, suppose the second order moments of p(x1; : : : ;xn) are fixed at Ai j:
Ai j =
Z
  
Z
xix j p(x1; : : : ;xn)dx1    dxn:
Then the maximum entropy occurs (by a similar calculation) when p(x1; : : : ;xn) is the n dimensional
Gaussian distribution with the second order moments Ai j.
36
6. The entropy of a one-dimensional Gaussian distribution whose standard deviation is  is given by
H(x) = log
p
2e:
This is calculated as follows:
p(x) =
1p
2
e (x
2=22)
  log p(x) = log
p
2+
x2
22
H(x) = 
Z
p(x) log p(x)dx
=
Z
p(x) log
p
2dx+
Z
p(x)
x2
22
dx
= log
p
2+
2
22
= log
p
2+ log
p
e
= log
p
2e:
Similarly the n dimensional Gaussian distribution with associated quadratic form ai j is given by
p(x1; : : : ;xn) =
jai jj 12
(2)n=2
exp

  12 ∑ai jxix j

and the entropy can be calculated as
H = log(2e)n=2jai jj  12
where jai jj is the determinant whose elements are ai j.
7. If x is limited to a half line (p(x) = 0 for x 0) and the first moment of x is fixed at a:
a =
Z ∞
0
p(x)xdx;
then the maximum entropy occurs when
p(x) =
1
a
e (x=a)
and is equal to logea.
8. There is one important difference between the continuous and discrete entropies. In the discrete case
the entropy measures in an absolute way the randomness of the chance variable. In the continuous
case the measurement is relative to the coordinate system. If we change coordinates the entropy will
in general change. In fact if we change to coordinates y1   yn the new entropy is given by
H(y) =
Z
  
Z
p(x1; : : : ;xn)J
x
y

log p(x1; : : : ;xn)J
x
y

dy1   dyn
where J
 
x
y

is the Jacobian of the coordinate transformation. On expanding the logarithm and chang-
ing the variables to x1   xn, we obtain:
H(y) = H(x) 
Z
  
Z
p(x1; : : : ;xn) logJ
x
y

dx1 : : :dxn:
37
Thus the new entropy is the old entropy less the expected logarithm of the Jacobian. In the continuous
case the entropy can be considered a measure of randomness relative to an assumed standard, namely
the coordinate system chosen with each small volume element dx1   dxn given equal weight. When
we change the coordinate system the entropy in the new system measures the randomness when equal
volume elements dy1   dyn in the new system are given equal weight.
In spite of this dependence on the coordinate system the entropy concept is as important in the con-
tinuous case as the discrete case. This is due to the fact that the derived concepts of information rate
and channel capacity depend on the difference of two entropies and this difference does not depend
on the coordinate frame, each of the two terms being changed by the same amount.
The entropy of a continuous distribution can be negative. The scale of measurements sets an arbitrary
zero corresponding to a uniform distribution over a unit volume. A distribution which is more confined
than this has less entropy and will be negative. The rates and capacities will, however, always be non-
negative.
9. A particular case of changing coordinates is the linear transformation
y j = ∑
i
ai jxi:
In this case the Jacobian is simply the determinant jai jj 1 and
H(y) = H(x)+ log jai jj:
In the case of a rotation of coordinates (or any measure preserving transformation) J = 1 and H(y) =
H(x).
21. ENTROPY OF AN ENSEMBLE OF FUNCTIONS
Consider an ergodic ensemble of functions limited to a certain band of width W cycles per second. Let
p(x1; : : : ;xn)
be the density distribution function for amplitudes x1; : : : ;xn at n successive sample points. We define the
entropy of the ensemble per degree of freedom by
H 0 = Lim
n!∞
1
n
Z
  
Z
p(x1; : : : ;xn) log p(x1; : : : ;xn)dx1 : : :dxn:
We may also define an entropy H per second by dividing, not by n, but by the time T in seconds for n
samples. Since n = 2TW , H = 2WH 0.
With white thermal noise p is Gaussian and we have
H 0 = log
p
2eN;
H =W log2eN:
For a given average power N, white noise has the maximum possible entropy. This follows from the
maximizing properties of the Gaussian distribution noted above.
The entropy for a continuous stochastic process has many properties analogous to that for discrete pro-
cesses. In the discrete case the entropy was related to the logarithm of the probability of long sequences,
and to the number of reasonably probable sequences of long length. In the continuous case it is related in
a similar fashion to the logarithm of the probability density for a long series of samples, and the volume of
reasonably high probability in the function space.
More precisely, if we assume p(x1; : : : ;xn) continuous in all the xi for all n, then for sufficiently large n log p
n
 H 0
< 
38
for all choices of (x1; : : : ;xn) apart from a set whose total probability is less than , with  and  arbitrarily
small. This follows form the ergodic property if we divide the space into a large number of small cells.
The relation of H to volume can be stated as follows: Under the same assumptions consider the n
dimensional space corresponding to p(x1; : : : ;xn). Let Vn(q) be the smallest volume in this space which
includes in its interior a total probability q. Then
Lim
n!∞
logVn(q)
n
= H 0
provided q does not equal 0 or 1.
These results show that for large n there is a rather well-defined volume (at least in the logarithmic sense)
of high probability, and that within this volume the probability density is relatively uniform (again in the
logarithmic sense).
In the white noise case the distribution function is given by
p(x1; : : : ;xn) =
1
(2N)n=2
exp  1
2N ∑x
2
i :
Since this depends only on ∑x2i the surfaces of equal probability density are spheres and the entire distri-
bution has spherical symmetry. The region of high probability is a sphere of radius
p
nN. As n ! ∞ the
probability of being outside a sphere of radius
p
n(N + ) approaches zero and 1n times the logarithm of the
volume of the sphere approaches log
p
2eN.
In the continuous case it is convenient to work not with the entropy H of an ensemble but with a derived
quantity which we will call the entropy power. This is defined as the power in a white noise limited to the
same band as the original ensemble and having the same entropy. In other words if H 0 is the entropy of an
ensemble its entropy power is
N1 =
1
2e
exp2H 0:
In the geometrical picture this amounts to measuring the high probability volume by the squared radius of a
sphere having the same volume. Since white noise has the maximum entropy for a given power, the entropy
power of any noise is less than or equal to its actual power.
22. ENTROPY LOSS IN LINEAR FILTERS
Theorem 14: If an ensemble having an entropy H1 per degree of freedom in band W is passed through a
filter with characteristic Y ( f ) the output ensemble has an entropy
H2 = H1 +
1
W
Z
W
log jY ( f )j2 d f :
The operation of the filter is essentially a linear transformation of coordinates. If we think of the different
frequency components as the original coordinate system, the new frequency components are merely the old
ones multiplied by factors. The coordinate transformation matrix is thus essentially diagonalized in terms
of these coordinates. The Jacobian of the transformation is (for n sine and n cosine components)
J =
n
∏
i=1
jY ( fi)j2
where the fi are equally spaced through the band W . This becomes in the limit
exp
1
W
Z
W
log jY ( f )j2 d f :
Since J is constant its average value is the same quantity and applying the theorem on the change of entropy
with a change of coordinates, the result follows. We may also phrase it in terms of the entropy power. Thus
if the entropy power of the first ensemble is N1 that of the second is
N1 exp
1
W
Z
W
log jY ( f )j2 d f :
39
TABLE I
ENTROPY ENTROPY
GAIN POWER POWER GAIN IMPULSE RESPONSE
FACTOR IN DECIBELS
0 1!
1
1 ! 1
e2
 8:69 sin
2(t=2)
t2=2
0 1!
1
1 !2 2
e
4
 5:33 2

sin t
t3
  cos t
t2

0 1!
1
1 !3
0:411  3:87 6

cos t 1
t4
  cos t
2t2
+
sin t
t3

0 1!
1
p
1 !2 2
e
2
 2:67 
2
J1(t)
t
0 1!
1

1
e2
 8:69 1
t2

cos(1 )t  cos t

The final entropy power is the initial entropy power multiplied by the geometric mean gain of the filter. If
the gain is measured in db, then the output entropy power will be increased by the arithmetic mean db gain
over W .
In Table I the entropy power loss has been calculated (and also expressed in db) for a number of ideal
gain characteristics. The impulsive responses of these filters are also given for W = 2, with phase assumed
to be 0.
The entropy loss for many other cases can be obtained from these results. For example the entropy
power factor 1=e2 for the first case also applies to any gain characteristic obtain from 1 ! by a measure
preserving transformation of the ! axis. In particular a linearly increasing gain G(!) = !, or a “saw tooth”
characteristic between 0 and 1 have the same entropy loss. The reciprocal gain has the reciprocal factor.
Thus 1=! has the factor e2. Raising the gain to any power raises the factor to this power.
23. ENTROPY OF A SUM OF TWO ENSEMBLES
If we have two ensembles of functions f(t) and g(t) we can form a new ensemble by “addition.” Suppose
the first ensemble has the probability density function p(x1; : : : ;xn) and the second q(x1; : : : ;xn). Then the
40
density function for the sum is given by the convolution:
r(x1; : : : ;xn) =
Z
  
Z
p(y1; : : : ;yn)q(x1  y1; : : : ;xn  yn)dy1   dyn:
Physically this corresponds to adding the noises or signals represented by the original ensembles of func-
tions.
The following result is derived in Appendix 6.
Theorem 15: Let the average power of two ensembles be N1 and N2 and let their entropy powers be N1
and N2. Then the entropy power of the sum, N3, is bounded by
N1 +N2  N3  N1 +N2:
White Gaussian noise has the peculiar property that it can absorb any other noise or signal ensemble
which may be added to it with a resultant entropy power approximately equal to the sum of the white noise
power and the signal power (measured from the average signal value, which is normally zero), provided the
signal power is small, in a certain sense, compared to noise.
Consider the function space associated with these ensembles having n dimensions. The white noise
corresponds to the spherical Gaussian distribution in this space. The signal ensemble corresponds to another
probability distribution, not necessarily Gaussian or spherical. Let the second moments of this distribution
about its center of gravity be ai j. That is, if p(x1; : : : ;xn) is the density distribution function
ai j =
Z
  
Z
p(xi i)(x j  j)dx1   dxn
where the i are the coordinates of the center of gravity. Now ai j is a positive definite quadratic form, and
we can rotate our coordinate system to align it with the principal directions of this form. ai j is then reduced
to diagonal form bii. We require that each bii be small compared to N, the squared radius of the spherical
distribution.
In this case the convolution of the noise and signal produce approximately a Gaussian distribution whose
corresponding quadratic form is
N +bii:
The entropy power of this distribution is h
∏(N +bii)
i1=n
or approximately
=
h
(N)n +∑bii(N)n 1
i1=n
:
= N +
1
n ∑bii:
The last term is the signal power, while the first is the noise power.
PART IV: THE CONTINUOUS CHANNEL
24. THE CAPACITY OF A CONTINUOUS CHANNEL
In a continuous channel the input or transmitted signals will be continuous functions of time f (t) belonging
to a certain set, and the output or received signals will be perturbed versions of these. We will consider
only the case where both transmitted and received signals are limited to a certain band W . They can then
be specified, for a time T , by 2TW numbers, and their statistical structure by finite dimensional distribution
functions. Thus the statistics of the transmitted signal will be determined by
P(x1; : : : ;xn) = P(x)
41
and those of the noise by the conditional probability distribution
Px1;:::;xn(y1; : : : ;yn) = Px(y):
The rate of transmission of information for a continuous channel is defined in a way analogous to that
for a discrete channel, namely
R = H(x) Hy(x)
where H(x) is the entropy of the input and Hy(x) the equivocation. The channel capacity C is defined as the
maximum of R when we vary the input over all possible ensembles. This means that in a finite dimensional
approximation we must vary P(x) = P(x1; : : : ;xn) and maximize
 
Z
P(x) logP(x)dx+
ZZ
P(x;y) log
P(x;y)
P(y)
dxdy:
This can be written ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy
using the fact that
ZZ
P(x;y) logP(x)dxdy =
Z
P(x) logP(x)dx. The channel capacity is thus expressed as
follows:
C = Lim
T!∞
Max
P(x)
1
T
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy:
It is obvious in this form that R and C are independent of the coordinate system since the numerator
and denominator in log
P(x;y)
P(x)P(y)
will be multiplied by the same factors when x and y are transformed in
any one-to-one way. This integral expression for C is more general than H(x) Hy(x). Properly interpreted
(see Appendix 7) it will always exist while H(x) Hy(x) may assume an indeterminate form ∞ ∞ in some
cases. This occurs, for example, if x is limited to a surface of fewer dimensions than n in its n dimensional
approximation.
If the logarithmic base used in computing H(x) and Hy(x) is two then C is the maximum number of
binary digits that can be sent per second over the channel with arbitrarily small equivocation, just as in
the discrete case. This can be seen physically by dividing the space of signals into a large number of
small cells, sufficiently small so that the probability density Px(y) of signal x being perturbed to point y is
substantially constant over a cell (either of x or y). If the cells are considered as distinct points the situation is
essentially the same as a discrete channel and the proofs used there will apply. But it is clear physically that
this quantizing of the volume into individual points cannot in any practical situation alter the final answer
significantly, provided the regions are sufficiently small. Thus the capacity will be the limit of the capacities
for the discrete subdivisions and this is just the continuous capacity defined above.
On the mathematical side it can be shown first (see Appendix 7) that if u is the message, x is the signal,
y is the received signal (perturbed by noise) and v is the recovered message then
H(x) Hy(x)H(u) Hv(u)
regardless of what operations are performed on u to obtain x or on y to obtain v. Thus no matter how we
encode the binary digits to obtain the signal, or how we decode the received signal to recover the message,
the discrete rate for the binary digits does not exceed the channel capacity we have defined. On the other
hand, it is possible under very general conditions to find a coding system for transmitting binary digits at the
rate C with as small an equivocation or frequency of errors as desired. This is true, for example, if, when we
take a finite dimensional approximating space for the signal functions, P(x;y) is continuous in both x and y
except at a set of points of probability zero.
An important special case occurs when the noise is added to the signal and is independent of it (in the
probability sense). Then Px(y) is a function only of the difference n = (y  x),
Px(y) = Q(y  x)
42
and we can assign a definite entropy to the noise (independent of the statistics of the signal), namely the
entropy of the distribution Q(n). This entropy will be denoted by H(n).
Theorem 16: If the signal and noise are independent and the received signal is the sum of the transmitted
signal and the noise then the rate of transmission is
R = H(y) H(n);
i.e., the entropy of the received signal less the entropy of the noise. The channel capacity is
C = Max
P(x)
H(y) H(n):
We have, since y = x+n:
H(x;y) = H(x;n):
Expanding the left side and using the fact that x and n are independent
H(y)+Hy(x) = H(x)+H(n):
Hence
R = H(x) Hy(x) = H(y) H(n):
Since H(n) is independent of P(x), maximizing R requires maximizing H(y), the entropy of the received
signal. If there are certain constraints on the ensemble of transmitted signals, the entropy of the received
signal must be maximized subject to these constraints.
25. CHANNEL CAPACITY WITH AN AVERAGE POWER LIMITATION
A simple application of Theorem 16 is the case when the noise is a white thermal noise and the transmitted
signals are limited to a certain average power P. Then the received signals have an average power P+N
where N is the average noise power. The maximum entropy for the received signals occurs when they also
form a white noise ensemble since this is the greatest possible entropy for a power P+N and can be obtained
by a suitable choice of transmitted signals, namely if they form a white noise ensemble of power P. The
entropy (per second) of the received ensemble is then
H(y) =W log2e(P+N);
and the noise entropy is
H(n) =W log2eN:
The channel capacity is
C = H(y) H(n) =W log P+N
N
:
Summarizing we have the following:
Theorem 17: The capacity of a channel of band W perturbed by white thermal noise power N when the
average transmitter power is limited to P is given by
C =W log
P+N
N
:
This means that by sufficiently involved encoding systems we can transmit binary digits at the rate
W log2
P+N
N
bits per second, with arbitrarily small frequency of errors. It is not possible to transmit at a
higher rate by any encoding system without a definite positive frequency of errors.
To approximate this limiting rate of transmission the transmitted signals must approximate, in statistical
properties, a white noise.6 A system which approaches the ideal rate may be described as follows: Let
6This and other properties of the white noise case are discussed from the geometrical point of view in “Communication in the
Presence of Noise,” loc. cit.
43
M = 2s samples of white noise be constructed each of duration T . These are assigned binary numbers from
0 to M  1. At the transmitter the message sequences are broken up into groups of s and for each group
the corresponding noise sample is transmitted as the signal. At the receiver the M samples are known and
the actual received signal (perturbed by noise) is compared with each of them. The sample which has the
least R.M.S. discrepancy from the received signal is chosen as the transmitted signal and the corresponding
binary number reconstructed. This process amounts to choosing the most probable (a posteriori) signal.
The number M of noise samples used will depend on the tolerable frequency  of errors, but for almost all
selections of samples we have
Lim
!0
Lim
T!∞
logM(;T )
T
=W log
P+N
N
;
so that no matter how small  is chosen, we can, by taking T sufficiently large, transmit as near as we wish
to TW log
P+N
N
binary digits in the time T .
Formulas similar to C = W log
P+N
N
for the white noise case have been developed independently
by several other writers, although with somewhat different interpretations. We may mention the work of
N. Wiener,7 W. G. Tuller,8 and H. Sullivan in this connection.
In the case of an arbitrary perturbing noise (not necessarily white thermal noise) it does not appear that
the maximizing problem involved in determining the channel capacity C can be solved explicitly. However,
upper and lower bounds can be set for C in terms of the average noise power N the noise entropy power N1.
These bounds are sufficiently close together in most practical cases to furnish a satisfactory solution to the
problem.
Theorem 18: The capacity of a channel of band W perturbed by an arbitrary noise is bounded by the
inequalities
W log
P+N1
N1
C W log P+N
N1
where
P = average transmitter power
N = average noise power
N1 = entropy power of the noise.
Here again the average power of the perturbed signals will be P+N. The maximum entropy for this
power would occur if the received signal were white noise and would be W log2e(P+N). It may not
be possible to achieve this; i.e., there may not be any ensemble of transmitted signals which, added to the
perturbing noise, produce a white thermal noise at the receiver, but at least this sets an upper bound to H(y).
We have, therefore
C = MaxH(y) H(n)
W log2e(P+N) W log2eN1:
This is the upper limit given in the theorem. The lower limit can be obtained by considering the rate if we
make the transmitted signal a white noise, of power P. In this case the entropy power of the received signal
must be at least as great as that of a white noise of power P+N1 since we have shown in in a previous
theorem that the entropy power of the sum of two ensembles is greater than or equal to the sum of the
individual entropy powers. Hence
MaxH(y)W log2e(P+N1)
7Cybernetics, loc. cit.
8“Theoretical Limitations on the Rate of Transmission of Information,” Proceedings of the Institute of Radio Engineers, v. 37,
No. 5, May, 1949, pp. 468–78.
44
and
CW log2e(P+N1) W log2eN1
=W log
P+N1
N1
:
As P increases, the upper and lower bounds approach each other, so we have as an asymptotic rate
W log
P+N
N1
:
If the noise is itself white, N = N1 and the result reduces to the formula proved previously:
C =W log

1+
P
N

:
If the noise is Gaussian but with a spectrum which is not necessarily flat, N1 is the geometric mean of
the noise power over the various frequencies in the band W . Thus
N1 = exp
1
W
Z
W
logN( f )d f
where N( f ) is the noise power at frequency f .
Theorem 19: If we set the capacity for a given transmitter power P equal to
C =W log
P+N 
N1
then  is monotonic decreasing as P increases and approaches 0 as a limit.
Suppose that for a given power P1 the channel capacity is
W log
P1 +N 1
N1
:
This means that the best signal distribution, say p(x), when added to the noise distribution q(x), gives a
received distribution r(y) whose entropy power is (P1 +N  1). Let us increase the power to P1 +P by
adding a white noise of power P to the signal. The entropy of the received signal is now at least
H(y) =W log2e(P1 +N 1 +P)
by application of the theorem on the minimum entropy power of a sum. Hence, since we can attain the
H indicated, the entropy of the maximizing distribution must be at least as great and  must be monotonic
decreasing. To show that ! 0 as P! ∞ consider a signal which is white noise with a large P. Whatever
the perturbing noise, the received signal will be approximately a white noise, if P is sufficiently large, in the
sense of having an entropy power approaching P+N.
26. THE CHANNEL CAPACITY WITH A PEAK POWER LIMITATION
In some applications the transmitter is limited not by the average power output but by the peak instantaneous
power. The problem of calculating the channel capacity is then that of maximizing (by variation of the
ensemble of transmitted symbols)
H(y) H(n)
subject to the constraint that all the functions f (t) in the ensemble be less than or equal to
p
S, say, for all
t. A constraint of this type does not work out as well mathematically as the average power limitation. The
most we have obtained for this case is a lower bound valid for all
S
N
, an “asymptotic” upper bound (valid
for large
S
N
) and an asymptotic value of C for
S
N
small.
45
Theorem 20: The channel capacity C for a band W perturbed by white thermal noise of power N is
bounded by
C W log 2
e3
S
N
;
where S is the peak allowed transmitter power. For sufficiently large
S
N
C W log
2
e S+N
N
(1+ )
where  is arbitrarily small. As
S
N
! 0 (and provided the band W starts at 0)
C
.
W log

1+
S
N

! 1:
We wish to maximize the entropy of the received signal. If
S
N
is large this will occur very nearly when
we maximize the entropy of the transmitted ensemble.
The asymptotic upper bound is obtained by relaxing the conditions on the ensemble. Let us suppose that
the power is limited to S not at every instant of time, but only at the sample points. The maximum entropy of
the transmitted ensemble under these weakened conditions is certainly greater than or equal to that under the
original conditions. This altered problem can be solved easily. The maximum entropy occurs if the different
samples are independent and have a distribution function which is constant from  pS to +pS. The entropy
can be calculated as
W log4S:
The received signal will then have an entropy less than
W log(4S+2eN)(1+ )
with ! 0 as S
N
! ∞ and the channel capacity is obtained by subtracting the entropy of the white noise,
W log2eN:
W log(4S+2eN)(1+ ) W log(2eN) =W log
2
e S+N
N
(1+ ):
This is the desired upper bound to the channel capacity.
To obtain a lower bound consider the same ensemble of functions. Let these functions be passed through
an ideal filter with a triangular transfer characteristic. The gain is to be unity at frequency 0 and decline
linearly down to gain 0 at frequency W . We first show that the output functions of the filter have a peak
power limitation S at all times (not just the sample points). First we note that a pulse
sin2Wt
2Wt
going into
the filter produces
1
2
sin2Wt
(Wt)2
in the output. This function is never negative. The input function (in the general case) can be thought of as
the sum of a series of shifted functions
a
sin2Wt
2Wt
where a, the amplitude of the sample, is not greater than
p
S. Hence the output is the sum of shifted functions
of the non-negative form above with the same coefficients. These functions being non-negative, the greatest
positive value for any t is obtained when all the coefficients a have their maximum positive values, i.e.,
p
S.
In this case the input function was a constant of amplitude
p
S and since the filter has unit gain for D.C., the
output is the same. Hence the output ensemble has a peak power S.
46
The entropy of the output ensemble can be calculated from that of the input ensemble by using the
theorem dealing with such a situation. The output entropy is equal to the input entropy plus the geometrical
mean gain of the filter: Z W
0
logG2 d f =
Z W
0
log
W   f
W
2
d f = 2W:
Hence the output entropy is
W log4S 2W =W log 4S
e2
and the channel capacity is greater than
W log
2
e3
S
N
:
We now wish to show that, for small
S
N
(peak signal power over average white noise power), the channel
capacity is approximately
C =W log

1+
S
N

:
More precisely C
.
W log

1+
S
N

! 1 as S
N
! 0. Since the average signal power P is less than or equal
to the peak S, it follows that for all
S
N
C W log

1+
P
N

W log

1+
S
N

:
Therefore, if we can find an ensemble of functions such that they correspond to a rate nearly W log

1+
S
N

and are limited to band W and peak S the result will be proved. Consider the ensemble of functions of the
following type. A series of t samples have the same value, either +
p
S or pS, then the next t samples have
the same value, etc. The value for a series is chosen at random, probability 12 for +
p
S and 12 for  
p
S. If
this ensemble be passed through a filter with triangular gain characteristic (unit gain at D.C.), the output is
peak limited to S. Furthermore the average power is nearly S and can be made to approach this by taking t
sufficiently large. The entropy of the sum of this and the thermal noise can be found by applying the theorem
on the sum of a noise and a small signal. This theorem will apply if
p
t
S
N
is sufficiently small. This can be ensured by taking
S
N
small enough (after t is chosen). The entropy power
will be S+N to as close an approximation as desired, and hence the rate of transmission as near as we wish
to
W log

S+N
N

:
PART V: THE RATE FOR A CONTINUOUS SOURCE
27. FIDELITY EVALUATION FUNCTIONS
In the case of a discrete source of information we were able to determine a definite rate of generating
information, namely the entropy of the underlying stochastic process. With a continuous source the situation
is considerably more involved. In the first place a continuously variable quantity can assume an infinite
number of values and requires, therefore, an infinite number of binary digits for exact specification. This
means that to transmit the output of a continuous source with exact recovery at the receiving point requires,
47
in general, a channel of infinite capacity (in bits per second). Since, ordinarily, channels have a certain
amount of noise, and therefore a finite capacity, exact transmission is impossible.
This, however, evades the real issue. Practically, we are not interested in exact transmission when we
have a continuous source, but only in transmission to within a certain tolerance. The question is, can we
assign a definite rate to a continuous source when we require only a certain fidelity of recovery, measured in
a suitable way. Of course, as the fidelity requirements are increased the rate will increase. It will be shown
that we can, in very general cases, define such a rate, having the property that it is possible, by properly
encoding the information, to transmit it over a channel whose capacity is equal to the rate in question, and
satisfy the fidelity requirements. A channel of smaller capacity is insufficient.
It is first necessary to give a general mathematical formulation of the idea of fidelity of transmission.
Consider the set of messages of a long duration, say T seconds. The source is described by giving the
probability density, in the associated space, that the source will select the message in question P(x). A given
communication system is described (from the external point of view) by giving the conditional probability
Px(y) that if message x is produced by the source the recovered message at the receiving point will be y. The
system as a whole (including source and transmission system) is described by the probability function P(x;y)
of having message x and final output y. If this function is known, the complete characteristics of the system
from the point of view of fidelity are known. Any evaluation of fidelity must correspond mathematically
to an operation applied to P(x;y). This operation must at least have the properties of a simple ordering of
systems; i.e., it must be possible to say of two systems represented by P1(x;y) and P2(x;y) that, according to
our fidelity criterion, either (1) the first has higher fidelity, (2) the second has higher fidelity, or (3) they have
equal fidelity. This means that a criterion of fidelity can be represented by a numerically valued function:
v
 
P(x;y)

whose argument ranges over possible probability functions P(x;y).
We will now show that under very general and reasonable assumptions the function v
 
P(x;y)

can be
written in a seemingly much more specialized form, namely as an average of a function (x;y) over the set
of possible values of x and y:
v
 
P(x;y)

=
ZZ
P(x;y)(x;y)dxdy:
To obtain this we need only assume (1) that the source and system are ergodic so that a very long sample
will be, with probability nearly 1, typical of the ensemble, and (2) that the evaluation is “reasonable” in the
sense that it is possible, by observing a typical input and output x1 and y1, to form a tentative evaluation
on the basis of these samples; and if these samples are increased in duration the tentative evaluation will,
with probability 1, approach the exact evaluation based on a full knowledge of P(x;y). Let the tentative
evaluation be (x;y). Then the function (x;y) approaches (as T ! ∞) a constant for almost all (x;y) which
are in the high probability region corresponding to the system:
(x;y)! v P(x;y)
and we may also write
(x;y)!
ZZ
P(x;y)(x;y)dxdy
since ZZ
P(x;y)dxdy = 1:
This establishes the desired result.
The function (x;y) has the general nature of a “distance” between x and y.9 It measures how undesirable
it is (according to our fidelity criterion) to receive y when x is transmitted. The general result given above
can be restated as follows: Any reasonable evaluation can be represented as an average of a distance function
over the set of messages and recovered messages x and y weighted according to the probability P(x;y) of
getting the pair in question, provided the duration T of the messages be taken sufficiently large.
The following are simple examples of evaluation functions:
9It is not a “metric” in the strict sense, however, since in general it does not satisfy either (x;y) = (y;x) or (x;y)+(y;z) (x;z).
48
1. R.M.S. criterion.
v =
 
x(t)  y(t)2:
In this very commonly used measure of fidelity the distance function (x;y) is (apart from a constant
factor) the square of the ordinary Euclidean distance between the points x and y in the associated
function space.
(x;y) =
1
T
Z T
0

x(t)  y(t)2 dt:
2. Frequency weighted R.M.S. criterion. More generally one can apply different weights to the different
frequency components before using an R.M.S. measure of fidelity. This is equivalent to passing the
difference x(t)  y(t) through a shaping filter and then determining the average power in the output.
Thus let
e(t) = x(t)  y(t)
and
f (t) =
Z ∞
 ∞
e()k(t  )d
then
(x;y) =
1
T
Z T
0
f (t)2 dt:
3. Absolute error criterion.
(x;y) =
1
T
Z T
0
x(t)  y(t)dt:
4. The structure of the ear and brain determine implicitly an evaluation, or rather a number of evaluations,
appropriate in the case of speech or music transmission. There is, for example, an “intelligibility”
criterion in which (x;y) is equal to the relative frequency of incorrectly interpreted words when
message x(t) is received as y(t). Although we cannot give an explicit representation of (x;y) in these
cases it could, in principle, be determined by sufficient experimentation. Some of its properties follow
from well-known experimental results in hearing, e.g., the ear is relatively insensitive to phase and the
sensitivity to amplitude and frequency is roughly logarithmic.
5. The discrete case can be considered as a specialization in which we have tacitly assumed an evaluation
based on the frequency of errors. The function (x;y) is then defined as the number of symbols in the
sequence y differing from the corresponding symbols in x divided by the total number of symbols in
x.
28. THE RATE FOR A SOURCE RELATIVE TO A FIDELITY EVALUATION
We are now in a position to define a rate of generating information for a continuous source. We are given
P(x) for the source and an evaluation v determined by a distance function (x;y) which will be assumed
continuous in both x and y. With a particular system P(x;y) the quality is measured by
v =
ZZ
(x;y)P(x;y)dxdy:
Furthermore the rate of flow of binary digits corresponding to P(x;y) is
R =
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy:
We define the rate R1 of generating information for a given quality v1 of reproduction to be the minimum of
R when we keep v fixed at v1 and vary Px(y). That is:
R1 = Min
Px(y)
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy
49
subject to the constraint:
v1 =
ZZ
P(x;y)(x;y)dxdy:
This means that we consider, in effect, all the communication systems that might be used and that
transmit with the required fidelity. The rate of transmission in bits per second is calculated for each one
and we choose that having the least rate. This latter rate is the rate we assign the source for the fidelity in
question.
The justification of this definition lies in the following result:
Theorem 21: If a source has a rate R1 for a valuation v1 it is possible to encode the output of the source
and transmit it over a channel of capacity C with fidelity as near v1 as desired provided R1 C. This is not
possible if R1 >C.
The last statement in the theorem follows immediately from the definition of R1 and previous results. If
it were not true we could transmit more than C bits per second over a channel of capacity C. The first part
of the theorem is proved by a method analogous to that used for Theorem 11. We may, in the first place,
divide the (x;y) space into a large number of small cells and represent the situation as a discrete case. This
will not change the evaluation function by more than an arbitrarily small amount (when the cells are very
small) because of the continuity assumed for (x;y). Suppose that P1(x;y) is the particular system which
minimizes the rate and gives R1. We choose from the high probability y’s a set at random containing
2(R1+)T
members where ! 0 as T ! ∞. With large T each chosen point will be connected by a high probability
line (as in Fig. 10) to a set of x’s. A calculation similar to that used in proving Theorem 11 shows that with
large T almost all x’s are covered by the fans from the chosen y points for almost all choices of the y’s. The
communication system to be used operates as follows: The selected points are assigned binary numbers.
When a message x is originated it will (with probability approaching 1 as T ! ∞) lie within at least one
of the fans. The corresponding binary number is transmitted (or one of them chosen arbitrarily if there are
several) over the channel by suitable coding means to give a small probability of error. Since R1 C this is
possible. At the receiving point the corresponding y is reconstructed and used as the recovered message.
The evaluation v01 for this system can be made arbitrarily close to v1 by taking T sufficiently large.
This is due to the fact that for each long sample of message x(t) and recovered message y(t) the evaluation
approaches v1 (with probability 1).
It is interesting to note that, in this system, the noise in the recovered message is actually produced by a
kind of general quantizing at the transmitter and not produced by the noise in the channel. It is more or less
analogous to the quantizing noise in PCM.
29. THE CALCULATION OF RATES
The definition of the rate is similar in many respects to the definition of channel capacity. In the former
R = Min
Px(y)
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy
with P(x) and v1 =
ZZ
P(x;y)(x;y)dxdy fixed. In the latter
C = Max
P(x)
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy
with Px(y) fixed and possibly one or more other constraints (e.g., an average power limitation) of the form
K =
RR
P(x;y)(x;y)dxdy.
A partial solution of the general maximizing problem for determining the rate of a source can be given.
Using Lagrange’s method we consider
ZZ 
P(x;y) log
P(x;y)
P(x)P(y)
+P(x;y)(x;y)+(x)P(x;y)

dxdy:
50
The variational equation (when we take the first variation on P(x;y)) leads to
Py(x) = B(x)e
 (x;y)
where  is determined to give the required fidelity and B(x) is chosen to satisfy
Z
B(x)e (x;y) dx = 1:
This shows that, with best encoding, the conditional probability of a certain cause for various received
y, Py(x) will decline exponentially with the distance function (x;y) between the x and y in question.
In the special case where the distance function (x;y) depends only on the (vector) difference between x
and y,
(x;y) = (x  y)
we have Z
B(x)e (x y) dx = 1:
Hence B(x) is constant, say , and
Py(x) = e
 (x y):
Unfortunately these formal solutions are difficult to evaluate in particular cases and seem to be of little value.
In fact, the actual calculation of rates has been carried out in only a few very simple cases.
If the distance function (x;y) is the mean square discrepancy between x and y and the message ensemble
is white noise, the rate can be determined. In that case we have
R = Min

H(x) Hy(x)

= H(x) MaxHy(x)
with N = (x  y)2. But the MaxHy(x) occurs when y x is a white noise, and is equal to W1 log2eN where
W1 is the bandwidth of the message ensemble. Therefore
R =W1 log2eQ W1 log2eN
=W1 log
Q
N
where Q is the average message power. This proves the following:
Theorem 22: The rate for a white noise source of power Q and band W1 relative to an R.M.S. measure
of fidelity is
R =W1 log
Q
N
where N is the allowed mean square error between original and recovered messages.
More generally with any message source we can obtain inequalities bounding the rate relative to a mean
square error criterion.
Theorem 23: The rate for any source of band W1 is bounded by
W1 log
Q1
N
 RW1 log QN
where Q is the average power of the source, Q1 its entropy power and N the allowed mean square error.
The lower bound follows from the fact that the MaxHy(x) for a given (x  y)2 = N occurs in the white
noise case. The upper bound results if we place points (used in the proof of Theorem 21) not in the best way
but at random in a sphere of radius
p
Q N.
51
ACKNOWLEDGMENTS
The writer is indebted to his colleagues at the Laboratories, particularly to Dr. H. W. Bode, Dr. J. R. Pierce,
Dr. B. McMillan, and Dr. B. M. Oliver for many helpful suggestions and criticisms during the course of this
work. Credit should also be given to Professor N. Wiener, whose elegant solution of the problems of filtering
and prediction of stationary ensembles has considerably influenced the writer’s thinking in this field.
APPENDIX 5
Let S1 be any measurable subset of the g ensemble, and S2 the subset of the f ensemble which gives S1
under the operation T . Then
S1 = TS2:
Let H be the operator which shifts all functions in a set by the time . Then
HS1 = H
TS2 = T H
S2
since T is invariant and therefore commutes with H. Hence if m[S] is the probability measure of the set S
m[HS1] = m[TH
S2] = m[H
S2]
= m[S2] = m[S1]
where the second equality is by definition of measure in the g space, the third since the f ensemble is
stationary, and the last by definition of g measure again.
To prove that the ergodic property is preserved under invariant operations, let S1 be a subset of the g
ensemble which is invariant under H, and let S2 be the set of all functions f which transform into S1. Then
HS1 = H
TS2 = T H
S2 = S1
so that HS2 is included in S2 for all . Now, since
m[HS2] = m[S1]
this implies
HS2 = S2
for all  with m[S2] 6= 0;1. This contradiction shows that S1 does not exist.
APPENDIX 6
The upper bound, N3  N1 +N2, is due to the fact that the maximum possible entropy for a power N1 +N2
occurs when we have a white noise of this power. In this case the entropy power is N1 +N2.
To obtain the lower bound, suppose we have two distributions in n dimensions p(xi) and q(xi) with
entropy powers N1 and N2. What form should p and q have to minimize the entropy power N3 of their
convolution r(xi):
r(xi) =
Z
p(yi)q(xi  yi)dyi:
The entropy H3 of r is given by
H3 = 
Z
r(xi) logr(xi)dxi:
We wish to minimize this subject to the constraints
H1 = 
Z
p(xi) log p(xi)dxi
H2 = 
Z
q(xi) logq(xi)dxi:
52
We consider then
U = 
Z 
r(x) log r(x)+p(x) log p(x)+q(x) logq(x)

dx
U = 
Z 
[1+ logr(x)]r(x)+[1+ log p(x)]p(x)+[1+ logq(x)]q(x)

dx:
If p(x) is varied at a particular argument xi = si, the variation in r(x) is
r(x) = q(xi  si)
and
U = 
Z
q(xi  si) logr(xi)dxi  log p(si) = 0
and similarly when q is varied. Hence the conditions for a minimum are
Z
q(xi  si) logr(xi)dxi =  log p(si)
Z
p(xi  si) logr(xi)dxi =  logq(si):
If we multiply the first by p(si) and the second by q(si) and integrate with respect to si we obtain
H3 = H1
H3 = H2
or solving for  and  and replacing in the equations
H1
Z
q(xi  si) logr(xi)dxi = H3 log p(si)
H2
Z
p(xi  si) logr(xi)dxi = H3 logq(si):
Now suppose p(xi) and q(xi) are normal
p(xi) =
jAi jjn=2
(2)n=2
exp  12 ∑Ai jxix j
q(xi) =
jBi jjn=2
(2)n=2
exp  12 ∑Bi jxix j:
Then r(xi) will also be normal with quadratic form Ci j. If the inverses of these forms are ai j, bi j, ci j then
ci j = ai j +bi j:
We wish to show that these functions satisfy the minimizing conditions if and only if ai j = Kbi j and thus
give the minimum H3 under the constraints. First we have
logr(xi) =
n
2
log
1
2
jCi jj  12 ∑Ci jxix jZ
q(xi  si) logr(xi)dxi = n2 log
1
2
jCi jj  12 ∑Ci jsis j  12 ∑Ci jbi j:
This should equal
H3
H1

n
2
log
1
2
jAi jj  12 ∑Ai jsis j

which requires Ai j =
H1
H3
Ci j. In this case Ai j =
H1
H2
Bi j and both equations reduce to identities.
53
APPENDIX 7
The following will indicate a more general and more rigorous approach to the central definitions of commu-
nication theory. Consider a probability measure space whose elements are ordered pairs (x;y). The variables
x, y are to be identified as the possible transmitted and received signals of some long duration T . Let us call
the set of all points whose x belongs to a subset S1 of x points the strip over S1, and similarly the set whose
y belong to S2 the strip over S2. We divide x and y into a collection of non-overlapping measurable subsets
Xi and Yi approximate to the rate of transmission R by
R1 =
1
T ∑i
P(Xi;Yi) log
P(Xi;Yi)
P(Xi)P(Yi)
where
P(Xi) is the probability measure of the strip over Xi
P(Yi) is the probability measure of the strip over Yi
P(Xi;Yi) is the probability measure of the intersection of the strips:
A further subdivision can never decrease R1. For let X1 be divided into X1 = X 01 +X
00
1 and let
P(Y1) = a P(X1) = b+ c
P(X 01) = b P(X
0
1;Y1) = d
P(X 001 ) = c P(X
00
1 ;Y1) = e
P(X1;Y1) = d + e:
Then in the sum we have replaced (for the X1, Y1 intersection)
(d + e) log
d + e
a(b+ c)
by d log
d
ab
+ e log
e
ac
:
It is easily shown that with the limitation we have on b, c, d, e,

d + e
b+ c
d+e
 d
dee
bdce
and consequently the sum is increased. Thus the various possible subdivisions form a directed set, with
R monotonic increasing with refinement of the subdivision. We may define R unambiguously as the least
upper bound for R1 and write it
R =
1
T
ZZ
P(x;y) log
P(x;y)
P(x)P(y)
dxdy:
This integral, understood in the above sense, includes both the continuous and discrete cases and of course
many others which cannot be represented in either form. It is trivial in this formulation that if x and u are
in one-to-one correspondence, the rate from u to y is equal to that from x to y. If v is any function of y (not
necessarily with an inverse) then the rate from x to y is greater than or equal to that from x to v since, in
the calculation of the approximations, the subdivisions of y are essentially a finer subdivision of those for
v. More generally if y and v are related not functionally but statistically, i.e., we have a probability measure
space (y;v), then R(x;v) R(x;y). This means that any operation applied to the received signal, even though
it involves statistical elements, does not increase R.
Another notion which should be defined precisely in an abstract formulation of the theory is that of
“dimension rate,” that is the average number of dimensions required per second to specify a member of
an ensemble. In the band limited case 2W numbers per second are sufficient. A general definition can be
framed as follows. Let f(t) be an ensemble of functions and let T [ f(t); f(t)] be a metric measuring
54
the “distance” from f to f over the time T (for example the R.M.S. discrepancy over this interval.) Let
N(;;T ) be the least number of elements f which can be chosen such that all elements of the ensemble
apart from a set of measure  are within the distance  of at least one of those chosen. Thus we are covering
the space to within  apart from a set of small measure . We define the dimension rate  for the ensemble
by the triple limit
= Lim
!0
Lim
!0
Lim
T!∞
logN(;;T )
T log
:
This is a generalization of the measure type definitions of dimension in topology, and agrees with the intu-
itive dimension rate for simple ensembles where the desired result is obvious.
55
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תכנה-וחומר-עזר\Hypotensive-Episode-Prediction-in-ICUs-via-Observation-Window-Splitting\Tsur2019.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Ulf Brefeld · Edward Curry
Elizabeth Daly · Brian MacNamee
Alice Marascu · Fabio Pinelli
Michele Berlingerio · Neil Hurley (Eds.)
 123
LN
AI
 1
10
53
European Conference, ECML PKDD 2018
Dublin, Ireland, September 10–14, 2018
Proceedings, Part III
Machine Learning and 
Knowledge Discovery 
in Databases
Hypotensive Episode Prediction in ICUs
via Observation Window Splitting
Elad Tsur1, Mark Last1(B), Victor F. Garcia2, Raphael Udassin3, Moti Klein4,
and Evgeni Brotfain4
1 Department of Software and Information Systems Engineering,
Ben-Gurion University of the Negev, 84105 Beer-Sheva, Israel
eladtsur@gmail.com, mlast@bgu.ac.il
2 Division of Pediatric Surgery, MLC 2023, Children’s Hospital Medical Center,
3333 Burnet Avenue, Cincinnati, OH 45229, USA
victor.garcia@cchmc.org
3 Pediatric Surgery Department, Hadassah University Hospital,
Ein-Karem, 9112001 Jerusalem, Israel
raphaelu@ekmd.huji.ac.il
4 General Intensive Care Unit, Soroka Medical Center, Beer Sheva, Israel
{MotiK,EvgeniBr}@clalit.org.il
Abstract. Hypotension, defined as dangerously low blood pressure, is
a significant risk factor in intensive care units (ICUs), which requires a
prompt therapeutic intervention. The goal of our research is to predict an
impending Hypotensive Episode (HE) by time series analysis of continu-
ously monitored physiological vital signs. Our prognostic model is based
on the last Observation Window (OW) at the prediction time. Exist-
ing clinical episode prediction studies used a single OW of 5–120 min
to extract predictive features, with no significant improvement reported
when longer OWs were used. In this work we have developed the In-
Window Segmentation (InWiSe) method for time series prediction, which
splits a single OW into several sub-windows of equal size. The resulting
feature set combines the features extracted from each observation sub-
window and then this combined set is used by the Extreme Gradient
Boosting (XGBoost) binary classifier to produce an episode prediction
model. We evaluate the proposed approach on three retrospective ICU
datasets (extracted from MIMIC II, Soroka and Hadassah databases)
using cross-validation on each dataset separately, as well as by cross-
dataset validation. The results show that InWiSe is superior to existing
methods in terms of the area under the ROC curve (AUC).
Keywords: Time series analysis · Clinical episode prediction
Feature extraction · Intensive care · Patient monitoring
Partially supported by the Cincinnati Children’s Hospital Medical Center; In collabo-
ration with Soroka Medical Center in Beer-Sheva and Hadassah University Hospital,
Ein Karem, Jerusalem
c© Springer Nature Switzerland AG 2019
U. Brefeld et al. (Eds.): ECML PKDD 2018, LNAI 11053, pp. 472–487, 2019.
https://doi.org/10.1007/978-3-030-10997-4_29
Hypotensive Episode Prediction in ICUs 473
1 Introduction
Hypotension is defined as dangerously low blood pressure. It is a major hemody-
namic instability symptom, as well as a significant risk factor in hospital mortal-
ity at intensive care units (ICUs) [1]. As a critical condition, which may result
in a fatal deterioration, an impending Hypotensive Episode (HE) requires a
prompt therapeutic intervention [2] by ICU clinicians. However, HE prediction
is a challenging task [3]. While the clinical staff time is limited, the amount
of accumulated physiologic data per patient is massive in terms of both data
variety (multi-channel waveforms, laboratory results, medication records, nurs-
ing notes, etc.) and data volume (length of waveform time series). Even with
sufficient time, resources, and data, it is very hard to accurately estimate the
likelihood of clinical deterioration with bare-eye analysis alone.
HE may be detectable in advance by automatic analysis of continuously
monitored physiologic data; more specifically, the analysis of vital signs (multi-
parameter temporal vital data), may inform on the underlying dynamics of
organs and cardiovascular system functioning. Particularly, vital signs may con-
tain subtle patterns which point to an impending instability [4]. Such pattern
identification is a suitable task for machine learning algorithms. Smart patient
monitoring software that could predict the clinical deterioration of high risk
patients well before there are changes in the parameters displayed by the cur-
rent ICU monitors would save lives, reduce hospitalization costs, and contribute
to better patient outcomes [5].
Our research goal is to give the physicians an early warning of an impending
HE by building a prediction model, which utilizes the maximal amount of infor-
mation from the currently available patient monitoring data and outperforms
state-of-the-art HE prediction systems. We present and evaluate the In-Window
Segmentation (InWiSe) algorithm for HE prediction, which extracts predictive
features from a set of multiple observation sub-windows rather than from a single
long observation window.
This paper is organized as follows. Section 2 surveys the previous works in
several related areas, elaborates on the limitations of these works and introduces
the contributions of our method. Section 3 describes the studied problem and
proposed methods in detail and Sect. 4 covers the results of an empirical evalu-
ation. Finally, Sect. 5 presents the conclusions along with possible directions for
future research.
2 Related Work and Original Contributions
Several works studied the problem of clinical deterioration prediction in ICUs.
This section reviews their problem definitions, feature extraction methods, slid-
ing window constellations, and prediction methodologies. Finally, a discussion
of the limitations of existing methods is followed by a presentation of the con-
tributions of this study.
474 E. Tsur et al.
2.1 Clinical Episode Definitions
Previous works on clinical deterioration prediction vary mainly in two aspects [6].
The first one is an episode definition, which may be based on the recorded clinical
treatment or on the behavior of vital signs within a specific time interval. The
second one is the warning time, a.k.a. the Gap Window, which will be called in
brief the gap in this study.
The objective in [3] was to predict the hemodynamic instability start time
with a 2-h gap. The episode start time was defined by a clinical intervention
recorded in the ICU clinical record of a patient. In [7], instability was also defined
by some given medications and gaps of 15 min to 12 h were explored.
The 10th annual PhysioNet/Computers in Cardiology Challenge [4] con-
ducted a competition to study an Acute Hypotensive Episode (AHE). They
defined AHE as an interval, in which at least 90% of the time the Mean Arterial
blood Pressure (MAP) is under 60 mmHg during any 30-min window within
the interval. Their goal was to predict whether an AHE will start in the next
60 min. In [1,8], the HE and AHE definitions were identical to [4], but a lower
MAP bound of 10 mmHg was added to prevent noise effects from outliers. Their
goal was to predict the patient condition in a Target Window (called herein
target) of 30 min, which occurs within a gap of 1–2 h (See Fig. 1a), and label it
as hypotensive or normotensive (normal blood pressure). As expected, and as
concluded in [8], the problem is more challenging when predicting further into
the future, thus resulting in poorer performance. Note that, as indicated in [8],
the accepted HE definitions for adults vary in the range of 60–80 mmHg MAP
for 30+ min, where the lowest case of 60 mmHg is sometimes excluded under
the definition of AHE [1,4].
2.2 Predictive Feature Types
In most works, the future episode predictive features are usually extracted from
a sliding Observation Window (OW) over a record, which is a collection of vital
sign time series of one patient in a single ICU admission. A minute-by-minute
vital signs time series, like blood pressure and Heart Rate (HR) are usually used
to extract features, while a few studies used the clinical information (age and
temporal medications data) as well. A typically used benchmark database is
MIMIC II [9], a multi-parameter ICU waveforms and clinical database.
Statistical features are the most obvious source for the extraction of predic-
tive features, also called patterns, from intervals like OWs. In [5], the authors
calculate extremes, moments, percentiles and inter-percentile ranges for every
vital sign, whereas in [8] interquartile ranges and slope are extracted as well.
In a more pragmatic statistical approach [10], several episode predictive indices
were used, derived from the blood pressure signals only. These indices were six
types of averages from Systolic Blood Pressure (SBP), Diastolic Blood Pressure
(DBP), and MAP, each taken as a single feature. Another statistical approach
derives cross-correlation features which capture the coupling between two time
Hypotensive Episode Prediction in ICUs 475
series by computing the sum of products of their values [8], or by estimating
their variance and covariance [5].
A more recent and widely accepted feature extraction approach is the use of
wavelets, which captures the relative energies in different spectral bands that are
localized in both time and frequency. Wavelets were proven to perform well as
episode predictors [11] as well as vital sign similarity detectors [12]. In [5] and [8],
Daubechies (DB) and Meyer wavelet types were used, respectively, noting that
the DB type dominates the basic Haar type wavelets [13] in terms of vital sign
time series, which are non-stationary [5].
Apart from vital signs, patient age and vasopressors (blood pressure medica-
tions) given during OWs are added as features by Lee and Mark [8] but found
to have low correlation with the target. In their other work [15], they achieve
similar results without those features. Moreover, Saeed [11] mentions the low
reliability of vasopressor medication timestamps, which are very important for
the episode prediction task.
2.3 Observation Window Constellations
The sliding OW plays an important role in the episode prediction task. In this
section, we survey the different approaches to constructing and collecting OWs.
The first important attribute of an OW is its duration. In [1,3,5,8,10,14],
various OW sizes were applied (5, 10, 30, 60, 90, and 120 min). Having imple-
mented a 60-min OW, it is claimed in [1,8] that extracting features from a longer
window does not result in improvement of prediction performance.
In [15], Lee and Mark extended their previous work by implementing a
weighted decision classifier that consists of four base classifiers, each predict-
ing an HE in the same target but within different gaps (1, 2, 3 and 4 h) using
a different corresponding 30-min OW. The final decision is made by weighting
the four posterior probabilities from each classifier. They report insignificant
improvement in prediction performance as well as independency of predictions
from the 3rd and the 4th past hours (the earliest hours).
A second matter is how to collect OWs for training the prediction model.
One simple approach, applied by Cao et al. [3], is to compile an OW ending
gap-minutes before the start time of the first episode in every unstable record
(having one or more HEs). For each stable record, one or more OWs are then
sampled randomly. According to [8], collecting multiple OWs from both stable
and unstable records in a random fashion, which does not collect windows exactly
gap-minutes before an episode start, is proved to outperform the first method of
Cao [3]. A sliding target window (with no overlap) traverses each record (Fig. 1a),
and as many OWs as possible are compiled. However, they note that collecting
OWs all the time, even when no HE is impending, and doing it from both stable
and unstable patients will result in an extremely imbalanced dataset. Having
two OW classes (hypotensive or normotensive), one way to solve this issue is by
undersampling the majority class (normotensive) [5,8].
476 E. Tsur et al.
2.4 Prediction Methodology
The problem of episode prediction is most logically approached by calculating
the probability of a future episode at a given time point, using features extracted
from the current OW, and then classifying the target window, which starts within
some gap-minutes, as hypotensive or not, based on a pre-defined probability
threshold. Multiple works tackled this problem by using numerous supervised
machine learning algorithms, particularly binary classifiers, with some exceptions
such as in [16], where a Recurrent Neural Networks approach is used to forecast
the target window MAP values, followed by a straightforward binary decision
based on the episode definition.
The classifiers used by some other papers are Logistic Regression [3], Artificial
Neural Network [8,15], Majority Vote [1], and Random Forest [5]. To the best
of our knowledge, the most accurate HE prediction so far is reported in [8,15],
which we reproduce and use for comparison in Sect. 4.
2.5 Limitations of Published Methods
Advanced methods and evaluation schemes such as in [1,5,8,14,15], solved some
of the problems found in the early works [3,10], yet left some open issues, includ-
ing low precision (14% in [8]) and a strict episode definition that is still far from
the practical definitions used in ICUs. Moreover, a machine learning solution
to a high precision HE prediction will probably need much more training data,
while the current MIMIC II [9] contains only several thousands1 of vital sign
records that are matched with the clinical data.
Unfortunately, there is a lack of comprehensive public sources of ICU mon-
itored vital signs beyond the existing MIMIC database. Consequently, the cur-
rent episode prediction studies miss the crucial cross-dataset validation, which
is needed to find a generic model that should work for any ICU, disregarding
availability of retrospective patient records for training.
Recent papers [1,5,8] include predictions of future episodes even if the patient
is going through an ongoing episode. These predictions may be less relevant to
the physicians and possibly excluded from the evaluation metrics.
Finally, studies conducted over the last decade show no improvement in uti-
lizing OWs greater than 120 min (and usually even 60 min), implying there are
no additional predictive patterns to be found in the near past. On the contrary,
the results from [1,5,7,8,14] show an accuracy decrease of only 1–2.5% when
switching from a 60-min gap window to a 120-min one, which may imply that
earlier observations may have a just a slightly lower correlation to the target.
Thus, there may be additional predictive patterns, which are not utilized prop-
erly by the existing methods.
1 Recently, The MIMIC III waveform database Matched Subset, four times larger than
the MIMIC II subset, was published
Hypotensive Episode Prediction in ICUs 477
2.6 Original Contributions
The main contribution of this paper is the In-Window Segmentation (InWiSe)
method, which aims to utilize the local predictive patterns in long OWs. The
method, presented in Sect. 3.2 and Fig. 1b, differs from previous methods by the
following: (i) it extracts hidden local features by splitting the OW into multiple
sub-windows, which improves the model predictive performance; (ii) it is flexible
in terms of OW definition - if a complete sub-window set is not valid for use at
the prediction time, a single OW option is used instead.
As mentioned in Sect. 2.3, a step towards multiple OW utilization was taken
in [15] by combining weighted predicted posteriors of four OWs, each making an
independent prediction with a distinct gap. Their approach is different from ours
mainly in that we let the classifier learn the association between cross-window
features, which is not possible in a weighted posterior decision. Another very
recent work (DC-Prophet) [19], published while writing this paper, combines
features from consecutive time series intervals (lags) to make early predictions
of server failures. Their approach is similar to ours, but it has neither been
applied to clinical episode prediction, nor it has handled invalid lags.
A further contribution of our work is evaluation of both our method and
earlier episode prediction methods in a cross-dataset setting, in addition to the
in-dataset cross-validation. Finally, our experiments are extended by a new eval-
uation approach, which excludes the clinically irrelevant in-episode predictions.
3 Methodology
This section starts with the problem definition, continues with introducing
InWiSe, and concludes with the description of the data compilation, feature
extraction, and classification methods used in this study.
3.1 Problem Definition and Prediction Modes
This study explores the problem of predicting a patient condition (hypotensive
or normotensive) within a 60-min gap. Following the work in [1,8], we define
a hypotensive episode (HE) as a 30-min target window where at least 90% of
MAP values are below 60 mmHg. Any valid target (see the validity definition in
Sect. 4.2) not meeting this criterion is labeled as normotensive. At the prediction
time, each sub-window set is labeled with respect to its corresponding target.
Considering the implementation of the proposed method in the clinical set-
ting, we distinguish between two alternative prediction modes: (i) all-time pre-
diction, where the assumption (found in previous papers) is that episode predic-
tion is needed continuously regardless of the clinical condition at the prediction
time; (ii) exclusive prediction, where episode prediction is needed only when the
patient is not in a currently recognized HE (the last 30 min of the ICU stay are
not an HE by definition).
478 E. Tsur et al.
Predic on 
Predic on 
(a)
(b)
Fig. 1. (a) Basic Method: traversing over a patient record with an impending HE is
demonstrated by the observation, gap and target windows with respect to the prediction
time. (b) InWiSe: a given OW is split into a sub-window set of size N , followed by a
prediction that is based on the combined feature set of the N sub-windows (SubW s).
3.2 Splitting Obervation Windows with InWiSe
In our study, which was developed based on the observation-gap-target windows
scheme demonstrated in Fig. 1a, we hypothesized that taking longer OWs, split-
ting them into several equally sized sub-windows, also called the sub-window set,
and combining all their features together (see Fig. 1b) would improve the predic-
tive accuracy of the induced model versus using a smaller feature set of a single
long or short OW. For example, a set of the mean MAP values from four, three,
two and one hours before the same target window may be more informative for
predicting the target label than the mean MAP value of a single 4-h OW.
The InWiSe method does not use a classifier based on a combined set of
features if one of the current sub-windows in the set is invalid (see Sect. 4.2). In
that case, the prediction is made by a simpler classification model using only the
features extracted from the latest sub-window (SubWN in Fig. 1b) unless that
window is invalid. Consequently, InWiSe misses less prediction points than the
single OW method (more details are in Sect. 4.4, in-dataset paragraph).
3.3 Feature Extraction
We use three basic vital signs (SBP, DBP, and HR) to derive two additional
vital signs for each record: Pulse Pressure calculated by PP = SBP − DBP ,
and Relative Cardiac Output calculated by CO = HR × PP . Next, we extract
the following three groups of features from each sub-window.
Hypotensive Episode Prediction in ICUs 479
Statistical Features: mean, median, standard deviation (Std), variance,
interquartile range (Iqr), skewness, kurtosis and linear regression slope are cal-
culated for each of the vital signs. Missing values are ignored.
Wavelet Features: Similarly to [5], multi-level discrete decomposition of each
vital sign can be conducted with DB wavelets. The decomposition of a single
time series (signal) X is denoted by WX = [an dn dn−1 . . . d1], where n is the
decomposition level (window size depended), an is the signal approximation, and
dk is the detail signal of level k. The elements in WX are then utilized as features
by calculating the relative energy for each of them as in [8,15]. Missing values
are interpolated.
Cross-Correlation Features: the cross correlation of two time series X =
{x1, x2, . . . , xn} and Y = {y1, y2, . . . , yn} is defined as ρXY = 1nΣxiyi and cal-
culated for each pair of vital signs.
The total amount of features extracted from a sub-window set is equal to the
number of sub-windows N multiplied by the feature set size.
3.4 Classification
Each instance in the training dataset is composed of a sub-window set feature
vector and a class label which is positive or negative (the target is either hypoten-
sive or normotensive, respectively). Before training a binary classifier, we both
normalize the training dataset (to zero mean and unit standard deviation) and
undersample it to overcome the imbalance issue (Sect. 4.3).
Our classifier produces a posterior probability of the positive class, which
may lead to an HE alert depending on the probability threshold determined
from the Receiver Operating Characteristic (ROC) curve built on the target
(testing) dataset. Following [3,8,15], the following selection criterion for the
optimal threshold can be used: Thselected = argmaxTh{sensitivity(Th) +
specificity(Th)}.
4 Experiments
The experimental setup of this study includes multiple prediction modes, meth-
ods, and model configurations. We first perform an in-dataset evaluation for
each prediction mode (all-time and exclusive) and for each method (single OW
and InWiSe). Next, we proceed with a cross-dataset validation for each dataset
pair. This section describes the datasets and their compiled OW and window-set
statistics, followed by the experiments and analysis of results.
4.1 Data Description
Three databases of adult ICU admission records were prepared for this study:
Soroka Medical Center in Beer Sheva (4, 757 records), Hadassah Hospital,
Ein-Karem, Jerusalem (8, 366 records), and MIMIC II [9] (downloaded from [17,
480 E. Tsur et al.
18] and comprising 5, 266 records). All time-series sampling rates are minute-by-
minute (some second-by-second MIMIC II records were undersampled by taking
each minute median). The common-shared vital signs among the three databases
are HR, SBP, DBP, MAP, peripheral capillary oxygen saturation and respira-
tion. Similarly to Lee and Mark [8,15], we included only the HR, SBP, DBP and
MAP vital signs in our data.
4.2 Data Compilation
As a pre-processing step, any outlier (out of the rage 10–200 for any vital sign) is
considered as a ‘missing value’. When compiling OWs from each admission record
we used the observation-gap-target window scheme (Sect. 3.2) called the single
OW method, as well as a first step for InWiSe. The window sizes of the single
OW were 30, 60, 120 or 240 min, while the gap and target sizes were constant
at 60 and 30 min, respectively. Furthermore, we followed Lee and Mark [8] who
claimed that a prediction rate of every 30 min should represent the performance
of a real-time system. Therefore, a 30-min. sliding target window was traversed
with no overlaps over each record and as many OWs as possible were compiled,
depending on the prediction mode. Following [8], targets with more than 10%
missing MAP values were excluded from this study, as their true labels are
unknown. Turning to OW validity, to prevent a classifier from learning outlier
instances, more than 5% missing values for any vital sign made the window
invalid and, consequently, excluded it from our work as well.
Five InWiSe configurations of window splitting were selected for this study:
60[m] → 2 × 30[m], 120 → 2 × 60, 120 → 4 × 30, 240 → 2 × 120 and 240 →
4 × 60. For each configuration and for every record in the dataset, at the predic-
tion time, we combine a sub-window set ([SubW1, . . . , SubWN ], Fig. 1b) if all
N sub-windows are valid. The label of a complete sub-window set is the same as
of its latest sub-window, which is labeled according to its corresponding target
window.
Following the window label counts, the imbalance ratio for the all-time com-
pilation was found to be 1:20 to 1:40 in favor of normotensive (negative) windows
(increasing with the observation window size), as opposed to the exclusive com-
pilation (no in-episode predictions), which was two times more imbalanced. As
for the window set method, the bigger the set size N the less positive and nega-
tive examples are available. Like in a single OW, we observed an increase in the
window-set labeling imbalance with an increase in the window size that reached
1:100 in the exclusive mode.
The reduction of sub-window sets availability with increasing N varied over
datasets and was caused by differences in the amount of missing values (i.e.,
MIMIC II misses more values than Soroka). Moreover, the reason behind cross-
dataset differences in terms of total OW count with respect to record count was
the variance in ICU stay duration, which was higher in Soroka than in other
datasets. Last, we note that using the exclusive mode resulted in a decrease
of over 50% in the positive window count, probably because the average HE
duration was much longer than 30 min (i.e., Hadassah average HE duration was
Hypotensive Episode Prediction in ICUs 481
98 min), increasing the time intervals where we do not make a prediction under
this mode.
4.3 Experimental Setup
In-dataset Evaluation: For each dataset, mode and algorithm a 5-fold cross-
validation (CV) was performed. To allow the classifier to successfully learn the
imbalanced data, training folds were undersampled (5 times without replace-
ment) to attain equal counts of stable and unstable admission records within
each training fold (test folds were left unbalanced). Moreover, for each record
all OWs or sub-window sets were either in training or test dataset to prevent
record characteristics from leaking into the test dataset. In total, the classi-
fier produced 25 outputs (5 folds × 5 samples) which were evaluated by five
metrics: area under the ROC curve (AUC), accuracy, sensitivity, specificity and
precision. Furthermore, to compare between the two methods fairly, we opti-
mized the hyper-parameters of each classifier: an inner 5-fold CV was utilized
in each undersampled training fold of the outer CV and the best chosen hyper-
parameters found by a grid search were used to train the outer fold (Nested CV).
To choose the prediction model, three classifiers were evaluated using a 60-
min OW (single OW method) and a 4× 60-min set (InWiSe) on all datasets
combined and in the all-time prediction mode (with hyper parameters opti-
mization). The AUCs were (0.932, 0.936) for Random Forest, (0.937, 0.940) for
Artificial Neural Network (ANN) with a single hidden layer of 100 neurons, and
(0.939, 0.943) for Extreme Gradient Boosting (XGBoost) [20], where each tuple
represents a <single OW, sub-window set> pair. Since XGBoost outperformed
Random Forest and ANN with p-values = 0.03 and 0.08, respectively (using
t-test), we chose XGBoost for inducing prediction models in this study. Still,
ANN was used as a baseline of [8].
XGBoost is a scalable implementation of the Gradient Boosting ensemble
method that affords some additional capabilities like feature sampling (in addi-
tion to instance sampling) for each tree in the ensemble, making it even more
robust to feature dimensionality and helping to avoid overfitting. Moreover,
considering our minimum training dataset size of approximately 2k instances
together with the maximal feature vector size of 392 features, the built-in fea-
ture selection capability of XGBoost is important.
The XGBoost classifier was grid-search optimized for each dataset or mode
and for each OW size or sub-window set configuration C, where the best hyper-
parameters were reproduced for all datasets, in most of the CV folds. The opti-
mized hyper-parameters were: number of trees (500, 1000, 1500), maximum tree
depth (3, 5), learning rate (0.001, 0.01, 0.1), instance sample rate (0.8, 0.4),
and feature sample rate (0.9, 0.6). The best choices are shown above in bold.
Finally, each algorithm was tried with several OW sizes and sub-window sets
Cs: four OW sizes for the single OW method and five Cs for InWiSe sub-window
sets (see Sect. 4.2). As a result, a total of 54 in-dataset CVs were conducted (3
datasets × 2 modes × 9 window-set Cs and single OW sizes).
482 E. Tsur et al.
Cross-Dataset Validation: The model induced from each dataset was evalu-
ated on other datasets using the all-time mode. XGBoost was trained on one full
dataset and tested on the two other datasets separately. The source dataset was
undersampled only once, justified by a mostly very low variance of AUC (<0.1%)
between undersamples, in each fold of the in-dataset CV. Both the single OW
size and the window-set C were chosen to optimize the AUC performance in
the in-dataset evaluation: 120/240-min sized OW for the single OW method and
4 × 60-min sub-window set for InWiSe. The hyper-parameters of the classifier
(XGBoost) of each method were chosen by a majority vote over the folds in
the in-dataset evaluation. A total of 18 experiments were performed (3 training
datasets × 2 test datasets × 3 window-set Cs and single OW sizes).
4.4 Analysis of Results
This subsection presents the results, followed by the feature importance analy-
sis. The reader should recall that all sub-window set results include some test
instances which were classified using the latest sub-window (SubWN in Fig. 1b),
if valid, in case that the sub-window set was invalid.
In-dataset: As a baseline, we reproduced the single OW method results of Lee
and Mark [8] on MIMIC II with ANN. In Fig. 2, we use MIMIC II to compare
the single OW method with ANN, using two more methods: single OW with
XGBoost (single OW method) and sub-window set best split using XGBoost as
well. In comparison with the baseline, the AUC of the 4× 60 sub-window set
(XGBoost) was significantly higher than for the single OW method with ANN
(60, 120 or 240-min OW size) with p-values 0.009 and 0.05 for all-time and
exclusive modes, respectively.
From Figs. 2 and 3, we first conclude that in terms of AUC, splitting a single
OW into several sub-windows is usually better than using a single OW; we note
that, the advantage of OW splitting grows with an increase in the OW duration,
which emphasizes the benefit from splitting a single OW that is longer than the
longest OWs used by current methods (240 → 4 × 60 versus 120 min).
Turning to XGBoost-only comparison on MIMIC II, InWiSe outperformed
the single OW in the all-time prediction mode, but only with a p-value of 0.13,
while performing only slightly better than the single OW in the exclusive mode
(in terms of AUC). However, while these all-time prediction trends are similar
in the Soroka dataset where InWiSe is better with p = 0.15 (Fig. 3), in the
Hadassah dataset InWiSe significantly outperforms the single OW method with
a p-value of 0.03. In addition, in Table 1 that shows the in-dataset best results on
its diagonal, we see that, although not always statistically significant, the sub-
window set method is better than the single OW alternatives in each dataset
and in all metrics (in bold) when evaluating in the all-time prediction mode.
Note that our significance test comparisons were between the best results of
each method rather than sub-window set versus its matching long window and
they were calculated with four degrees of freedom (due to five folds).
As for the exclusive mode, the AUC was lower, as expected, since less posi-
tive OW instances were available, making the prediction task harder in general.
Hypotensive Episode Prediction in ICUs 483
0.933
0.936 0.937
0.929
0.931
0.931 0.931
0.922 0.924
0.923 0.924
0.921
0.926
0.931
0.936
0.941
AU
C
Single OW vs. Best Window-Set Split
MIMIC I I  - A l l -T ime Predic t ion
Best Split (XGB) Single (XGB) Single Baseline (ANN)
0.883 0.884 0.884
0.878 0.882
0.884 0.883
0.865 0.867 0.866
0.870
0.864
0.874
0.884
AU
C
Single OW vs. Best Window-Set Split
MIMIC  I I  - Exc lus ive  Predic t ion
Best Split (XGB) Single (XGB) Single Baseline (ANN)
30 60, 30x2 120, 60x2 240, 120x230 60, 30x2 120, 60x2 240, 60x4
Fig. 2. The single OW method compared with its InWiSe best split, and with the ANN
baseline on MIMIC II (all-time mode at the left and exclusive mode to the right).
The smaller improvement of InWiSe in comparison with a single OW is prob-
ably related to the considerable decrease in available positive sub-window sets
relatively to the single OW count, compared to a small decrease in the case of
the all-time mode. For example, Soroka HE labeled OW count decreases by 5-7%
for the all-time mode, but by 35-45% for the exclusive mode, between the 4-sub-
window set and the single OW methods. Table 2 shows further in-dataset metric
results as Table 1 did for the all-time mode. In contrast to the all-time results,
in the exclusive mode the AUC is still better with InWiSe (relatively to single
OW), whereas other metrics domination depends on the posterior probability
threshold.
Finally, we observed an average increase of 2.5% in valid prediction times
when using InWiSe in comparison with a single OW in the size of N x SubWsize.
This was mainly caused by the relaxed validation conditions in terms of missing
values when splitting the windows, as well as by being able to use a single
sub-window instead of a sub-window set at the beginning of a record when the
available OWs are too short to be valid.
Cross-Datasets: The results of the cross-dataset experiments for the all-time
prediction mode are shown in Table 1. First, one can observe the expected, but
relatively small, drop in performance when training with one dataset and testing
with another (0.1–0.5% in AUC). Nevertheless, we see that InWiSe outperforms
other methods in terms of the AUC metric, even when applying the model to a
new dataset. However, similarly to the in-dataset exclusive mode case, the other
metrics domination in the cross-dataset validation (all-time mode) is threshold
dependent, but this time with dependence on the source dataset. For example,
the Soroka dataset sensitivity of the single OW method is higher than the sub-
window set one (0.897 vs. 0.868, respectively) in the case where the model was
trained on MIMIC II, while the opposite is true when it was trained on Hasassah
(0.920 vs. 0.940). The reason for these results is probably the difference between
the optimal threshold values in the source and the target datasets.
Feature Importance: The goal of splitting OWs was to let the classifier learn
feature correlations with the target window from each sub-window separately, as
well as their association and cross-window correlation. Table 3 presents the top
484 E. Tsur et al.
0.942 0.943
0.944
0.939
0.940 0.941 0.941
0.938
0.943AU
C
Single OW vs. Best Window-Set Split
S o r o k a  - A l l - T i m e  P r e d i c t i o n
Best Split Single
30 60, 30x2 120, 60x2 240, 60x4
0.904
0.906 0.906
0.899
0.902 0.904 0.904
0.898
0.903
0.908
AU
C
Single OW vs. Best Window-Set Split
S o r o k a  - E x c l u s i v e  P r e d i c t i o n
Best Split Single
30 60, 30x2 120, 60x2 240, 120x2
0.938
0.940 0.941
0.934 0.937 0.938 0.937
0.933
0.938AU
C
Single OW vs. Best Window-Set Split
H a d a s s a h  - A l l - T i m e  P r e d i c t i o n
Best Split Single
30 60, 30x2 120, 60x2 240, 60x4
0.891
0.895 0.895
0.885 0.891
0.894 0.894
0.884
0.894
AU
C
Single OW vs. Best Window-Set Split
H a d a s s a h  - E x c l u s i v e  P r e d i c t i o n
Best Split Single
30 60, 30x2 120, 60x2 240, 120x2
Fig. 3. In-dataset comparison between the single OW method and its InWiSe best split
on the Hadassah and Soroka datasets (both prediction modes, XGBoost only).
Table 1. In-dataset and cross-dataset methods comparison (InWiSe best configuration
vs. single OW best size vs. window-set matching single OW size) using all-time predic-
tion. Metrics from top to bottom: AUC, accuracy, sensitivity, specificity and precision
T
arget
Source Dataset (Train)
MIMIC II Soroka Hadassah
4x60 120 240 4x60 120 240 4x60 120 240
M
IM
IC
II
AUC 0.937±0.003 0.931±0.006 0.931±0.007 0.933 0.929 0.932 0.937 0.933 0.934
Acc. 0.878±0.007 0.864±0.013 0.866±0.011 0.846 0.828 0.860 0.898 0.903 0.902
Sens. 0.849±0.011 0.847±0.025 0.843±0.023 0.869 0.873 0.849 0.829 0.807 0.803
Spec. 0.879±0.008 0.865±0.013 0.866±0.011 0.846 0.826 0.861 0.900 0.906 0.905
Prec. 0.168±0.007 0.150±0.011 0.151±0.016 0.136 0.124 0.146 0.187 0.194 0.192
Soroka
AUC 0.939 0.935 0.935 0.944±0.003 0.941±0.003 0.941±0.003 0.942 0.938 0.939
Acc. 0.867 0.822 0.858 0.870±0.007 0.861±0.005 0.856±0.004 0.761 0.800 0.793
Sens. 0.868 0.897 0.865 0.875±0.014 0.875±0.011 0.875±0.014 0.940 0.915 0.920
Spec. 0.867 0.820 0.857 0.869±0.008 0.860±0.006 0.856±0.007 0.754 0.796 0.789
Prec. 0.187 0.150 0.178 0.191±0.015 0.182±0.016 0.177±0.017 0.119 0.137 0.134
H
adassah
AUC 0.936 0.933 0.932 0.938 0.935 0.935 0.941±0.002 0.938±0.002 0.937±0.002
Acc. 0.842 0.816 0.815 0.838 0.814 0.834 0.870±0.006 0.867±0.007 0.861±0.006
Sens. 0.889 0.899 0.896 0.894 0.904 0.888 0.871±0.006 0.866±0.005 0.866±0.006
Spec. 0.839 0.812 0.811 0.835 0.809 0.831 0.870±0.007 0.867±0.007 0.860±0.007
Prec. 0.207 0.184 0.182 0.204 0.183 0.198 0.241±0.011 0.235±0.011 0.226±0.010
Table 2. In-dataset methods comparison (InWiSe best configuration vs. single OW
best size) in the exclusive prediction mode
MIMIC II Soroka Hadassah
2× 60 120 2× 60 120 2× 60 120
AUC 0.884± 0.011 0.884± 0.006 0.906± 0.002 0.904± 0.002 0.895± 0.003 0.894± 0.003
Accuracy 0.790± 0.020 0.816± 0.013 0.787± 0.011 0.809± 0.010 0.783± 0.009 0.807± 0.007
Sensitivity 0.826± 0.021 0.793± 0.023 0.875± 0.009 0.851± 0.012 0.855± 0.009 0.829± 0.007
Specificity 0.789± 0.021 0.817± 0.014 0.785± 0.010 0.808± 0.010 0.782± 0.009 0.807± 0.007
Precision 0.054± 0.006 0.060± 0.004 0.069± 0.004 0.075± 0.004 0.082± 0.003 0.089± 0.003
Hypotensive Episode Prediction in ICUs 485
Table 3. Top 10 features rank in terms of frequency over trees in the XGBoost ensemble
All-Time Prediction Exclusive Prediction
60-min 240-min 4x60-min 60-min 120-min 2x60-min
SubW1 SubW2 SubW3 SubW4 SubW1 SubW2
SBP Slope 10 10 – – – 9 – 10 – 9
SBP Skewness – – – – – – 9 – – –
DBP Slope – – – – – – – – – 10
MAP Mean 1 1 5 – 6 1 1 1 4 1
MAP Std 6 8 – – – – 8 9 – –
MAP Median 2 3 7 – – 2 3 3 7 5
MAP Iqr 9 – – – – – 10 – – –
MAP Skewness 8 7 – – – – 6 6 – 8
MAP Slope 4 2 – – – 3 5 4 – 3
HR Slope 5 6 – – – 8 4 5 – 6
SBP Cross Correlation w/ DBP 7 5 – – – 10 7 7 – –
PP Cross Correlation w/ RCO 3 4 – – – 4 2 2 – 2
MAP Wavelet Detail Level-3 – – – – – – – 8 – –
HR Wavelet Approximation – 9 – – – – – – – –
ten important features of XGBoost (most frequent over the ensemble trees) for
the best InWiSe configuration compared with its sub-window sized OW as well
as the corresponding single OW (in two prediction modes). The sub-window set
columns are divided into their sub-windows, where SubWN is the sub-window
ending at the prediction time and SubW1 is the earliest in the set (Fig. 1b).
We first see that the Mean Arterial blood Pressure (MAP) mean is clearly
dominant in all cases, which makes sense since MAP values are the ones that
define an HE. Next, we observe that features from all three types (statisti-
cal, cross-correlation and wavelets) are top-ten-ranked, with the statistical ones
(especially of MAP) used more frequently. Moreover, the two derived param-
eters, Pulse Pressure (PP) and Relative Cardiac Output (RCO), are proved
to contribute particularly in their cross correlation with the target. Turning to
sub-window sets, while SubWN has obviously more weight, the algorithm repeat-
edly chooses to combine the MAP mean and median from early sub-windows as
well, with a surprisingly high rank. The early sub-window features are in favor
of other higher ranked features in the single OWs (i.e., HR Slope and SBP
cross-correlation with DBP). These findings support and explain our hypothesis
that the model may be improved by using local sub-window features instead of
extracting features from a single long OW.
5 Conclusions and Future Work
The current study presented and explored InWiSe, an enhanced feature extrac-
tion algorithm for clinical episode prediction, where physiological features are
extracted from a set of observation sub-windows instead of a single OW. Our
evaluation experiments have shown that the prediction performance may be
improved by combining local sub-window features instead of extracting the same
486 E. Tsur et al.
features from a single OW (of any size), observing an increased improvement
when splitting longer OWs than used in existing methods (i.e., 240-min OW).
The importance of sub-window features is confirmed by a feature importance
analysis. Moreover, in the all-time prediction mode, used by the recent works,
we show an improvement in comparison with the single OW method over all
three experimental datasets w.r.t. all evaluated metrics2 (up to 1% in accuracy
and specificity and up to 10% in precision, while maintaining the sensitivity
equal or better). We particularly focus on the AUC metric that was improved
by up to 0.6%, with a statistically significant improvement in AUC performance
in the case of the Hadassah dataset.
In addition to the above, we successfully evaluated the methods in a cross-
dataset fashion, showing that the induced models are capable of predicting
episode on a new dataset, with just a little degradation in the performance.
Moreover, the AUC metric repeatedly favors the InWiSe method, even when
testing the model on a new dataset. Furthermore, we explored a new prediction
mode (exclusive) which may better reflect ICU needs.
With regard to InWiSe future improvements, better accuracy results may
be achieved in the case of an invalid sub-window set, especially in the exclu-
sive prediction mode. One may also evaluate alternative approaches to multiple
OW utilization such as the weighted ensemble method of [15]. Another possible
approach to episode prediction may be built on predicting the Mean Arterial
blood Pressure (MAP) values in the target window with multivariate time series
forecasting models.
From the dataset perspective, any future analysis should use the recently pub-
lished MIMIC III dataset mentioned in Sect. 2.5. Applying an existing model on
a new dataset should be further investigated in terms of determining a dataset-
specific optimal classification threshold. Finally, the proposed methodology can
be extended to other episode prediction domains.
References
1. Ghosh, S., et al.: Hypotension risk prediction via sequential contrast patterns of
ICU blood pressure. IEEE J. Biomed. Health Inform. 20(5), 1416–1426 (2016)
2. Sebat, F., et al.: Effect of a rapid response system for patients in shock on time to
treatment and mortality during 5 years. Crit. Care Med. 35(11), 2568–2575 (2007)
3. Cao, H., et al.: Predicting ICU hemodynamic instability using continuous multi-
parameter trends. In: Engineering in Medicine and Biology Society (EMBS), pp.
3803–3806. IEEE (2008)
4. Moody, G.B., Lehman, L.W.H.: Predicting acute hypotensive episodes: the 10th
annual physionet/computers in cardiology challenge. In: Computers in Cardiology,
pp. 541–544. IEEE (2009)
5. Forkan, A.R.M., et al.: ViSiBiD: a learning model for early discovery and real-time
prediction of severe clinical events using vital signs as big data. Comput. Netw.
113, 244–257 (2017)
2 All improvement percentages are in terms of a ratio between the two measures
Hypotensive Episode Prediction in ICUs 487
6. Kamio, T., et al.: Use of machine-learning approaches to predict clinical deterio-
ration in critically Ill patients: a systematic review. Int. J. Med. Res. Health Sci.
6(6), 1–7 (2017)
7. Eshelman, L.J., et al.: Development and evaluation of predictive alerts for hemody-
namic instability in ICU patients. In: 2008 AMIA Annual Symposium Proceedings,
p. 379. American Medical Informatics Association (2008)
8. Lee, J., Mark, R.G.: An investigation of patterns in hemodynamic data indicative
of impending hypotension in intensive care. Biomed. Eng. Online 9(1), 62 (2010)
9. Saeed, M., et al.: Multiparameter intelligent monitoring in intensive care II
(MIMIC-II): a public-access ICU database. Crit. Car. Med. 39(5), 952 (2011)
10. Chen, X., et al.: Forecasting acute hypotensive episodes in intensive care patients
based on a peripheral arterial blood pressure waveform. In: 2009 Computers in
Cardiology, pp. 545–548. IEEE (2009)
11. Saeed, M.: Temporal pattern recognition in multiparameter ICU data, Doctoral
dissertation, Massachusetts Institute of Technology (2007)
12. Saeed, M., Mark, R.: A novel method for the efficient retrieval of similar mul-
tiparameter physiologic time series using wavelet-based symbolic representations.
In: AMIA Annual Symposium Proceedings, p. 679. American Medical Information
Association (2006)
13. Rocha, T., et al.: Wavelet based time series forecast with application to acute
hypotensive episodes prediction. In: Engineering in medicine and biology society
(EMBC), pp. 2403–2406. IEEE (2010)
14. Ghosh, S., et al.: Septic shock prediction for ICU patients via coupled HMM walk-
ing on sequential contrast patterns. J. Biomed. Info. 66, 19–31 (2017)
15. Lee, J., Mark, R.G.: A hypotensive episode predictor for intensive care based on
heart rate and blood pressure time series. In: Computing in Cardiology, pp. 81–84.
IEEE (2010)
16. Rocha, T., et al.: Prediction of acute hypotensive episodes by means of neural
network multi-models. Comp. Biol. Med. 41(10), 881–890 (2011)
17. Goldberger, A.L., et al.: PhysioBank, PhysioToolkit, and PhysioNet: components
of a new research resource for complex physiologic signals. Circulation 101(23),
e215–e220 (2000)
18. The MIMIC II Waveform Database Matched Subset (Physionet Database).
https://physionet.org/physiobank/database/mimic2wdb/matched/
19. Lee, Y.-L., Juan, D.-C., Tseng, X.-A., Chen, Y.-T., Chang, S.-C.: DC-Prophet:
predicting catastrophic machine failures in DataCentre. In: Altun, Y., et al. (eds.)
Machine Learning and Knowledge Discovery in Databases. LNCS, vol. 10536, pp.
64–76. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-71273-4 6
20. Chen, T., Guestrin, C.: XGBoost: a scalable tree boosting system. In: 22nd ACM
SIGKDD International Conference, pp. 785–794. ACM (2016)
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תכנה-וחומר-עזר\Induction-of-Decision-Trees\Quinlan86.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Machine Learning 1: 81-106, 1986
© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands
Induction of Decision Trees
J.R, QUINLAN (munnarilnswitgould.oz!quinlan@seismo,css.gov)
Centre for Advanced Computing Sciences, New South Wales Institute of Technology, Sydney 2007,
Australia
(Received August 1, 1985)
Key words: classification, induction, decision trees, information theory, knowledge acquisition, expert
systems
Abstract. The technology for building knowledge-based systems by inductive inference from examples has
been demonstrated successfully in several practical applications. This paper summarizes an approach to
synthesizing decision trees that has been used in a variety of systems, and it describes one such system,
ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal
with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is
discussed and two means of overcoming it are compared. The paper concludes with illustrations of current
research directions.
1. Introduction
Since artificial intelligence first achieved recognition as a discipline in the mid 1950's,
machine learning has been a central research area. Two reasons can be given for this
prominence. The ability to learn is a hallmark of intelligent behavior, so any attempt
to understand intelligence as a phenomenon must include an understanding of learn-
ing. More concretely, learning provides a potential methodology for building high-
performance systems.
Research on learning is made up of diverse subfields. At one extreme there are
adaptive systems that monitor their own performance and attempt to improve it by
adjusting internal parameters. This approach, characteristic of a large proportion of
the early learning work, produced self-improving programs for playing games
(Samuel, 1967), balancing poles (Michie, 1982), solving problems (Quinlan, 1969)
and many other domains. A quite different approach sees learning as the acquisition
of structured knowledge in the form of concepts (Hunt, 1962; Winston, 1975),
discrimination nets (Feigenbaum and Simon, 1963), or production rules (Buchanan,
1978).
The practical importance of machine learning of this latter kind has been underlin-
82 J.R. QUINLAN
ed by the advent of knowledge-based expert systems. As their name suggests, these
systems are powered by knowledge that is represented explicitly rather than being im-
plicit in algorithms. The knowledge needed to drive the pioneering expert systems was
codified through protracted interaction between a domain specialist and a knowledge
engineer. While the typical rate of knowledge elucidation by this method is a few
rules per man day, an expert system for a complex task may require hundreds or even
thousands of such rules. It is obvious that the interview approach to knowledge ac-
quisition cannot keep pace with the burgeoning demand for expert systems; Feigen-
baum (1981) terms this the 'bottleneck' problem. This perception has stimulated the
investigation of machine learning methods as a means of explicating knowledge
(Michie, 1983).
This paper focusses on one microcosm of machine learning and on a family of
learning systems that have been used to build knowledge-based systems of a simple
kind. Section 2 outlines the features of this family and introduces its members. All
these systems address the same task of inducing decision trees from examples. After
a more complete specification of this task, one system (ID3) is described in detail in
Section 4. Sections 5 and 6 present extensions to ID3 that enable it to cope with noisy
and incomplete information. A review of a central facet of the induction algorithm
reveals possible improvements that are set out in Section 7. The paper concludes with
two novel initiatives that give some idea of the directions in which the family may
grow.
2. The TDIDT family of learning systems
Carbonell, Michalski and Mitchell (1983) identify three principal dimensions along
which machine learning systems can be classified:
• the underlying learning strategies used;
• the representation of knowledge acquired by the system; and
• the application domain of the system.
This paper is concerned with a family of learning systems that have strong common
bonds in these dimensions.
Taking these features in reverse order, the application domain of these systems is
not limited to any particular area of intellectual activity such as Chemistry or Chess;
they can be applied to any such area. While they are thus general-purpose systems,
the applications that they address all involve classification. The product of learning
is a piece of procedural knowledge that can assign a hitherto-unseen object to one
of a specified number of disjoint classes. Examples of classification tasks are:
INDUCTION OF DECISION TREES 83
1. the diagnosis of a medical condition from symptoms, in which the classes could
be either the various disease states or the possible therapies;
2. determining the game-theoretic value of a chess position, with the classes won for
white, lost for white, and drawn; and
3. deciding from atmospheric observations whether a severe thunderstorm is unlike-
ly, possible or probable.
It might appear that classification tasks are only a minuscule subset of procedural
tasks, but even activities such as robot planning can be recast as classification prob-
lems (Dechter and Michie, 1985).
The members of this family are sharply characterized by their representation of ac-
quired knowledge as decision trees. This is a relatively simple knowledge formalism
that lacks the expressive power of semantic networks or other first-order representa-
tions. As a consequence of this simplicity, the learning methodologies used in the
TDIDT family are considerably less complex than those employed in systems that can
express the results of their learning in a more powerful language. Nevertheless, it is
still possible to generate knowledge in the form of decision trees that is capable of
solving difficult problems of practical significance.
The underlying strategy is non-incremental learning from examples. The systems
are presented with a set of cases relevant to a classification task and develop a deci-
sion tree from the top down, guided by frequency information in the examples but
not by the particular order in which the examples are given. This contrasts with in-
cremental methods such as that employed in MARVIN (Sammut, 1985), in which a
dialog is carried on with an instructor to 'debug' partially correct concepts, and that
used by Winston (1975), in which examples are analyzed one at a time, each produc-
ing a small change in the developing concept; in both of these systems, the order in
which examples are presented is most important. The systems described here search
for patterns in the given examples and so must be able to examine and re-examine
all of them at many stages during learning. Other well-known programs that share
this data-driven approach include BACON (Langley, Bradshaw and Simon, 1983)
and INDUCE (Michalski, 1980).
In summary, then, the systems described here develop decision trees for classifica-
tion tasks. These trees are constructed beginning with the root of the tree and pro-
ceeding down to its leaves. The family's palindromic name emphasizes that its
members carry out the Top-Down Induction of Decision Trees.
The example objects from which a classification rule is developed are known only
through their values of a set of properties or attributes, and the decision trees in turn
are expressed in terms of these same attributes. The examples themselves can be
assembled in two ways. They might come from an existing database that forms a
history of observations, such as patient records in some area of medicine that have
accumulated at a diagnosis center. Objects of this kind give a reliable statistical pic-
ture but, since they are not organized in any way, they may be redundant or omit
84 J.R. QUINLAN
Figure 1, The TDIDT family tree.
uncommon cases that have not been encountered during the period of record-
keeping. On the other hand, the objects might be a carefully culled set of tutorial ex-
amples prepared by a domain expert, each with some particular relevance to a com-
plete and correct classification rule. The expert might take pains to avoid redundancy
and to include examples of rare cases. While the family of systems will deal with col-
lections of either kind in a satisfactory way, it should be mentioned that earlier
TDIDT systems were designed with the 'historical record' approach in mind, but all
systems described here are now often used with tutorial sets (Michie, 1985).
Figure 1 shows a family tree of the TDIDT systems. The patriarch of this family
is Hunt's Concept Learning System framework (Hunt, Marin and Stone, 1966). CLS
constructs a decision tree that attempts to minimize the cost of classifying an object.
This cost has components of two types: the measurement cost of determining the
value of property A exhibited by the object, and the misclassification cost of deciding
that the object belongs to class J when its real class is K. CLS uses a lookahead
strategy similar to minimax. At each stage, CLS explores the space of possible deci-
sion trees to a fixed depth, chooses an action to minimize cost in this limited space,
then moves one level down in the tree. Depending on the depth of lookahead chosen,
CLS can require a substantial amount of computation, but has been able to unearth
subtle patterns in the objects shown to it.
ID3 (Quinlan, 1979, 1983a) is one of a series of programs developed from CLS in
response to a challenging induction task posed by Donald Michie, viz. to decide from
pattern-based features alone whether a particular chess position in the King-Rook vs
King-Knight endgame is lost for the Knight's side in a fixed number of ply. A full
description of ID3 appears in Section 4, so it is sufficient to note here that it embeds
a tree-building method in an iterative outer shell, and abandons the cost-driven
lookahead of CLS with an information-driven evaluation function.
ACLS (Paterson and Niblett, 1983) is a generalization of ID3. CLS and ID3 both
require that each property used to describe objects has only values from a specified
set. In addition to properties of this type, ACLS permits properties that have
INDUCTION OF DECISION TREES 85
unrestricted integer values. The capacity to deal with attributes of this kind has allow-
ed ACLS to be applied to difficult tasks such as image recognition (Shepherd, 1983).
ASSISTANT (Kononenko, Bratko and Roskar, 1984) also acknowledges ID3 as
its direct ancestor. It differs from ID3 in many ways, some of which are discussed
in detail in later sections. ASSISTANT further generalizes on the integer-valued at-
tributes of ACLS by permitting attributes with continuous (real) values. Rather than
insisting that the classes be disjoint, ASSISTANT allows them to form a hierarchy,
so that one class may be a finer division of another. ASSISTANT does not form a
decision tree iteratively in the manner of ID3, but does include algorithms for choos-
ing a 'good' training set from the objects available. ASSISTANT has been used in
several medical domains with promising results.
The bottom-most three systems in the figure are commercial derivatives of ACLS.
While they do not significantly advance the underlying theory, they incorporate
many user-friendly innovations and utilities that expedite the task of generating and
using decision trees. They all have industrial successes to their credit. Westinghouse
Electric's Water Reactor Division, for example, points to a fuel-enrichment applica-
tion in which the company was able to boost revenue by 'more than ten million
dollars per annum' through the use of one of them.1
3. The induction task
We now give a more precise statement of the induction task. The basis is a universe
of objects that are described in terms of a collection of attributes. Each attribute
measures some important feature of an object and will be limited here to taking a
(usually small) set of discrete, mutually exclusive values. For example, if the objects
were Saturday mornings and the classification task involved the weather, attributes
might be
outlook, with values {sunny, overcast, rain}
temperature, with values {cool, mild, hot}
humidity, with values {high, normal}
windy, with values {true, false }
Taken together, the attributes provide a zeroth-order language for characterizing ob-
jects in the universe. A particular Saturday morning might be described as
outlook: overcast
temperature: cool
humidity: normal
windy: false
1 Letter cited in the journal Expert Systems (January, 1985), p. 20.
86 J.R. QUINLAN
Each object in the universe belongs to one of a set of mutually exclusive classes,
To simplify the following treatment, we will assume that there are only two such
classes denoted P and N, although the extension to any number of classes is not dif-
ficult. In two-class induction tasks, objects of class P and N are sometimes referred
to as positive instances and negative instances, respectively, of the concept being
learned.
The other major ingredient is a training set of objects whose class is known. The
induction task is to develop a classification rule that can determine the class of any
object from its values of the attributes. The immediate question is whether or not the
attributes provide sufficient information to do this. In particular, if the training set
contains two objects that have identical values for each attribute and yet belong to
different classes, it is clearly impossible to differentiate between these objects with
reference only to the given attributes. In such a case attributes will be termed inade-
quate for the training set and hence for the induction task.
As mentioned above, a classification rule will be expressed as a decision tree. Table
1 shows a small training set that uses the 'Saturday morning' attributes. Each object's
value of each attribute is shown, together with the class of the object (here, class P
mornings are suitable for some unspecified activity). A decision tree that correctly
classifies each object in the training set is given in Figure 2. Leaves of a decision tree
are class names, other nodes represent attribute-based tests with a branch for each
possible outcome. In order to classify an object, we start at the root of the tree,
evaluate the test, and take the branch appropriate to the outcome. The process con-
tinues until a leaf is encountered, at which time the object is asserted to belong to
Table 1 . A small training set
No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Outlook
sunny
sunny
overcast
rain
rain
rain
overcast
sunny
sunny
rain
sunny
overcast
overcast
rain
Attributes
Temperature
hot
hot
hot
mild
cool
cool
cool
mild
cool
mild
mild
mild
hot
mild
Humidity
high
high
high
high
normal
normal
normal
high
normal
normal
normal
high
normal
high
Windy
false
true
false
false
false
true
true
false
false
false
true
true
false
true
Class
N
N
P
P
P
N
P
N
P
P
P
P
P
N
INDUCTION OF DECISION TREES 87
Figure 2. A simple decision tree
the class named by the leaf. Taking the decision tree of Figure 2, this process con-
cludes that the object which appeared as an example at the start of this section, and
which is not a member of the training set, should belong to class P. Notice that only
a subset of the attributes may be encountered on a particular path from the root of
the decision tree to a leaf; in this case, only the outlook attribute is tested before
determining the class.
If the attributes are adequate, it is always possible to construct a decision tree that
correctly classifies each object in the training set, and usually there are many such
correct decision trees. The essence of induction is to move beyond the training set,
i.e. to construct a decision tree that correctly classifies not only objects from the
training set but other (unseen) objects as well. In order to do this, the decision tree
must capture some meaningful relationship between an object's class and its values
of the attributes. Given a choice between two decision trees, each of which is correct
over the training set, it seems sensible to prefer the simpler one on the grounds that
it is more likely to capture structure inherent in the problem. The simpler tree would
therefore be expected to classify correctly more objects outside the training set. The
decision tree of Figure 3, for instance, is also correct for the training set of Table 1,
but its greater complexity makes it suspect as an 'explanation' of the training set.2
4. ID3
One approach to the induction task above would be to generate all possible decision
trees that correctly classify the training set and to select the simplest of them. The
2 The preference for simpler trees, presented here as a commonsense application of Occam's Razor, is
also supported by analysis. Pearl (1978b) and Quinlan (1983a) have derived upper bounds on the expected
error using different formalisms for generalizing from a set of known cases. For a training set of predeter-
mined size, these bounds increase with the complexity of the induced generalization.
88 J.R. QUINLAN
Figure 3. A complex decision tree.
number of such trees is finite but very large, so this approach would only be feasible
for small induction tasks. ID3 was designed for the other end of the spectrum, where
there are many attributes and the training set contains many objects, but where a
reasonably good decision tree is required without much computation. It has generally
been found to construct simple decision trees, but the approach it uses cannot
guarantee that better trees have not been overlooked.
The basic structure of ID3 is iterative. A subset of the training set called the win-
dow is chosen at random and a decision tree formed from it; this tree correctly
classifies all objects in the window. All other objects in the training set are then
classified using the tree. If the tree gives the correct answer for all these objects then
it is correct for the entire training set and the process terminates. If not, a selection
of the incorrectly classified objects is added to the window and the process continues.
In this way, correct decision trees have been found after only a few iterations for
training sets of up to thirty thousand objects described in terms of up to 50 attributes.
Empirical evidence suggests that a correct decision tree is usually found more quickly
by this iterative method than by forming a tree directly from the entire training set.
However, O'Keefe (1983) has noted that the iterative framework cannot be
guaranteed to converge on a final tree unless the window can grow to include the en-
tire training set. This potential limitation has not yet arisen in practice.
The crux of the problem is how to form a decision tree for an arbitrary collection
C of objects. If C is empty or contains only objects of one class, the simplest decision
tree is just a leaf labelled with the class. Otherwise, let T be any test on an object with
possible outcomes O1, O2, ... Ow. Each object in C will give one of these outcomes
for T, so T produces a partition {C1, C2, ... Cw } of C with Ci containing those ob-
INDUCTION OF DECISION TREES 89
Figure 4. A tree structuring of the objects in C.
jects having outcome Oi. This is represented graphically by the tree form of Figure
4. If each subset Ci in this figure could be replaced by a decision tree for Ci, the result
would be a decision tree for all of C. Moreover, so long as two or more Ci's are non-
empty, each Ci is smaller than C. In the worst case, this divide-and-conquer strategy
will yield single-object subsets that satisfy the one-class requirement for a leaf. Thus,
provided that a test can always be found that gives a non-trivial partition of any set
of objects, this procedure will always produce a decision tree that correctly classifies
each object in C.
The choice of test is crucial if the decision tree is to be simple. For the moment,
a test will be restricted to branching on the values of an attribute, so choosing a test
comes down to selecting an attribute for the root of the tree. The first induction pro-
grams in the ID series used a seat-of-the-pants evaluation function that worked
reasonably well. Following a suggestion of Peter Gacs, ID3 adopted an information-
based method that depends on two assumptions. Let C contain p objects of class P
and n of class N. The assumptions are:
(1) Any correct decision tree for C will classify objects in the same proportion as
their representation in C. An arbitrary object will be determined to belong to
class P with probability p/(p + n) and to class N with probability n/(p + n).
(2) When a decision tree is used to classify an object, it returns a class. A decision
tree can thus be regarded as a source of a message 'P' or 'N', with the expected
information needed to generate this message given by
If attribute A with values {A1, A2, ... Av) is used for the root of the decision tree,
it will partition C into (C1, C2, ... CV} where Ci contains those objects in C that
have value Ai of A. Let Ci contain pi objects of class P and ni of class N. The expected
90 J.R. QUINLAN
information required for the subtree for Ci is I(pi, ni). The expected information re-
quired for the tree with A as root is then obtained as the weighted average
where the weight for the ith branch is the proportion of the objects in C that belong
to Ci. The information gained by branching on A is therefore
A good rule of thumb would seem to be to choose that attribute to branch on which
gains the most information.3 ID3 examines all candidate attributes and chooses A to
maximize gain(A), forms the tree as above, and then uses the same process recursively
to form decision trees for the residual subsets C1, C2, ... Cv.
To illustrate the idea, let C be the set of objects in Table 1. Of the 14 objects, 9
are of class P and 5 are of class N, so the information required for classification is
Now consider the outlook attribute with values {sunny, overcast, rain}. Five of the
14 objects in C have the first value (sunny), two of them from class P and three from
class N, so
and similarly
The expected information requirement after testing this attribute is therefore
3 Since I(p,n) is constant for all attributes, maximizing the gain is equivalent to minimizing E(A), which
is the mutual information of the attribute A and the class. Pearl (1978a) contains an excellent account of
the rationale of information-based heuristics.
INDUCTION OF DECISION TREES 91
The gain of this attribute is then
gain(outlook) = 0.940 - E(outlook) = 0.246 bits
Similar analysis gives
gain(temperature) = 0.029 bits
gain(humidity) = 0.151 bits
gain(windy) = 0.048 bits
so the tree-forming method used in ID3 would choose outlook as the attribute for
the root of the decision tree. The objects would then be divided into subsets according
to their values of the outlook attribute and a decision tree for each subset would be
induced in a similar fashion. In fact, Figure 2 shows the actual decision tree generated
by ID3 from this training set.
A special case arises if C contains no objects with some particular value Aj of A,
giving an empty Cj. ID3 labels such a leaf as 'null' so that it fails to classify any object
arriving at that leaf. A better solution would generalize from the set C from which
Cj came, and assign this leaf the more frequent class in C.
The worth of ID3's attribute-selecting heuristic can be assessed by the simplicity
of the resulting decision trees, or, more to the point, by how well those trees express
real relationships between class and attributes as demonstrated by the accuracy with
which they classify objects other than those in the training set (their predictive ac-
curacy). A straightforward method of assessing this predictive accuracy is to use only
part of the given set of objects as a training set, and to check the resulting decision
tree on the remainder.
Several experiments of this kind have been carried out. In one domain, 1.4 million
chess positions described in terms of 49 binary-valued attributes gave rise to 715
distinct objects divided 65%:35% between the classes. This domain is relatively com-
plex since a correct decision tree for all 715 objects contains about 150 nodes. When
training sets containing 20% of these 715 objects were chosen at random, they pro-
duced decision trees that correctly classified over 84% of the unseen objects. In
another version of the same domain, 39 attributes gave 551 distinct objects with a
correct decision tree of similar size; training sets of 20% of these 551 objects gave
decision trees of almost identical accuracy. In a simpler domain (1,987 objects with
a correct decision tree of 48 nodes), randomly-selected training sets containing 20%
of the objects gave decision trees that correctly classified 98% of the unseen objects.
In all three cases, it is clear that the decision trees reflect useful (as opposed to ran-
dom) relationships present in the data.
This discussion of ID3 is rounded off by looking at the computational require-
ments of the procedure. At each non-leaf node of the decision tree, the gain of each
untested attribute A must be determined. This gain in turn depends on the values pi
92 J.R. QUINLAN
and ni for each value Ai of A, so every object in C must be examined to determine
its class and its value of A. Consequently, the computational complexity of the pro-
cedure at each such node is O ( I C I - I A I ) , where IAI is the number of attributes
above. ID3's total computational requirement per iteration is thus proportional to
the product of the size of the training set, the number of attributes and the number
of non-leaf nodes in the decision tree. The same relationship appears to extend to the
entire induction process, even when several iterations are performed. No exponential
growth in time or space has been observed as the dimensions of the induction task
increase, so the technique can be applied to large tasks.
5. Noise
So far, the information supplied in the training set has been assumed to be entirely
accurate. Sadly, induction tasks based on real-world data are unlikely to find this
assumption to be tenable. The description of objects may include attributes based on
measurements or subjective judgements, both of which may give rise to errors in the
values of attributes. Some of the objects in the training set may even have been
misclassified. To illustrate the idea, consider the task of developing a classification
rule for medical diagnosis from a collection of patient histories. An attribute might
test for the presence of some substance in the blood and will almost inevitably give
false positive or negative readings some of the time. Another attribute might assess
the patient's build as slight, medium, or heavy, and different assessors may apply dif-
ferent criteria. Finally, the collection of case histories will probably include some pa-
tients for whom an incorrect diagnosis was made, with consequent errors in the class
information provided in the training set.
What problems might errors of these kinds pose for the tree-building procedure
described earlier? Consider again the small training set in Table 1, and suppose now
that attribute outlook of object 1 is incorrectly recorded as overcast. Objects 1 and
3 will then have identical descriptions but belong to different classes, so the attributes
become inadequate for this training set. The attributes will also become inadequate
if attribute windy of object 4 is corrupted to true, because that object will then con-
flict with object 14. Finally, the initial training set can be accounted for by the simple
decision tree of Figure 2 containing 8 nodes. Suppose that the class of object 3 were
corrupted to N. A correct decision tree for this corrupted training set would now have
to explain the apparent special case of object 3. The smallest such tree contains twelve
nodes, half again as complex as the 'real' tree. These illustrations highlight two prob-
lems: errors in the training set may cause the attributes to become inadequate, or may
lead to decision trees of spurious complexity.
Non-systematic errors of this kind in either the values of attributes or class infor-
mation are usually referred to as noise. Two modifications are required if the tree-
building algorithm is to be able to operate with a noise-affected training set.
INDUCTION OF DECISION TREES 93
(1) The algorithm must be able to work with inadequate attributes, because noise can
cause even the most comprehensive set of attributes to appear inadequate.
(2) The algorithm must be able to decide that testing further attributes will not im-
prove the predictive accuracy of the decision tree. In the last example above, it
should refrain from increasing the complexity of the decision tree to accom-
modate a single noise-generated special case.
We start with the second requirement of deciding when an attribute is really rele-
vant to classification. Let C be a collection of objects containing representatives of
both classes, and let A be an attribute with random values that produces subsets {C1,
C2, ... Cv }. Unless the proportion of class P objects in each of the Ci is exactly the
same as the proportion of class P objects in C itself, branching on attribute A will
give an apparent information gain. It will therefore appear that testing attribute A
is a sensible step, even though the values of A are random and so cannot help to
classify the objects in C.
One solution to this dilemma might be to require that the information gain of any
tested attribute exceeds some absolute or percentage threshold. Experiments with this
approach suggest that a threshold large enough to screen out irrelevant attributes also
excludes attributes that are relevant, and the performance of the tree-building pro-
cedure is degraded in the noise-free case.
An alternative method based on the chi-square test for stochastic independence has
been found to be more useful. In the previous notation, suppose attribute A produces
subsets {C1, C2, ... Cv} of C, where Ci contains pi and ni objects of class P and N,
respectively. If the value of A is irrelevant to the class of an object in C, the expected
value p' i of pi should be
If n'i is the corresponding expected value of ni, the statistic
is approximately chi-square with v-1 degrees of freedom. Provided that none of the
values p'i or n' i are very small, this statistic can be used to determine the confidence
with which one can reject the hypothesis that A is independent of the class of objects
in C (Hogg and Craig, 1970). The tree-building procedure can then be modified to
prevent testing any attribute whose irrelevance cannot be rejected with a very high
(e.g. 99%) confidence level. This has been found effective in preventing over-
94 J.R. QUINLAN
complex trees that attempt to 'fit the noise' without affecting performance of the pro-
cedure in the noise-free case.4
Turning now to the first requirement, we see that the following situation can arise:
a collection of C objects may contain representatives of both classes, yet further
testing of C may be ruled out, either because the attributes are inadequate and unable
to distinguish among the objects in C, or because each attribute has been judged to
be irrelevant to the class of objects in C. In this situation it is necessary to produce
a leaf labelled with class information, but the objects in C are not all of the same
class.
Two possibilities suggest themselves. The notion of class could be generalized to
allow the value p/(p + n) in the interval (0,1), a class of 0.8 (say) being interpreted
as 'belonging to class P with probability 0.8'. An alternative approach would be to
opt for the more numerous class, i.e. to assign the leaf to class P if p > n, to class
N if p < n, and to either if p = n. The first approach minimizes the sum of the
squares of the error over objects in C, while the second minimizes the sum of the ab-
solute errors over objects in C. If the aim is to minimize expected error, the second
approach might be anticipated to be superior, and indeed this has been found to be
the case.
Several studies have been carried out to see how this modified procedure holds up
under varying levels of noise (Quinlan 1983b, 1985a). One such study is outlined here,
based on the earlier-mentioned task with 551 objects and 39 binary-valued attributes.
In each experiment, the whole set of objects was artificially corrupted as described
below and used as a training set to produce a decision tree. Each object was then cor-
rupted anew, classified by this tree and the error rate determined. This process was
repeated twenty times to give more reliable averages.
In this study, values were corrupted as follows. A noise level of n percent applied
to a value meant that, with probability n percent, the true value was replaced by a
value chosen at random from among the values that could have appeared.5 Table 2
shows the results when noise levels varying from 5% to 100% were applied to the
values of the most noise-sensitive attribute, to the values of all attributes
simultaneously, and to the class information. This table demonstrates the quite dif-
ferent forms of degradation observed. Destroying class information produces a
linear increase in error so that, when all class information is noise, the resulting deci-
sion tree classifies objects entirely randomly. Noise in a single attribute does not have
a dramatic effect. Noise in all attributes together, however, leads to a relatively rapid
increase in error which reaches a peak and declines. The peak is somewhat inter-
4 ASSISTANT uses an information-based measure to perform much the same function, but no com-
parative results are available to date.
5 It might seem that the value should be replaced by an incorrect value. Consider, however, the case
of a two-valued attribute corrupted with 100% noise. If the value of each object were replaced by the (on-
ly) incorrect value, the initial attribute will have been merely inverted with no loss of information.
INDUCTION OF DECISION TREES 95
Table 2. Error rates produced by noise in a single attribute, all attributes, and class information
Noise
level
5%
10%
15%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Single
attribute
1.3%
2.5%
3.3%
4.6%
6.1%
7.6%
8.8%
9.4%
9.9%
10.4%
10.8%
10.8%
All
attributes
11.9%
18.9%
24.6%
27.8%
29.5%
30.3%
29.2%
27.5%
25.9%
26.0%
25.6%
25.9%
Class
information
2.6%
5.5%
8.3%
9.9%
14.8%
18.1%
21.8%
26.4%
27.2%
29.5%
34.1%
49.6%
esting, and can be explained as follows. Let C be a collection of objects containing
p from class P and n from class N, respectively. At noise levels around 50%, the
algorithm for constructing decision trees may still find relevant attributes to branch
on, even though the performance of this tree on unseen but equally noisy objects will
be essentially random. Suppose the tree for C classifies objects as class P with pro-
bability p/(n + p). The expected error if objects with a similar class distribution to
those in C were classified by this tree is given by
At very high levels of noise, however, the algorithm will find all attributes irrelevant
and classify everything as the more frequent class; assume without loss of generality
that this class is P. The expected error in this case is
which is less than the above expression since we have assumed that p is greater than
n. The decline in error is thus a consequence of the chi-square cutoff coming into play
as noise becomes more intense.
The table brings out the point that low levels of noise do not cause the tree-building
machinery to fall over a cliff. For this task, a 5% noise level in a single attribute pro-
duces a degradation in performance of less than 2%; a 5% noise level in all attributes
together produces a 12% degradation in classification performance; while a similar
96 J.R. QUINLAN
noise level in class information results in a 3% degradation. Comparable figures have
been obtained for other induction tasks.
One interesting point emerged from other experiments in which a correct decision
tree formed from an uncorrupted training set was used to classify objects whose
descriptions were corrupted. This scenario corresponds to forming a classification
rule under controlled and sanitized laboratory conditions, then using it to classify ob-
jects in the field. For higher noise levels, the performance of the correct decision tree
on corrupted data was found to be inferior to that of an imperfect decision tree form-
ed from data corrupted to a similar level! (This phenomenon has an explanation
similar to that given above for the peak in Table 2.) The moral seems to be that it
is counter-productive to eliminate noise from the attribute information in the train-
ing set if these same attributes will be subject to high noise levels when the induced
decision tree is put to use.
6. Unknown attribute values
The previous section examined modifications to the tree-building process that en-
abled it to deal with noisy or corrupted values. This section is concerned with an allied
problem that also arises in practice: unknown attribute values. To continue the
previous medical diagnosis example, what should be done when the patient case
histories that are to form the training set are incomplete?
One way around the problem attempts to fill in an unknown value by utilizing in-
formation provided by context. Using the previous notation, let us suppose that a
collection C of objects contains one whose value of attribute A is unknown. ASSIS-
TANT (Kononenko et al, 1984) uses a Bayesian formalism to determine the prob-
ability that the object has value Ai of A by examining the distribution of values of
A in C as a function of their class. Suppose that the object in question belongs to
class P. The probability that the object has value Ai for attribute A can be expressed
as
where the calculation of pi and p is restricted to those members of C whose value of
A is known. Having determined the probability distribution of the unknown value
over the possible values of A, this method could either choose the most likely value
or divide the object into fractional objects, each with one possible value of A,
weighted according to the probabilities above.
Alen Shapiro (private communication) has suggested using a decision-tree ap-
proach to determine the unknown values of an attribute. Let C' be the subset of C
consisting of those objects whose value of attribute A is defined. In C', the original
INDUCTION OF DECISION TREES 97
Table 3. Proportion of times that an unknown attribute value is replaced by an incorrect value
Replacement
method
Bayesian
Decision tree
Most common value
1
28%
19%
28%
Attribute
2
27%
22%
27%
3
38%
19%
40%
class (P or N) is regarded as another attribute while the value of attribute A becomes
the 'class' to be determined. That is, C' is used to construct a decision tree for deter-
mining the value of attribute A from the other attributes and the class. When con-
structed, this decision tree can be used to 'classify' each object in C - C' and the
result assigned as the unknown value of A.
Although these methods for determining unknown attribute values look good on
paper, they give unconvincing results even when only a single value of one attribute
is unknown; as might be expected, their performance is much worse when several
values of several attributes are unknown. Consider again the 551-object 39-attribute
task. We may ask how well the methods perform when asked to fill in a single
unknown attribute value. Table 3 shows, for each of the three most important at-
tributes, the proportion of times each method fails to replace an unknown value by
its correct value. For comparison, the table also shows the same figure for the simple
strategy: always replace an unknown value of an attribute with its most common
value. The Bayesian method gives results that are scarcely better than those given by
the simple strategy and, while the decision-tree method uses more context and is
thereby more accurate, it still gives disappointing results.
Rather than trying to guess unknown attribute values, we could treat 'unknown'
as a new possible value for each attribute and deal with it in the same way as other
values. This can lead to an anomalous situation, as shown by the following example.
Suppose A is an attribute with values {A1, A2} and let C be a collection of objects
such that
giving a value of 1 bit for E(A). Now let A' be an identical attribute except that one
of the objects with value A1 of A has an unknown value of A'. A' has the values
(A'1 , A'2, A ' 3 = unknown), so the corresponding values might be
98 J.R. QUINLAN
resulting in a value of 0.84 bits for E(A'). In terms of the selection criterion
developed earlier, A' now seems to give a higher information gain than A. Thus, hav-
ing unknown values may apparently increase the desirability of an attribute, a result
entirely opposed to common sense. The conclusion is that treating 'unknown' as a
separate value is not a solution to the problem.
One strategy which has been found to work well is as follows. Let A be an attribute
with values {A1, A2, ... Av}. For some collection C of objects, let the numbers of
objects with value Ai of A be pi and ni, and let pu and nu denote the numbers of ob-
jects of class P and N respectively that have unknown values of A. When the informa-
tion gain of attribute A is assessed, these objects with unknown values are distributed
across the values of A in proportion to the relative frequency of these values in C.
Thus the gain is assessed as if the true value of pi were given by
where
and similarly for ni. (This expression has the property that unknown values can only
decrease the information gain of an attribute.) When an attribute has been chosen
by the selection criterion, objects with unknown values of that attribute are discarded
before forming decision trees for the subsets { C i } .
The other half of the story is how unknown attribute values are dealt with during
classification. Suppose that an object is being classified using a decision tree that
wishes to branch on attribute A, but the object's value of attribute A is unknown.
The correct procedure would take the branch corresponding to the real value Ai but,
since this value is unknown, the only alternative is to explore all branches without
forgetting that some are more probable than others.
Conceptually, suppose that, along with the object to be classified, we have been
passed a token with some value T. In the situation above, each branch of Ai is then
explored in turn, using a token of value
T • ratioi
i.e. the given token value is distributed across all possible values in proportion to the
ratios above. The value passed to a branch may be distributed further by subsequent
tests on other attributes for which this object has unknown values. Instead of a single
path to a leaf, there may now be many, each qualified by its token value. These token
values at the leaves are summed for each class, the result of the classification being
INDUCTION OF DECISION TREES 99
Figure 5. Error produced by unknown attribute values.
that class with the higher value. The distribution of values over the possible classes
might also be used to compute a confidence level for the classification.
Straightforward though it may be, this procedure has been found to give a very
graceful degradation as the incidence of unknown values increases. Figure 5 sum-
marizes the results of an experiment on the now-familiar task with 551 objects and
39 attributes. Various 'ignorance levels' analogous to the earlier noise levels were ex-
plored, with twenty repititions at each level. For each run at an ignorance level of
m percent, a copy of the 551 objects was made, replacing each value of every attribute
by 'unknown' with m percent probability. A decision tree for these (incomplete)
objects was formed as above, and then used to classify a new copy of each object
corrupted in the same way. The figure shows that the degradation of performance
with ignorance level is gradual. In practice, of course, an ignorance level even as high
as 10% is unlikely - this would correspond to an average of one value in every ten
of the object's description being unknown. Even so, the decision tree produced from
such a patchy training set correctly classifies nearly ninety percent of objects that also
have unknown values. A much lower level of degradation is observed when an object
with unknown values is classified using a correct decision tree.
This treatment has assumed that no information whatsoever is available regarding
an unknown attribute. Catlett (1985) has taken this approach a stage further by
allowing partial knowledge of an attribute value to be stated in Shafer notation
(Garvey, Lowrance and Fischler, 1981). This notation permits probabilistic asser-
tions to be made about any subset or subsets of the possible values of an attribute
that an object might have.
100 J.R. QUINLAN
7. The selection criterion
Attention has recently been refocussed on the evaluation function for selecting the
best attribute-based test to form the root of a decision tree. Recall that the criterion
described earlier chooses the attribute that gains most information. In the course of
their experiments, Bratko's group encountered a medical induction problem in which
the attribute selected by the gain criterion ('age of patient', with nine value ranges)
was judged by specialists to be less relevant than other attributes. This situation was
also noted on other tasks, prompting Kononenko et al (1984) to suggest that the gain
criterion tends to favor attributes with many values.
Analysis supports this finding. Let A be an attribute with values A1, A2, ... Av
and let A' be an attribute formed from A by splitting one of the values into two. If
the values of A were sufficiently fine for the induction task at hand, we would not
expect this refinement to increase the usefulness of A. Rather, it might be anticipated
that excessive fineness would tend to obscure structure in the training set so that A'
was in fact less useful than A. However, it can be proved that gain(A') is greater than
or equal to gain(A), being equal to it only when the proportions of the classes are
the same for both subdivisions of the original value. In general, then, gain(A') will
exceed gain(A) with the result that the evaluation function of Section 4 will prefer
A' to A. By analogy, attributes with more values will tend to be preferred to at-
tributes with fewer.
As another way of looking at the problem, let A be an attribute with random values
and suppose that the set of possible values of A is large enough to make it unlikely
that two objects in the training set have the same value for A. Such an attribute would
have maximum information gain, so the gain criterion would select it as the root of
the decision tree. This would be a singularly poor choice since the value of A, being
random, contains no information pertinent to the class of objects in the training set.
ASSISTANT (Kononenko et al, 1984) solves this problem by requiring that all tests
have only two outcomes. If we have an attribute A as before with v values A1, A2,
... Av, the decision tree no longer has a branch for each possible value. Instead, a
subset S of the values is chosen and the tree has two branches, one for all values in
the set and one for the remainder. The information gain is then computed as if all
values in S were amalgamated into one single attribute value and all remaining values
into another. Using this selection criterion (the subset criterion), the test chosen for
the root of the decision tree uses the attribute and subset of its values that maximizes
the information gain. Kononenko et al report that this modification led to smaller
decision trees with an improved classification performance. However, the trees were
judged to be less intelligible to human beings, in agreement with a similar finding of
Shepherd (1983).
Limiting decision trees to a binary format harks back to CLS, in which each test
was of the form 'attribute A has value Ai', with two branches corresponding to true
and false. This is clearly a special case of the test implemented in ASSISTANT, which
INDUCTION OF DECISION TREES 101
permits a set of values, rather than a single value, to be distinguished from the others.
It is also worth noting that the method of dealing with attributes having continuous
values follows the same binary approach. Let A be such an attribute and suppose that
the distinct values of A that occur in C are sorted to give the sequence V1, V2, . . . .
Vk. Each pair of values Vi, Vi+ 1 suggests a possible threshold
that divides the objects of C into two subsets, those with a value of A above and
below the threshold respectively. The information gain of this division can then be
investigated as above.
If all tests must be binary, there can be no bias in favor of attributes with large
numbers of values. It could be argued, however, that ASSISTANT'S remedy has
undesirable side-effects that have to be taken into account. First, it can lead to deci-
sion trees that are even more unintelligible to human experts than is ordinarily the
case, with unrelated attribute values being grouped together and with multiple tests
on the same attribute.
More importantly, the subset criterion can require a large increase in computation.
An attribute A with v values has 2V value subsets and, when trivial and symmetric
subsets are removed, there are still 2 V - 1 -1 different ways of specifying the
distinguished subset of attribute values. The information gain realized with each of
these must be investigated, so a single attribute with v values has a computational
requirement similar to 2V - 1 -1 binary attributes. This is not of particular conse-
quence if v is small, but the approach would appear infeasible for an attribute with
20 values.
Another method of overcoming the bias is as follows. Consider again our training
set containing p and n objects of class P and N respectively. As before, let attribute
A have values A1, A2, ... Av and let the numbers of objects with value Ai of attribute
A be pi and ni respectively. Enquiring about the value of attribute A itself gives rise
to information, which can be expressed as
IV(A) thus measures the information content of the answer to the question, 'What
is the value of attribute A?' As discussed earlier, gain(A) measures the reduction in
the information requirement for a classification rule if the decision tree uses attribute
A as a root. Ideally, as much as possible of the information provided by determining
the value of an attribute should be useful for classification purposes or, equivalently,
as little as possible should be 'wasted'. A good choice of attribute would then be one
for which the ratio
102 J.R. QUINLAN
gain(A) / IV(A)
is as large as possible. This ratio, however, may not always be defined - IV(A) may
be zero - or it may tend to favor attributes for which IV(A) is very small. The gain
ratio criterion selects, from among those attributes with an average-or-better gain,
the attribute that maximizes the above ratio.
This can be illustrated by returning to the example based on the training set of
Table 1. The information gain of the four attributes is given in Section 4 as
gain(outlook) = 0.246 bits
gain(temperature) = 0.029 bits
gain(humidity) = 0.151 bits
gain(windy) = 0.048 bits
Of these, only outlook and humidity have above-average gain. For the outlook at-
tribute, five objects in the training set have the value sunny, four have overcast and
five have rain. The information obtained by determining the value of the outlook at-
tribute is therefore
Similarly,
So,
The gain ratio criterion would therefore still select the outlook attribute for the root
of the decision tree, although its superiority over the humidity attribute is now much
reduced.
The various selection criteria have been compared empirically in a series of ex-
periments (Quinlan, 1985b). When all attributes are binary, the gain ratio criterion
has been found to give considerably smaller decision trees: for the 551-object task,
it produces a tree of 143 nodes compared to the smallest previously-known tree of
175 nodes. When the task includes attributes with large numbers of values, the subset
criterion gives smaller decision trees that also have better predictive performance, but
can require much more computation. However, when these many-valued attributes
are augmented by redundant attributes which contain the same information at a
INDUCTION OF DECISION TREES 103
lower level of detail, the gain ratio criterion gives decision trees with the greatest
predictive accuracy. All in all, these experiments suggest that the gain ratio criterion
does pick a good attribute for the root of the tree. Testing an attribute with many
values, however, will fragment the training set C into very small subsets {Ci} and
the decision trees for these subsets may then have poor predictive accuracy. In such
cases, some mechanism such as value subsets or redundant attributes is needed to pre-
vent excessive fragmentation.
The three criteria discussed here are all information-based, but there is no reason
to suspect that this is the only possible basis for such criteria. Recall that the
modifications to deal with noise barred an attribute from being used in the decision
tree unless it could be shown to be relevant to the class of objects in the training set.
For any attribute A, the value of the statistic presented in Section 5, together with
the number v of possible values of A, determines the confidence with which we can
reject the null hypothesis that an object's value of A is irrelevant to its class. Hart
(1985) has proposed that this same test could function directly as a selection criterion:
simply pick the attribute for which this confidence level is highest. This measure takes
explicit account of the number of values of an attribute and so may not exhibit bias.
Hart notes, however, that the chi-square test is valid only when the expected values
of p' i and n' i are uniformly larger than four. This condition could be violated by
a set C of objects either when C is small or when few objects in C have a particular
value of some attribute, and it is not clear how such sets would be handled. No em-
pirical results with this approach are yet available.
8. Conclusion
The aim of this paper has been to demonstrate that the technology for building deci-
sion trees from examples is fairly robust. Current commercial systems are powerful
tools that have achieved noteworthy successes. The groundwork has been done for
advances that will permit such tools to deal even with noisy, incomplete data typical
of advanced real-world applications. Work is continuing at several centers to im-
prove the performance of the underlying algorithms.
Two examples of contemporary research give some pointers to the directions in
which the field is moving. While decision trees generated by the above systems are
fast to execute and can be very accurate, they leave much to be desired as representa-
tions of knowledge. Experts who are shown such trees for classification tasks in their
own domain often can identify little familiar material. It is this lack of familiarity
(and perhaps an underlying lack of modularity) that is the chief obstacle to the use
of induction for building large expert systems. Recent work by Shapiro (1983) offers
a possible solution to this problem. In his approach, called Structured Induction, a
rule-formation task is tackled in the same style as structured programming. The task
is solved in terms of a collection of notional super-attributes, after which the subtasks
104 J.R. QUINLAN
of inducing classification rules to find the values of the super-attributes are ap-
proached in the same top-down fashion. In one classification problem studied, this
method reduced a totally opaque, large decision tree to a hierarchy of nine small deci-
sion trees, each of which 'made sense' to an expert.
ID3 allows only two classes for any induction task, although this restriction has
been removed in most later systems. Consider, however, the task of developing a rule
from a given set of examples for classifying an animal as a monkey, giraffe, elephant,
horse, etc. A single decision tree could be produced in which these various classes ap-
peared as leaves. An alternative approach taken by systems such as INDUCE
(Michalski, 1980) would produce a collection of classification rules, one to
discriminate monkeys from non-monkeys, another to discriminate giraffes from
non-giraffes, and so on. Which approach is better? In a private communication,
Marcel Shoppers has set out an argument showing that the latter can be expected to
give more accurate classification of objects that were not in the training set. The
multi-tree approach has some associated problems - the separate decision trees may
classify an animal as both a monkey and a giraffe, or fail to classify it as anything,
for example - but if these can be sorted out, this approach may lead to techniques
for building more reliable decision trees.
Acknowledgements
It is a pleasure to acknowledge the stimulus and suggestions provided over many
years by Donald Michie, who continues to play a central role in the development of
this methodology. Ivan Bratko, Igor Kononenko, Igor Mosetic and other members
of Bratko's group have also been responsible for many insights and constructive
criticisms. I have benefited from numerous discussions with Ryszard Michalski, Alen
Shapiro, Jason Catlett and other colleagues. I am particularly grateful to Pat Langley
for his careful reading of the paper in draft form and for the many improvements
he recommended.
References
Buchanan, B.G., & Mitchell, T.M. (1978). Model-directed learning of production rules. In D.A. Water-
man, F. Hayes-Roth (Eds.), Pattern directed inference systems. Academic Press.
Carbonell, J.G., Michalski, R.S., & Mitchell, T.M. (1983). An overview of machine learning, In R.S.
Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), Machine learning: An artificial intelligence ap-
proach. Palo Alto: Tioga Publishing Company.
Catlett, J. (1985). Induction using the shafer representation (Technical report). Basser Department of
Computer Science, University of Sydney, Australia.
Dechter, R., & Michie, D. (1985). Structured induction of plans and programs (Technical report). IBM
Scientific Center, Los Angeles, CA.
INDUCTION OF DECISION TREES 105
Feigenbaum, E.A., & Simon, H.A. (1963). Performance of a reading task by an elementary perceiving
and memorizing program, Behavioral Science, 8.
Feigenbaum, E.A. (1981). Expert systems in the 1980s. In A. Bond (Ed.), State of the art report on
machine intelligence. Maidenhead: Pergamon-Infotech.
Garvey, T.D., Lowrance, J.D., & Fischler, M.A. (1981). An inference technique for integrating
knowledge from disparate sources. Proceedings of the Seventh International Joint Conference on Arti-
ficial Intelligence. Vancouver, B.C., Canada: Morgan Kaufmann.
Hart, A.E. (1985). Experience in the use of an inductive system in knowledge engineering. In M.A. Bramer
(Ed.), Research and development in expert systems. Cambridge University Press.
Hogg, R.V., & Craig, A.T. (1970). Introduction to mathematical statistics. London: Collier-Macmillan.
Hunt, E.B. (1962). Concept learning: An information processing problem. New York: Wiley.
Hunt, E.B., Marin, J., & Stone, P.J. (1966). Experiments in induction. New York: Academic Press.
Kononenko, I., Bratko, I., & Roskar, E. (1984). Experiments in automatic learning of medical diagnostic
rules (Technical report). Jozef Stefan Institute, Ljubljana, Yugoslavia.
Langley, P., Bradshaw, G.L., & Simon, H.A. (1983). Rediscovering chemistry with the BACON system.
In R.S. Michalski, J.G. Carbonell and T.M. Mitchell (Eds.), Machine learning: An artificial intel-
ligence approach. Palo Alto: Tioga Publishing Company.
Michalski, R.S (1980). Pattern recognition as rule-guided inductive inference. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 2.
Michalski, R.S., & Stepp, R.E. (1983). Learning from observation: conceptual clustering. In R.S.
Michalski, J.G. Carbonell & T.M. Mitchell (Eds.), Machine learning: An artificial intelligence ap-
proach. Palo Alto: Tioga Publishing Company.
Michie, D. (1982). Experiments on the mechanisation of game-learning 2 - Rule-based learning and the
human window. Computer Journal 25.
Michie, D. (1983). Inductive rule generation in the context of the Fifth Generation. Proceedings of the
Second International Machine Learning Workshop. University of Illinois at Urbana-Champaign.
Michie, D. (1985). Current developments in Artificial Intelligence and Expert Systems. In International
Handbook of Information Technology and Automated Office Systems. Elsevier.
Nilsson, N.J. (1965). Learning machines, New York: McGraw-Hill.
O'Keefe, R.A. (1983). Concept formation from very large training sets. In Proceedings of the Eighth
International Joint Conference on Artificial Intelligence. Karlsruhe, West Germany: Morgan
Kaufmann.
Patterson, A., & Niblett, T. (1983). ACLS user manual. Glasgow: Intelligent Terminals Ltd.
Pearl, J. (1978a). Entropy, information and rational decisions (Technical report). Cognitive Systems
Laboratory, University of California, Los Angeles.
Pearl, J. (1978b). On the connection between the complexity and credibility of inferred models. Interna-
tional Journal of General Systems, 4.
Quinlan, J.R. (1969). A task-independent experience gathering scheme for a problem solver. Proceedings
of the First International Joint Conference on Artificial Intelligence. Washington, D.C.: Morgan
Kaufmann.
Quinlan, J.R. (1979). Discovering rules by induction from large collections of examples. In D. Michie
(Ed.), Expert systems in the micro electronic age. Edinburgh University Press.
Quinlan, J.R. (1982). Semi-autonomous acquisition of pattern-based knowledge. In J.E. Hayes, D.
Michie & Y-H. Pao (Eds.), Machine intelligence 10. Chichester: Ellis Horwood.
Quinlan, J.R. (1983a). Learning efficient classification procedures and their application to chess
endgames. In R.S. Michalski, J.G. Carbonell & T.M. Mitchell, (Eds.), Machine learning: An artificial
intelligence approach. Palo Alto: Tioga Publishing Company.
Quinlan, J.R. (1983b). Learning from noisy data, Proceedings of the Second International Machine
Learning Workshop. University of Illinois at Urbana-Champaign.
Quinlan, J.R. (1985a). The effect of noise on concept learning. In R.S. Michalski, J.G. Carbonell & T.M.
106 J.R. QUINLAN
Mitchell (Eds.), Machine learning. Los Altos: Morgan Kaufmann (in press).
Quinlan, J.R. (1985b). Decision trees and multi-valued attributes. In J.E. Hayes & D. Michie (Eds.),
Machine intelligence 11. Oxford University Press (in press).
Sammut, C.A. (1985). Concept development for expert system knowledge bases. Australian Computer
Journal 17.
Samuel, A. (1967). Some studies in machine learning using the game of checkers II: Recent progress. IBM
J. Research and Development 11.
Shapiro, A. (1983). The role of structured induction in expert systems. Ph.D. Thesis, University of
Edinburgh.
Shepherd, B. A. (1983). An appraisal of a decision-tree approach to image classification. Proceedings of
the Eighth International Joint Conference on Artificial Intelligence. Karlsruhe, West Germany:
Morgan Kaufmann.
Winston, P.H. (1975). Learning structural descriptions from examples. In P.H. Winston (Ed.), The
psychology of computer vision. McGraw-Hill.
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תכנה-וחומר-עזר\Optimizing-a-batch-manufacturing-process-through-interpretable-data-mining-models\JIM2009.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
J Intell Manuf
DOI 10.1007/s10845-008-0148-7
Optimizing a batch manufacturing process through interpretable data
mining models
Mark Last · Guy Danon · Sholomo Biderman ·
Eli Miron
Received: 12 December 2007 / Accepted: 23 June 2008
© Springer Science+Business Media, LLC 2008
Abstract In this paper, we present a data mining based
methodology for optimizing the outcome of a batch manufac-
turing process. Predictive data mining techniques are applied
to a multi-year set of manufacturing data with the purpose
of reducing the variation of a crystal manufacturing process,
which suffers from frequent fluctuations of the average out-
going yield. Our study is focused on specific defects that are
the most common causes for scraping a manufactured crystal.
A set of probabilistic rules explaining the likelihood of each
defect as a function of interaction between the controllable
variables are induced using the single-target and the multi-
target Information Network algorithms. The rules clearly
define the worst and the best conditions for the manufacturing
process, also providing a complete explanation of all major
fluctuations in the outgoing quality observed over the recent
years. In addition, we show that an early detection of nearly
the same predictive model was possible almost two years
before the end of the data collection period, which could save
many of the flawed crystals. The paper provides a detailed
description of the optimization process, including the deci-
sions taken at various stages and their outcomes. Conclusions
applicable to similar engineering tasks are also outlined.
M. Last (B) · G. Danon
Department of Information Systems Engineering, Ben-Gurion
University of the Negev, Beer-Sheva 84105, Israel
e-mail: mlast@bgu.ac.il
G. Danon
e-mail: guyda@bgu.ac.il
S. Biderman
Rotem Industries Ltd., Dimona, Israel
e-mail: sbider@netvision.net.il
E. Miron
Nuclear Research Center–Negev, P.O. Box 9001,
Beer-Sheva, Israel
e-mail: emiron@nrcn.org.il
Keywords Data mining · Process optimization · Rule
induction · Predictive modeling · Information networks
Introduction
Outgoing quality assurance is a critical issue in irreversible
manufacturing processes, where certain defects found in the
final products cannot be removed or repaired. Consequently,
each finished product that does not meet the quality requi-
rements has to be disposed, or “scrapped”, thus decreasing
the overall “yield” of the manufacturing process. Though the
processing of each manufacturing unit (e.g., a lot or a batch)
involves exactly the same sequence of steps and each step is
performed under a rigorously defined set of conditions, the
final product quality may still be subject to significant varia-
tions due to a large number of uncontrollable factors such as
changes in environmental variables (temperature, humidity,
etc.). The complex, nonlinear nature of these processes often
results in a poor understanding of the true relationships bet-
ween controllable and uncontrollable factors on one hand,
and the product quality parameters on the other hand.
In this paper, we present a novel process optimization
methodology based on the Data Mining approach. The ope-
rating model relating the process quality (output variables) to
controllable (manipulated) variables is constructed from past
operational data using single-target and multi-target Infor-
mation Network (IN) algorithms for induction of oblivious
decision-trees (Maimon and Last 2000; Last and Maimon
2004; Last 2004). The proposed optimization methodology
is applied to the data of a batch manufacturing process, which
suffers from high variability of the outgoing yield.
The IN algorithms were chosen for predicting the process
outcomes for several reasons. First, as shown in (Last and
Maimon 2004), they tend to produce considerably smaller
123
J Intell Manuf
models than other decision-tree algorithms of similar accuracy.
The interpretability of the induced models is critical here for
two main reasons: on one hand, the operators need a small
set of simple rules to control the process parameters in the
future; on the other hand, the process engineers are highly
interested to explain the ups and downs in the process qua-
lity that were recorded in the past. Second, an Information
Network associates probability estimates rather than catego-
rical predictions with each leaf node. The leaf nodes with the
highest probability of a successful outcome can be directly
converted into sets of operational recommendations without
any manual involvement of domain experts. An additional
benefit of the Information Network methodology is the auto-
mated removal of irrelevant and redundant attributes from
the induced models.
The rest of this paper is organized as follows. “Litera-
ture survey” section covers two related areas: optimization
of manufacturing processes and induction of predictive data
mining models from batch manufacturing data. “Crystals
manufacturing process” section provides a brief description
of the crystal manufacturing process, which served as our
case study. “Optimization with Information Network models”
section presents an overview of Information Networks and
their suggested use for optimization of complex, nonlinear
processes. Then, in “mining crystal quality data” section,
we utilize the Information Network algorithms to induce
models predicting the crystals quality as a function of control-
lable parameters. In “optimizing the crystal quality with IN
models” section, we proceed with converting the predictive
models into process recommendations, while showing how
the same models can be used to explain the past behavior of
the outgoing yield. Conclusions are outlined in the “Conclu-
sions” section.
Literature survey
In the absence of an accurate mathematical model of the
manufacturing process, it takes time to reverse each tempo-
rary decline in the outgoing quality. The classical statistical
approach to the problem of optimizing the outcome (“res-
ponse”) of complex manufacturing processes, especially in
the chemical industry, is the Response Surface Methodology,
or RSM (Myers and Montgomery 2002). To optimize the res-
ponse of a given process, the process engineer needs to deter-
mine the optimal levels of “independent variables”, which
are subject to his/her control. The Response Surface Metho-
dology suggests an efficient Design of Experiments (DOE)
strategy for exploring the space of process conditions, deve-
loping an approximate relationship (“response function”)
between the quality and process variables, and finally opti-
mizing the levels of the process variables to meet the quality
requirements. The RSM is a multi-phase process that builds
mostly upon the results of carefully designed and potentially
costly experiments rather than on the actual process measu-
rements taken in the past.
Lin and Lin (2005) present a fuzzy logic based approach
for optimization of a machining process with multiple res-
ponses. The entire machining parameter space is studied
using a nine row orthogonal array. Multiple quality charac-
teristics of the corresponding nine experiments are combi-
ned by a fuzzy logic unit resulting in “grey-fuzzy reasoning
grades” of each configuration setting. The highest reasoning
grade leads to the choice of the optimal machining parame-
ters for the given process.
Liau et al. (2004) build an expert system for finding the
optimal operating conditions of a crude oil distillation pro-
cess. The operating model relating the product quality (output
variables) to uncontrollable and controllable (manipulated)
variables is constructed by applying the ANN (Artificial Neu-
ral Network) algorithm to collected experimental data.
Though the induced operating model cannot be directly inter-
preted by the process engineers, it can be utilized by an
optimization toolbox to solve the non-linear constrained opti-
mization problem of the given process.
Data mining techniques (see Han and Kamber 2006) pro-
vide a valid alternative for control and optimization of manu-
facturing processes by utilizing the historical operational data
rather than spending a considerable effort on expensive engi-
neering experiments. Thus, Babu and Frieda (1993) use artifi-
cial neural networks (ANN) to induce predictive models from
past operational data of an autoclave curing process used to
manufacture composite materials. The variables associated
with the process are partitioned into the following categories:
the initial state of the system, input disturbances, manipu-
lated (controllable) inputs, and intermediate measurements.
The induced ANN model is used as a “black-box” predicting
the expected quality of each batch. The batch neurocontroller
is calculating the optimal values of the manipulated inputs
using the predictive model and a standard gradient optimi-
zation package. The optimal process conditions for a given
batch can be determined either off-line (before the start of the
process) or on-line (using intermediate measurements from
the batch). All optimization results in (Babu and Frieda 1993)
were obtained using a simulation model of the actual process,
since, as indicated by the authors, it is difficult to generate
a good distribution of operational data for training the algo-
rithm.
Another attempt to optimize parameters of a manufac-
turing process using an artificial neural network is done in
(Cook et al. 2000), where the trained ANN model is again
used as a quality predictor. However, in this work, the optimal
values of the process parameters are determined by a genetic
algorithm (GA). The combination of a neural network with a
genetic algorithm is used in the different stages of a particle-
board manufacturing process to obtain the desired strength
123
J Intell Manuf
of the final board. Since a neural network is a “black-box”
model, it cannot significantly improve the understanding of
the actual relationships between process and product para-
meters. Not surprisingly, the ability of the ANN-GA tool to
explain its recommendations is also quite limited.
One of the most natural ways to obtain an interpretable,
“white-box” model of a manufacturing process would be
decision-tree induction. Thus, Famili (1994) has used Quin-
lan’s ID3 algorithm (Quinlan 1986) to induce decision-tree
models of electrochemical machining (ECM) process. An
upper and a lower limit were defined for each one of the 26
quality parameters resulting in 52 classification problems.
The decision tree induced for each classification problem
was converted into a set of production rules. However, the
total number of obtained rules (320) was far beyond the
capability of operators or process engineers to use. Thus,
a significant amount of rules had to be manually removed or
simplified based on a complex set of criteria. At the end of
this process, only 10 useful rules (out of 320) were left and
converted into recommendations for the process engineers
and operators.
Hur et al. (2006) suggest a hybrid process diagnosis
system, which infers cause-and-effect rules for the manu-
facturing process condition by using the C4.5 decision tree
algorithm (Quinlan 1993) combined with a genetic algorithm
(GA). The real training data was obtained from a coil-spring
manufacturing company in Korea. If the current condition of
the process is diagnosed as an abnormal condition, the most
effective maintenance action is recommended by the sys-
tem. To select an appropriate maintenance action, the authors
construct a decision network which represents maintenance
actions as possible paths from the detected abnormal node
(abnormal condition) to the normal node (normal condition)
based on the cause-and-effect rules inferred by hybrid lear-
ning. The most appropriate maintenance action is selected
by the Analytical Hierarchy Process (AHP).
Last and Kandel (2001) apply the Information Network
(IN) algorithm to the WIP (Work-in-Process) datacollected
in a semiconductor plant. The induced models are aimed at
predicting the line yield and the flow time of each manufac-
turing batch. The IN predictive accuracy is shown to be com-
parable with the C4.5 decision-tree algorithm, while C4.5
tends to produce much more complex models than IN. Fuzzy-
based techniques of automated perception are further used
for making the sets of if-then rules extracted from the IN
models more compact and interpretable. Braha et al. (2007)
show that a combination of classifiers, such as C4.5 and IN,
trained on the same semiconductor dataset can increase the
utility of a decision-making process related to each individual
batch. In (Braha et al. 2007), two available actions include
‘scrap a batch’ vs. ‘continue production’, whereas the ope-
rating conditions of the batch manufacturing process remain
changed.
The process optimization methodology presented in this
paper has the following original contributions vs. the existing
work:
• Using the compact and interpretable Information Net-
work (IN) classification models for optimizing the qua-
lity of a batch manufacturing process. Previous works
either used the “black-box” Artificial Neural Network
(ANN) models (Babu and Frieda 1993; Liau et al. 2004;
Cook et al. 2000) or the standard decision-tree algorithms
(Famili 1994; Hur et al. 2006), which tend to produce
incomprehensibly large sets of prediction rules.
• Introducing an automated methodology for optimizing
a set of independent quality dimensions using a multi-
target classification model. Previous works aimed at opti-
mizing multiple quality parameters (such as Famili 1994)
assumed a manual integration of several single-target
classification models.
• Using the induced data mining models as an explora-
tive tool for explaining the variability in the past failure
rates. Such analysis, unmentioned in previous works on
process optimization with data mining, can significantly
contribute to the credibility of model recommendations
in the view of process engineers.
• Finally, to the best of our knowledge, this study is the
first published application of data mining techniques in
the crystal manufacturing industry.
Crystals manufacturing process
The crystal growth division at Rotem Industries LTD has
developed a unique technique (Gradient Solidification
Method—GSM) for growing large sapphire dome-shaped
crystals (see Horowitz et al. 1987; Biderman et al. 1991;
Horowitz et al. 1993). This process significantly shortens the
production time of Sapphire domes, which constitute the
transparent “nose” of smart anti-aircraft missiles that rely
on detection of Infra Red radiation.
The near-net shaped single-crystal Sapphire domes are
grown in double-walled molybdenum crucibles. The cru-
cible, loaded with sapphire powder and crackles, is placed in
a vacuum furnace, which includes graphite heating element
and molybdenum heat shields. The growth is performed by
melting of the raw material via heating the crucible, followed
by temperature reduction to obtain the various growth rates
required at different stages of the process. Typical duration
of growing a medium sized crystal (100 mm diameter and
50–70 mm height) is about one week.
The temperature gradients, which are required for pro-
per growth, may cause quality problems since the upper
part of the crucible can be exposed to elevated tempera-
tures that enhance chemical reactions between the furnace
123
J Intell Manuf
constituents. The reaction products could then enter the crys-
tal, usually causing defects. The growth parameters are inten-
tionally tweaked in an attempt to increase the yield. However,
the interpretation (whether the parameter change improved
the yield) is not immediate due to the long growth period and
the non-deterministic occurrence of the problems.
In order to understand the influence of the various para-
meters, we built a database that includes full listing of the
growth processes performed over recent ten years. About 50
variable parameters and 50 measured parameters are recor-
ded for each growth. The rejects are related to one or more
reason (of about 30 possible).
This database has been used for the data mining process.
Optimization with Information Network models
The single-target Information Network (IN)
The single-target Information Network (Maimon and Last
2000) is an oblivious tree-like classification model, which is
designed to minimize the total number of input (predictive)
attributes. It is similar to the Oblivious Read-Once Decision
Graph (OODG) model (Kohavi and Li 1995). “Read-once”
means that each nominal feature is tested at most once along
any path, which is a common property of most decision-tree
algorithms such as C4.5 (Quinlan 1993). The name “obli-
vious” indicates the fact that all nodes at a given level are
labeled by the same feature. The same ordering restriction is
imposed by Bryant (1986) on Function Graphs. An Informa-
tion Network has nearly the same structure as an oblivious
read-once decision graph with two important differences: it
extends the “read-once” restriction of (Kohavi and Li 1995)
to continuous features by allowing multi-way splits of a conti-
nuous domain at the same level and it associates probability
estimates rather than categorical predictions with each leaf
node.
The underlying principle of the Information Network (IN)
induction algorithm (Last and Maimon 2004) is to construct a
multi-layered network that maximizes the statistically signi-
ficant Mutual Information (MI) between input and target
attributes. Each hidden layer is related to a specific input attri-
bute and represents the interaction between this input attri-
bute and those associated with previous layers. The first layer
(layer 0) includes only the root node and is not associated with
any input attribute. In each iteration, the input attribute having
the maximum conditional mutual information is selected by
the algorithm resulting in adding a new hidden layer to the
network. The Information Network construction algorithm
is using a pre-pruning strategy: a node is split if this proce-
dure brings about a statistically significant decrease in the
entropy (equal to increase in the mutual information) of the
target attribute. This entropy decrease is called “conditional
mutual information” between an input attribute and a target
attribute. If none of the remaining input attributes provides
statistically significant conditional mutual information, the
network construction stops.
For each candidate input (predictive) attribute Ai in a layer
n, the algorithm calculates the conditional mutual informa-
tion of Ai and the target (classification) attribute T given n-1
input attributes X1, . . ., Xn−1 by the following formula:
MI(T ; Ai/X1, . . . , Xn−1) =
∑
z∈Ln−1
MI(T ; Ai/z) (1)
where MI(T ; Ai/z) is the conditional mutual information
of a candidate input attribute Ai with the target attribute T
given a terminal node z in the layer n −1 (denoted by Ln−1).
Like in any decision tree, each terminal (leaf) node z of the
k-th layer of an Information Network represents a specific
conjunction of values of k predictive attributes associated
with k hidden layers, respectively. However, due to the read-
once nature of Information Networks, the order of testing
predictive attributes is identical at all leaf nodes.
For nominal predictive attributes, the conditional mutual
information of a candidate input (predictive) attribute Ai and
the target (classification) attribute T given a node z is calcu-
lated by the following formula:
MI(T ; Ai/z) =
MT −1∑
t=o
Mi −1∑
j=0
P(Ct ; Vi j ; z)
∗ log P(V
t
i j/z)
P(Vi j/z) ∗ P(Ct/z) (2)
where
MT /Mi : number of distinct values (“classes”) of the target
attribute T / candidate input attribute i , respectively;
P(Vi j/z): an estimated conditional probability of a value
j of the candidate input attribute i given the node z; it is
calculated as the proportion of instances at the node z, where
the value of the candidate input attribute i is j .
P(V ti j/z): an estimated conditional probability of a value
j of the candidate input attribute i and a value (“class”) t
of the target attribute T given the node z; it is calculated as
the proportion of instances at the node z, where the value
of the candidate input attribute i is j and the value (“class”)
of the target attribute T is t .
P(Ct/z): an estimated conditional probability of a value
(“class”) t of the target attribute T given the node z; it is
calculated as the proportion of instances at the node z, where
the value of the target attribute T is t .
P(Ct ; Vi j ; z): an estimated joint probability of a value
(“class”) t of the target attribute T , a value j of the can-
didate input attribute i , and the node z; it is calculated as
the proportion of all training instances, where the value of
123
J Intell Manuf
the candidate input attribute i is j , the value (“class”) of the
target attribute T is t , and the node is z.
It is important to note that the probability calculations
above skip the instances where the values of Ai and/or T
attributes are missing.
The conditional entropy of the target (classification) attri-
bute can only be calculated with respect to input attributes
taking a finite number of values. For continuous predictive
attributes, the algorithm performs discretization “on-the-fly”
by recursively finding a binary partition of an input attri-
bute that minimizes the conditional entropy. The conditional
mutual information of partitioning an interval S of a candi-
date input attribute at the threshold Th and the target attribute
T given a node z is calculated by the following formula (Last
and Maimon 2004):
MI(T h; T/S, z) =
MT −1∑
t=o
2∑
y=1
P(Sy; Ct ; z)
∗ log P(Sy; Ct/S, z)
P(Sy/S, z) ∗ P(Ct/S, z) (3)
where,
P(Sy/S, z): an estimated conditional probability of a sub-
interval Sy , given the partitioned interval S and the node z.
It is calculated as the proportion of instances belonging to
the partitioned interval S at the node z, where the value of
the candidate input attribute belongs to the subinterval Sy .
The number of subintervals in each partitioned interval is
two.
P(Ct/S, z): an estimated conditional probability of a value
(“class”) t of the target attribute T given the interval S and
the node z; it is calculated as the proportion of instances
belonging to the partitioned interval S at the node z, where
the value of the target attribute T is t .
P(Sy; Ct/S, z): an estimated joint probability of a value of
the target attribute T and a subinterval Sy given the interval
S and the node z; it is calculated as the proportion of ins-
tances belonging to the partitioned interval S at the node z,
where the value of the target attribute T is t and the value of
the candidate input attribute belongs to the subinterval Sy .
P(Sy; Ct ; z): an estimated joint probability of a value Ct
of the target attribute T , a subinterval Sy , and the node z.
It is calculated as the proportion of all training instances,
where the value of the target attribute T is t , the value of
the candidate input attribute belongs to the subinterval Sy ,
and the node is z.
The main steps of the recursive discretization procedure,
initially introduced in (Last and Maimon 2004), are shown
in the Appendix.
The statistical significance of the estimated conditional
mutual information between a candidate input attribute Ai
and the target attribute T given a node z is evaluated by
using the following likelihood-ratio statistic:
G2(T ; Ai/z) = 2 ∗ (ln 2) ∗ E∗ ∗ MI(T ; Ai/z) (4)
where E∗ is the total number of training cases in the data-
set. The null hypothesis is that the actual conditional mutual
information is zero and that hypothesis is rejected if the G2
statistic is significant at the pre-specified confidence level.
Based on the empirical results with real-world datasets
(Maimon and Last 2000), the default confidence level, lea-
ding to the most compact and accurate models, is set to
99.9%, but it can be reduced if larger models involving more
predictive features are needed.
The Multi-target Information Network (M-IN)
Product quality is usually measured by multiple dimensions,
such as presence/absence of various defects. In order to pro-
vide a unified framework for single-target and multi-target
classification tasks, Last (2004) has defined an extended clas-
sification task using the following notation:
• R = (A1, . . . , Ak)—a set of k attributes in the dataset
(k ≥ 2).
• C—a non-empty subset of n candidate input features
(C ⊂ R, |C | = n ≥ 1). The values of these features are
usually known and they can be used to predict the values
of target attributes (see next). Some of candidate input
features may represent controllable parameters, which
can affect the quality dimensions of the manufactured
product.
• O—a non-empty subset of m target (“output”) attributes
(O ⊂ R, |O| = m ≥ 1). This is a subset of attributes
representing the variables to predict, such as product qua-
lity dimensions. The extended classification task is to
build an accurate model (or models) for predicting the
values of all target attributes, based on the corresponding
dependency subset (or subsets) I ⊆ C of input features.
As shown in (Last 2004), an m-target classification func-
tion can be represented by a multi-target information net-
work (M-IN), where the nodes of the target layer represent
the values of all output attributes. Like a single-target IN,
a multi-target information network has a single root node
and its internal “read-once” structure is identical for all tar-
get variables. This means that every hidden node is shared
among all outputs and each terminal (leaf) node is connected
to at least one target node associated with every output.
At every iteration, the M-IN construction algorithm
chooses an input (predictive) feature, which maximizes the
total conditional mutual information between an input feature
123
J Intell Manuf
and the subset of output attributes. For each candidate input
(predictive) attribute Ai in a layer n, the algorithm calculates
the conditional mutual information of Ai and the target (clas-
sification) attributes T1, . . . , Tm given n − 1 input attributes
X1, . . . , Xn−1 by the following formula:
MI(T1, . . . , Tm; Ai/X1, . . . , Xn−1)
=
m∑
j=1
MI(Tj ; Ai/X1, . . . , Xn−1) (5)
The remaining calculations are identical to the IN construc-
tion algorithm described in the previous sub-section.
According to (Last 2004), the M-IN model has the follo-
wing information-theoretic properties:
• The average conditional entropy of m target attributes in
an n-input m-target model M is not greater than the ave-
rage conditional entropy over m single-objective models
Si (i = 1, . . . , m) based on the same n input features.
This inequality is strengthened if the multi-objective
model M is trained on more features than the single-
objective models. Consequently, we may expect that the
average accuracy of a multi-objective model in predic-
ting the values of m target attributes will not be worse, or
even will be better, than the average accuracy of m single-
objective models using the same set of input features.
• If all target attributes are either mutually independent or
completely dependent on each other, the input feature
selected by the M-IN construction algorithm will mini-
mize the joint conditional entropy of all target attributes.
This implies that in both these extreme cases, the M-IN
algorithm is expected to produce the optimal (most accu-
rate) model.
Optimization with IN and M-IN
In a classification setting, the oblivious decision trees produ-
ced by the IN or the M-IN algorithms can be used to predict
the values of the target attributes using the majority voting at
the terminal node associated with each unlabeled record. The
predictive performance of the IN and the M-IN algorithms
has been extensively evaluated in Last and Maimon (2004)
and Last (2004), respectively. According to the empirical
results, the Information Network induction algorithms tend
to produce much more compact models than C4.5, while pre-
serving nearly the same level of classification accuracy. This
result supports the theorem proven in Bryant (1986) that each
Boolean function has a unique function graph representation
having a minimal number of vertices.
However, even when the predictive capability of the Infor-
mation Network is limited due to the high noisiness of real-
world data, the model structure can still provide us with some
useful information about the explored phenomenon. Thus,
the user may be interested in the estimated distribution of
the target attribute across different leaf nodes of the Informa-
tion Network. For example, in case of a batch manufacturing
process, a small increase or decrease in the probability of the
“success” class as a result of certain changes in the process
settings may lead over time to significant savings in manu-
facturing costs. The single-target network structure induced
for a quality dimension Tj can be used to find the optimal
process settings z∗ corresponding to a leaf node z with the
highest probability of a successful outcome:
z∗ = arg max
z
Pr(Tj = ‘success’/z) (6)
In case of m independent quality dimensions represented by
a multi-target Information Network, we may be interested to
find the settings that minimize the probability of any failure
using the following equation:
z∗ = arg max
z
m

j=1
Pr(Tj = ‘success’/z) (7)
Definitely, the above equations aimed at maximizing the suc-
cess probability can be enhanced with cost considerations as
well.
The induction of Information Network models for the
crystal manufacturing data will be covered by the next sec-
tion (“mining crystal quality data”), whereas the search for
the optimal process parameters using Eqs. 6 and 7 above will
be presented in the section “optimizing the crystal quality
with IN models.”
Mining crystal quality data
Data description
For this study, we have obtained a database that included
1,289 records of the growth processes performed between
July 1995 and November 2005. Each process is described by
close to 50 variables. About one-third of those variables are
represented by text fields containing code words, special cha-
racters (such as ‘**’), and free text, whereas the remaining
variables are stored in a numeric format. Using information
provided by the process engineers, all textual fields and some
numeric fields have been converted into pre-specified nume-
ric codes. Inconsistent and abnormal attribute values were
manually corrected by the process engineers using informa-
tion from other fields. All missing values were assigned a
special missing value code (999999) so that the Information
Network algorithm can handle missing values appropriately.
After marking several variables as irrelevant to the process
outcome, we were left with 45 candidate input (predictive)
attributes. These attributes included six controllable variables
(process settings) and 39 uncontrollable (random) variables.
Following additional data cleaning, we have removed four
123
J Intell Manuf
Target Attributes
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Jan-98 Jan-99 Jan-00 Jan-01 Jan-02 Jan-03 Jan-04 Jan-05 Jan-06
504_mov_ave 501-503_mov_ave
Fig. 1 Temporal behavior of Outcome_501–503 (501–503_mov_ave)
and Outcome_504 (504_mov_ave)
records leaving us with 1,285 records of manufactured crys-
tals only.
Our study was aimed at explaining the two most com-
mon reasons of process failure denoted by the failure codes
“501–503” and “504”. Accordingly, we have defined the tar-
get variables “Outcome_501–503” and “Outcome_504” as
follows:
1) Outcome_501–503 = 0 (failure) if the failure code is 501,
502 or 503; otherwise Outcome_501–503 = 1 (success).
This defect was found in 10.1% of crystals (130 out of
1,285).
2) Outcome_504 = 0 (failure) if the failure code is 504;
otherwise Outcome_504 = 1 (success). This defect was
found in 4.7% of crystals (60 out of 1,285).
The temporal behavior of both target attributes is shown in
Fig. 1, which presents the moving averages of Outcome_501–
503 (501–503_mov_ave) and Outcome_504 (504_mov_ave)
over a sliding window of 100 observations. In other words,
each point on the chart represents the portion of successful
outcomes out of the 100 most recent batches. The process
engineers were particularly interested to explain the sharp
decrease in the process quality, in terms of both target attri-
butes, which took place in the course of 2004. Equally impor-
tant for them was to understand the reasons for a gradual
improvement of Outcome_501–503, which continued since
the end of 2004, along with the apparent instability of Out-
come_504 during the same period. The most recent improve-
ment in Outcome_504, which started around August 2005,
also required an explanation.
Induction of predictive models
First, we have attempted to find models explaining the varia-
bility in both target attributes by running the Information
Network algorithm on all 45 predictive attributes. However,
the selected input features and the induced rules were judged
as useless by the process engineers, since they mainly indica-
ted the well-known relationship between the production date
and the process failure rate without providing any clues to
improving the settings of the manufacturing process. Conse-
quently, we have decided to focus on the six controllable
variables only. We have also learned that these six parame-
ters can in no way be treated as “random variables”, since they
are changed by process engineers quite infrequently (usually
not more than once in a year). Thus, we needed to find out,
which parameters were less stable (more time-dependent)
during the period of study.
The effect of time on each controllable variable (denoted
in this paper by the letters S, E, O, M, N, and C) was evalua-
ted using the single-target Information Network algorithm
by defining each controllable variable as a target attribute
and the FDATE (completion date) variable as the only input
attribute. The default confidence level of the IN algorithm
(99.9%) remained unchanged. The ranges of continuous tar-
get attributes (S, E, O, and M) were discretized into three
intervals of equal frequency. The results of the correspon-
ding six runs of the IN algorithm are presented in Table 1.
Each table row shows the name of a controllable variable, the
number of its discretization intervals or nominal values, its
unconditional entropy, the mutual information between the
FDATE variable and the controllable variable, and the ratio
of the mutual information to the total entropy of the control-
lable variable. A higher ratio indicates a stronger effect of
time on the controllable variable, since it means that the
time can explain a higher portion of the variable entropy
(uncertainty). Though the effect of time on all six variables
Table 1 The effect of time on
controllable Variables Target attribute Number of intervals/values Total entropy Mutual Information (MI) MI/entropy %
S 3 0.979 0.905 92.44
C 2 0.972 0.850 87.45
E 3 1.554 0.482 31.02
O 3 1.557 0.139 8.73
M 3 1.200 0.087 7.25
N 4 1.792 0.085 4.74
123
J Intell Manuf
S Histogram
2 24 35
506
21
569
131
1
0
100
200
300
400
500
600
85 86 87 88 89 90 91 92 93 94 95 96 97
Bin
F
re
q
u
en
cy
Fig. 2 Distribution of the S variable
C Histogram
502
762
0
200
400
600
800
1000
10
Bin
F
re
q
u
en
cy
Fig. 3 Distribution of the C variable
was found statistically significant by the algorithm, the first
two variables (S and C) were apparently subject to much
more frequent changes than the other four variables, causing
a considerable decrease in their entropy (nearly 90%) given
the FDATE attribute.
The probability distributions of S and C are shown in
Figs. 2 and 3, respectively. C is a discrete binary variable,
which can be set to either 0 or 1, whereas S is a continuous
variable taking integer values between 85 and 96. The two
most common values of S were 91 and 95 though sometimes
it was set to seven other values in its range such as 96. C
was assigned the level of zero in nearly 40% of all processes
(502 records) and the level of one in the remaining 60% (762
records). The reliability of all recorded values was verified
by the process experts.
The moving averages of C and S as a function of time are
shown in Fig. 4. On the S curve, one can identify a continuous
increase until May 2002, stability from May 2002 to July
2003, and then a decrease back to the initial level except for
a brief increase after May 2004. On the other hand, C, which
is a discrete binary variable, was set to 1 between July 2002
and February 2003, kept at the level of 1 until July 2005, and
then reset back to 0.
To find the exact effect of both controllable variables
(S and C) on each failure type, we have run the IN algo-
rithm separately for every target attribute defined above. In
the case of Outcome_501–503, the default confidence level
of 99.9% has produced a model containing only one input
attribute (S). The level reduction to 99.0%, has added the
second input attribute (C) to the model. The detailed results
of each iteration of the IN algorithm are shown in Table 2. The
mutual information (MI) between the first input variable (S)
and the target Outcome_501–503 is 0.014 bits leaving the
conditional entropy of the target at 0.459 bits. The second
input variable (C) is reducing the conditional entropy of the
target by additional 0.004 bits (see the “Conditional MI”
column). The total mutual information of this model is
very small (0.018 bits) compared to the total entropy of the
target attribute (0.473 bits). This implies that S and C can
only explain a small portion of the Outcome_501–503
uncertainty.
The structure of the induced Information Network is
shown in Fig. 5. The two network hidden layers represent
the first input attribute (S) and the second input attribute (C),
respectively. Connections representing input attribute values,
such as “S < 94”, are denoted by dotted lines, whereas
connections between terminal nodes (1, 3, 4, and 5) and the
nodes of the target layer (Failure/Success) are shown as solid
lines. Each terminal-target connection is accompanied by the
probability of the corresponding outcome (Failure/Success).
Thicker lines represent the most likely (majority) outcome
at each terminal node. On the other hand, the prediction
model induced for the second target attribute (Outcome_504)
contains both input attributes for the default confidence level
of 99.9%. The detailed results of each iteration of the IN algo-
rithm are shown in Table 3. The mutual information (MI) bet-
ween the first input variable (S) and the target Outcome_504
is 0.018 bits leaving the conditional entropy of the target
at 0.254 bits. The second input variable (C) is reducing the
conditional entropy of the target by additional 0.011 bits (see
the “Conditional MI” column). The total mutual information
(uncertainty reduction) of the obtained model is 0.029 bits,
which is still small compared to the unconditional entropy
of the target (0.272), but much higher than the MI of the
Outcome_501–503 model (0.018 bits).
The structure of the induced Information Network is
shown in Fig. 6. The two network hidden layers represent
the first input attribute (S) and the second input attribute
(C), respectively. Like in Fig. 5, connections representing
input attribute values, such as “S < 94”, are denoted by
dotted lines, whereas connections between terminal nodes
(2, 3, and 4) and the nodes of the target layer (Failure/Success)
are shown as solid lines. Each terminal-target connection is
accompanied by the probability of the corresponding out-
come (Failure/Success) with thicker lines representing the
most likely (majority) outcome at each terminal node.
The optimal process conditions based on the induced
models are explored in the next section.
123
J Intell Manuf
Fig. 4 Moving averages of S
and C as a function of time
Feb-03 Jul-05
May-02 Jul-03
May-04
0
0.2
0.4
0.6
0.8
1
1.2
Jul-95 Jun-01 Jul-02 Jan-03 Oct-03 Sep-04 May-05 Oct-05
C
87
88
89
90
91
92
93
94
95
96
S
C S
Table 2 IN run summary for target = Outcome_501–503
Iteration Attribute Mutual Conditional Conditional Split
name Information MI entropy nodes
(MI)
0 S 0.014 0.014 0.459 1
1 C 0.018 0.004 0.455 1
0
2
4
3
0
1
Target layer
(Process Outcome)
S < 94
S ≥ 96
Layer 1
(S )
Layer 2
(C)Root 
node
Terminal 
node
0.851
0.079
Failure
Success
C = 0
C = 1
0.149
0.212
0.788
1
5
94≤S < 96
0.065
0.935
0.921
Hidden node
Fig. 5 Information Network for target = Outcome_501–503
Table 3 IN run summary for target = Outcome_504
Iteration Attribute Mutual Conditional Conditional Split
name Information MI entropy nodes
(MI)
0 S 0.018 0.018 0.254 1
1 C 0.029 0.011 0.243 1
0
1
3
2
0
1
Target layer
(Process Outcome)
S < 94
S ≥ 94
Layer 1
(S )
Layer 2
(C)Root 
node
Hidden node
Terminal 
node
0.975
0.123
Failure
Success
4
C = 0
C = 1
0.025
0.017
0.983
0.877
Fig. 6 Information Network for target = Outcome_504
Optimizing the crystal quality with IN models
Table 4 shows the estimated distribution of the target attri-
bute Outcome_501–503 across different leaf nodes of the
Information Network presented in Fig. 5 above. By applying
Eq. 6 to the data in Table 4, we can conclude that the lowest
failure rate of 6.5% for failures 501–503 is achieved if the S
parameter is kept below 94, disregarding the value of C. On
the other hand, raising S to the value of 96 and higher should
triple the failure rate to the level of 21.2%.
Based on the probability estimation rules in Table 4 and
moving averages of S and C in Fig. 4, we can finally explain
the variability in the 501–503 failure rates between the years
1995-2005 as observed in Fig. 1 above:
• The average value of S was gradually increased from 90 to
95 between July 1998 and May 2002, whereas C was kept
at the level of zero. Based on Rules 0 and 2, this caused an
increase in the failure rate from 6.5% to 14.9%.
123
J Intell Manuf
Table 4 Estimated distribution
of the target attribute
Outcome_501–503
Probability Rule condition Prob. (501–503 = 0) Prob. (501–503 = 1)
estimation rule No.
0 If S is between 0 and 94 0.065 0.935
1 If S is 96 and higher 0.212 0.788
2 If S is between 94 and 96 and C is 0 0.149 0.851
3 If S is between 94 and 96 and C is 1 0.079 0.921
Table 5 Estimated distribution
of the target attribute
Outcome_504
Probability Rule condition Prob. (Outcome = 0)% Prob. (Outcome = 1)%
estimation rule No.
1 S is 94 and higher 1.7 98.3
2 S is between 0 and 94 and C is 0 2.5 97.5
3 S is between 0 and 94 and C is 1 12.3 87.7
• The average value of S was kept close to 95 until July
2003, but between July 2002 and February 2003 C was
raised to the level of one. Based on Rule 3, this change
reduced the failure rate to 7.9%.
• Between January 2004 and March 2005 some crystals
were grown with S = 96 resulting in an increase of S ave-
rage value. Based on Rule 1, this change caused a drastic
drop in the crystals quality represented by a failure rate
of 21.2%.
• Starting from May 2005 nearly all crystals were grown
with S = 91. Based on Rule 0, this change restored the
former minimal failure rate of 6.5%.
Table 5 shows the estimated distribution of the target attri-
bute Outcome_504 across different leaf nodes of the Infor-
mation Network presented in Fig. 6 above. By applying Eq. 6
to the data in Table 5, we can conclude that the lowest failure
rate of 1.7% for failure 504 is achieved if the S parameter is
kept at the level of 94 and higher, disregarding the value of
C. On the other hand, decreasing S below the value of 94 and
keeping C at the level of one should increase the failure rate
to the level of 12.3%, which is more than seven times higher
(!) than the lowest achievable failure rate.
Based on the probability estimation rules in Table 5 and
moving averages of S and C in Fig. 4, we can also explain the
variability in the 504 failure rates between the years 1995–
2005 as observed in Fig. 1 above:
• Before May 2002 the average value of S was low (below
94), whereas C was kept at the level of zero. Based on
Rule 2, these conditions resulted in a relatively low failure
rate of 2.5%.
• By mid-2002 the average value of S was raised above 94.
Based on Rule 1, this did not harm the average quality
and even slightly improved it by reducing the failure rate
from 2.5% to 1.7%.
• At the end of 2002—beginning of 2003, the C level was
raised to one without changing the value of S. Based
on Rule 1, this change did not affect the average crystal
quality.
• In the second half of 2003—beginning of 2004, S was
restored to its former level (below 94) without changing
the C level of one. Based on Rule 3, this change caused a
dramatic increase in the failure rate from 1.7% to 12.3%.
• In July 2005, C was reset to the level of zero, which
was maintained before mid-2002. Based on Rule 2, this
change returned the failure rate to the low level of 2.5%,
which was experienced before 2002.
For natural reasons, the process engineers are interested in
reducing all kinds of defects including failures 501–503 and
504. However, there is a clear conflict between the recom-
mendations of the two models induced above. The condition
that minimizes the rate of 504 failures (“S of 94 and higher”)
contradicts the recommendation of the Outcome_501–503
model (“S below 94”). To find the optimal process settings
that take into account both failure types, we have run the
multi-target Information Network (M-IN) algorithm to esti-
mate simultaneously the distributions of two target attributes:
Outcome_501–503 and Outcome_504. The default confi-
dence level (99.9%) remained unchanged. The run summary
and the induced probability estimation rules are shown in
Tables 6 and 7, respectively.
The last column of Table 7 calculates the probability of any
of the two failure types based on Eq. 7, which assumes that
these are two independent events. The independency assump-
tion was confirmed by the process engineers with respect to
failures 501–503 vs. 504. Based on Table 7, Rule 1 provides
the optimal process settings, which minimize the probability
of both failure types:
S is between 0 and 94 and C is 0
123
J Intell Manuf
Table 6 M-IN run summary for
targets = Outcome_501–503 and
Outcome_504
Iteration Attribute Total Mutual Total Total conditional Split
Name Information (MI) conditional MI entropy nodes
0 S 0.027 0.027 0.718 1
1 C 0.038 0.011 0.707 1
Table 7 Estimated distributions of the target attributes Outcome_501–503 and Outcome_504
Probability
estimation
rule No.
Rule condition Prob. (504 = 0) Prob. (504 = 1) Prob. (501–503 = 0) Prob. (501–503 = 1) Prob. (501–504)
0 If S is 94 and higher 0.017 0.983 0.131 0.869 0.146
1 If S is between 0 and 94
and C is 0
0.025 0.975 0.08 0.92 0.103
2 If S is between 0 and 94
and C is 1
0.123 0.877 0.056 0.944 0.172
Under the above conditions, the expected failure rate is 10.3%
only.
We have also explored the potential benefit of this data
mining analysis if it were applied to the collected data at
an earlier date. It turned out that very similar recommenda-
tions could be extracted from a multi-target IN model using
the 99.0% confidence level and 759 records collected until
December 2003. The discovery of the optimal process set-
tings in December 2003 would probably keep the overall
failure rate between January 2004 and December 2005 at the
low level of 10.3% instead of the actual failure rate of 18.8%
during the same period.
Conclusions
In this study, we have demonstrated that probability esti-
mation models induced by Information Network algorithms
can be successfully utilized to determine the optimal process
settings of a complex manufacturing process such as crys-
tal growth. For an effective implementation of the proposed
methodology, it is very important to identify the controllable
parameters that are frequently changed by process engineers.
Several independent quality dimensions can be taken into
account using multi-target Information Network models. We
have also shown that a smaller dataset collected at an earlier
date could improve the past outgoing quality in terms of the
failure rate.
We believe that process optimization with probability esti-
mation models can be further explored in several directions.
These include application of alternative data mining algo-
rithms, estimation of minimal required sample sizes, online
change detection, cost-sensitive optimization, and many
others. It is also important to keep in mind that Informa-
tion Networks, due to their “oblivious” nature, are restricted
by a constant order of attribute tests along each path. Conse-
quently, we may lose some potentially more accurate rules,
where the set and the order of tested attributes are different.
The effect of this limitation on the Information Network pre-
dictive accuracy was studied extensively in (Last and Mai-
mon, 2004).
Acknowledgements This work was partially supported by a research
grant from Israel Ministry of Defense.
Appendix: discretization of continuous attributes with IN
algorithm
Partition (Data Table r, Information Network, Attribute Ai ,
Interval S, Significance level sign)
Input: the set of n training instances, an information-theoretic network,
a continuous attribute Ai to be discretized, the interval S to be parti-
tioned (the first and the last distinct values of Ai ), and the minimum
significance level sign for splitting an interval (default: sign = 0.1%).
Output: the list of threshold values for Ai
Step 1— For every distinct value Th included in the interval S (except
for the last distinct value) Do:
Step 1.1— For every node z of the final hidden layer Do:
Step 1.1.1— Calculate the likelihood-ratio
test for the partition of the inter-
val S at the threshold Th and the
target attribute T given the node
z. All values below or equal to
Th belong to the first sub-
interval S1. Distinct values
above Th belong to the second
sub-interval S2.
Step 1.1.2— If the likelihood-ratio statistic
is significant, mark the node as
“split” by the threshold Th
Step 1.1.3— End Do
Step 1.2 — End Do
123
J Intell Manuf
Step 3— Find the threshold T hmax maximizing the conditional
mutual information over all nodes
Step 4— If the maximum estimated conditional mutual information
is greater than zero, then Do:
Step 4.1— For every node z of the final hidden layer Do:
Step 4.1.1— If the node z is split by the thre-
shold T hmax, mark the node
as split by the candidate input
attribute Ai
Step 4.2— If the threshold T hmax is the first distinct value
in the interval S, mark T hmax as the lower
bound of a new discretization interval, else
Partition (Data Table r, Network, Attribute Ai ,
Interval S1, sign).
Step 4.3— Partition (Data Table r, Network, Attribute Ai ,
Interval S2, sign)
Step 4.4— End Do
Step 5— Else return the list of threshold values for Ai
More details are provided in (Last and Maimon 2004).
References
Babu, J., & Frieda, W. H. (1993). Predictive control of quality in a
batch manufacturing process using artificial neural network models.
Industrial & Engineering Chemistry Research, 32(9), 1951–1961.
doi:10.1021/ie00021a019.
Biderman, S., Horowitz, A., Einav, Y., Ben Amar, G., Gazit, D., Stern,
A., et al. (1991). Production of sapphire domes by the growth of
near-net-shape single crystals. In Proceedings of SPIE 1535 (Passive
Materials for Optical Elements) (pp. 27–34).
Braha, D., Elovici, Y., & Last, M. (2007). Theory of actionable data
mining with application to semiconductor manufacturing control.
International Journal of Production Research, 45(13), 3059–3084.
doi:10.1080/00207540600654475.
Bryant, R. E. (1986). Graph-based algorithms for Boolean function
manipulation. IEEE Transactions on Computers, C-35-8, 677–691.
Cook, D. F., Ragsdale, C. T., & Major, R. L. (2000). Combining a neu-
ral network with a genetic algorithm for process parameter optimi-
zation. Engineering Applications of Artificial Intelligence, 13, 391–
396. doi:10.1016/S0952-1976(00)00021-X.
Famili, A. (1994). Use of decision-tree induction for process optimi-
zation and knowledge refinement of an industrial process. Artificial
Intelligence for Engineering Design, Analysis and Manufacturing.
(AI EDAM), 8(1), 63–75.
Han, J., & Kamber, M. (2006). Data mining: Concepts and techniques
(2nd ed.). San Francisco, CA: Morgan Kaufmann.
Horowitz, A., Biderman, S., Ben Amar, G., Laor, U., Weiss, M., &
Stern, A. (1987). The growth of single crystals of optical materials
via the gradient solidification method. Journal of Crystal Growth,
85(1–2), 215–222. doi:10.1016/0022-0248(87)90225-9.
Horowitz, A., Biderman, S., Gazit, D., Einav, Y., Ben Amar, G., & Weiss,
M. (1993). The growth of dome shaped sapphire crystals by the GSM
method. Journal of Crystal Growth, 128(1–4 pt 2), 824–828.
Hur, J., Lee, H., & Baek, J.-G. (2006). An intelligent manufacturing
process diagnosis system using hybrid data mining. In Advances in
Data Mining (pp. 561–575). Springer-Verlag.
Kohavi, R., & Li, C.-H. (1995). Oblivious decision trees, raphs, and
top-down pruning. In Proceedings of International Joint Conference
on Artificial Intelligence (IJCAI) (pp. 1071–1077).
Last, M. (2004). Multi-objective classification with info-fuzzy net-
works. In Proceedings of the 15th European Conference on Machine
Learning (ECML 2004). Lecture Notes in Artificial Intelligence 3201
(pp. 239–249). Springer-Verlag.
Last, M., & Kandel, A. (2001). Data mining for process and quality
control in the semiconductor industry. In D. Braha (Ed.), Data mining
for design and manufacturing: Methods and applications. Kluwer
Massive Computing Series (Vol. 524, pp. 207–234). Norwell, MA:
Kluwer Academic Publishers.
Last, M., & Maimon, O. (2004). A compact and accurate model for clas-
sification. IEEE Transactions on Knowledge and Data Engineering,
16(2), 203–215. doi:10.1109/TKDE.2004.1269598.
Liau, L. C.-K., Yang, T. C.-K., & Tsai, M.-T. (2004). Expert system
of a crude oil distillation unit for process optimization using neural
networks. Expert Systems with Applications, 26(2), 247–255. doi:10.
1016/S0957-4174(03)00139-8.
Lin, J. L., & Lin, C. L. (2005). The use of grey-fuzzy logic for the optimi-
zation of the manufacturing process. Journal of Materials Processing
Technology, 160(1), 9–14. doi:10.1016/j.jmatprotec.2003.11.040.
Maimon, O., & Last, M. (2000). Knowledge discovery and data mining
—the Info-Fuzzy Network (IFN) methodology. Boston: Kluwer Aca-
demic Publishers, Massive Computing.
Myers, R. H., & Montgomery, D. (2002). Response surface methodo-
logy: Process and product optimization using designed experiments
(2nd ed.). John Wiley and Sons.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning,
1(1), 81–106.
Quinlan, J. R. (1993) C4.5: Programs for machine learning. Morgan
Kaufmann.
123
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תכנה-וחומר-עזר\בדיקת-השערות\ch5.pdf </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Evaluating Hypotheses
Read Ch 
Recommended exercises   	
 Sample error true error
 Con
dence intervals for observed hypothesis
error
 Estimators
 Binomial distribution Normal distribution
Central Limit Theorem
 Paired t tests
 Comparing learning methods
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Two Denitions of Error
The true error of hypothesis h with respect to
target function f and distribution D is the
probability that h will misclassify an instance
drawn at random according to D
errorDh  Pr
xD
fx  hx
The sample error of h with respect to target
function f and data sample S is the proportion of
examples h misclassi
es
errorSh  
n
X
xS
fx  hx
Where fx  hx is  if fx  hx and 
otherwise
How well does errorSh estimate errorDh
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Problems Estimating Error
 Bias If S is training set errorSh is
optimistically biased
bias  EerrorSh errorDh
For unbiased estimate h and S must be chosen
independently
 Variance Even with unbiased S errorSh may
still vary from errorDh
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Example
Hypothesis h misclassi
es  of the 	 examples in
S
errorSh 

	
 
What is errorDh
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Estimators
Experiment
 choose sample S of size n according to
distribution D
 measure errorSh
errorSh is a random variable ie result of an
experiment
errorSh is an unbiased estimator for errorDh
Given observed errorSh what can we conclude
about errorDh
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Condence Intervals
If
 S contains n examples drawn independently of
h and each other
 n  
Then
With approximately  probability errorDh
lies in interval
errorSh 
vuuuuterrorSh errorSh
n
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Condence Intervals
If
 S contains n examples drawn independently of
h and each other
 n  
Then
With approximately N probability errorDh
lies in interval
errorSh zN
vuuuuterrorSh errorSh
n
where
N       
zN     	   
	
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
errorSh is a Random Variable
Rerun the experiment with dierent randomly
drawn S of size n
Probability of observing r misclassi
ed examples
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0 5 10 15 20 25 30 35 40
P
(r
)
Binomial distribution for n = 40, p = 0.3
P r 
n
rn r errorDh
r errorDhnr
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Binomial Probability Distribution
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0 5 10 15 20 25 30 35 40
P
(r
)
Binomial distribution for n = 40, p = 0.3
P r 
n
rn r p
r pnr
Probability P r of r heads in n coin ips if
p  Prheads
 Expected or mean value of X EX is
EX  nX
i
iP i  np
 Variance of X is
V arX  EX EX  np p
 Standard deviation of X X is
X 
r
EX EX 
r
np p
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Normal Distribution Approximates Bino
mial
errorSh follows a Binomial distribution with
mean errorSh  errorDh
 standard deviation errorSh
errorSh 
vuuuuterrorDh errorDh
n
Approximate this by a Normal distribution with
mean errorSh  errorDh
 standard deviation errorSh
errorSh 
vuuuuterrorSh errorSh
n
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Normal Probability Distribution
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
-3 -2 -1 0 1 2 3
Normal distribution with mean 0, standard deviation 1
px 
p

e


x 

The probability that X will fall into the interval
a b is given by Z b
a
pxdx
 Expected or mean value of X EX is
EX  
 Variance of X is
V arX  
 Standard deviation of X X is
X  
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Normal Probability Distribution
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
-3 -2 -1 0 1 2 3
 of area probability lies in  
N of area probability lies in  zN
N       
zN     	   
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Condence Intervals More Correctly
If
 S contains n examples drawn independently of
h and each other
 n  
Then
With approximately  probability errorSh
lies in interval
errorDh 
vuuuuterrorDh errorDh
n
equivalently errorDh lies in interval
errorSh 
vuuuuterrorDh errorDh
n
which is approximately
errorSh 
vuuuuterrorSh errorSh
n
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Central Limit Theorem
Consider a set of independent identically
distributed random variables Y    Yn all governed
by an arbitrary probability distribution with mean
 and 
nite variance  De
ne the sample mean
Y  
n
nX
i
Yi
Central Limit Theorem As n	 the
distribution governing Y approaches a Normal
distribution with mean  and variance 

n

	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Calculating Condence Intervals
 Pick parameter p to estimate
 errorDh
 Choose an estimator
 errorSh
 Determine probability distribution that governs
estimator
 errorSh governed by Binomial distribution
approximated by Normal when n  
	 Find interval LU such that N of probability
mass falls in the interval
 Use table of zN values
		 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Dierence Between Hypotheses
Test h on sample S test h on S
 Pick parameter to estimate
d  errorDh errorDh
 Choose an estimator
d  errorSh errorSh
 Determine probability distribution that governs
estimator
d

s
errorSh errorSh
n

errorSh errorSh
n
	 Find interval LU such that N of probability
mass falls in the interval
dzN
vuuuuut
errorSh errorSh
n

errorSh errorS
n
	 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Paired t test to compare hAhB
 Partition data into k disjoint test sets
T T     Tk of equal size where this size is at
least 
 For i from  to k do
i 
 errorTihA errorTihB
 Return the value  where
  
k
kX
i
i
N con
dence interval estimate for d
  tNk s
s 
vuuuuut

kk  
kX
i
i  
Note i approximately Normally distributed

 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Comparing learning algorithms LA and LB
What wed like to estimate
ESDerrorDLAS errorDLBS
where LS is the hypothesis output by learner L
using training set S
ie the expected dierence in true error between
hypotheses output by learners LA and LB when
trained using randomly selected training sets S
drawn according to distribution D
But given limited data D what is a good
estimator
 could partition D into training set S and
training set T and measure
errorTLAS errorTLBS
 even better repeat this many times and average
the results next slide
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Comparing learning algorithms LA and LB
 Partition data D into k disjoint test sets
T T     Tk of equal size where this size is at
least 
 For i from  to k do
use Ti for the test set and the remaining data
for training set Si
 Si
 fD  Tig
 hA 
 LASi
 hB 
 LBSi
 i 
 errorTihA errorTihB
 Return the value  where
  
k
kX
i
i
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
Comparing learning algorithms LA and LB
Notice wed like to use the paired t test on  to
obtain a con
dence interval
but not really correct because the training sets in
this algorithm are not independent they overlap
more correct to view algorithm as producing an
estimate of
ESDerrorDLAS errorDLBS
instead of
ESDerrorDLAS errorDLBS
but even this approximation is better than no
comparison
 lecture slides for textbook Machine Learning T Mitchell McGraw Hill 
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\1---Preprocessing\1---Preprocessing.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
Data Science and Business Intelligence
 
#1 - Data Pre-Processing

nivah@post.bgu.ac.il


Data preprocessing steps:
March 5, 2019
2
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Prof. Mark Last
2


Data transformation
Transform data into forms appropriate for mining.
March 5, 2019
3
Smoothing
Aggregation
Generalization
Normalization
Feature construction



3
Normalization: scaled to fall within a small, specified range.
May improve the accuracy and efficiency of mining algorithms involving distance measurements.
Min-max normalization
Z-score normalization
Normalization by decimal scaling

March 5, 2019
4
Data Transformation: Normalization


4
Min-max normalization

Z-score normalization
Normalization by decimal scaling


March 5, 2019
5



Where j is the smallest integer such that Max(|     |)<1

Data Transformation: Normalization
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
5

A – attribute 
תכונה, פיצ'ר
Example – min-max normalization
min-max	X	id
7.7	23	1
10	30	2
5	15	3
5.7	17	4
8.3	25	5
3	9	6
1	3	7
6	18	8
7	21	9
March 5, 2019
6



Min = 3 
Max = 30 
New_min = 1 
New_max = 10

 (23-3)/(30-3) (10-1) + 1 = 7.7
 (30-3)/(30-3) * 9 + 1 =  10
 (15-3)/(30-3) * 9 + 1 =  5
 (17-3)/(30-3) * 9 + 1 =  5.7
 (25-3)/(30-3) * 9 + 1 =  8.3
 (9-3)/(30-3) * 9 + 1 =  3
 (3-3)/(30-3) * 9 + 1 = 1
 (18-3)/(30-3) * 9 + 1 =  6
 (21-3)/(30-3) * 9 + 1 =  7 






6
Example - z-score normalization
z-score	X	id
0.6	23	1
1.5	30	2
-0.4	15	3
-0.1	17	4
0.9	25	5
-1.1	9	6
-1.8	3	7
0.0	18	8
0.4	21	9
March 5, 2019
7
Mean = 17.9 
Stdev = 8.2

 (23-17.9) / 8.2 = 0.6
 (30- 17.9) / 8.2 = 1.5
 (15- 17.9) / 8.2 
 (17- 17.9) / 8.2
 (25- 17.9) / 8.2
 (9- 17.9) / 8.2
 (3- 17.9) / 8.2 
 (18- 17.9) / 8.2
 (21- 17.9) / 8.2 

  




7
Example - normalization by decimal scaling
 decimal scaling	X	id
0.23	23,000	1
0.30	30,000	2
0.15	15,000	3
0.17	17,000	4
0.25	25,000	5
0.09	9,000	6
0.03	3,000	7
0.18	18,000	8
0.21	21,000	9
March 5, 2019
8

j is the smallest integer 
such that Max(|     |)<1 
j = 5

 23,000/100,000 = 0.23
 30,000/100,000=  0.3
 15,000/100,000  = 0.15 
 17,000/100,000  = 0.17
 25,000/100,000  = 0.25
 9,000/100,000  = 0.09
 0.03
 0.18
 0.21 





8
Moving Average Methods
Goal
Determining the trend of time series data
Most common methods
Simple Moving Average
Weighted Moving Average
Exponential Moving Average
March 5, 2019
9
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
9


Simple Moving Average
The forecast is simply the average of the most recent k observations:

March 5, 2019
10
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
10


Selecting k
Smoothing effect (large k)
Responsiveness (small k)
Useful to compare results with different k values
March 5, 2019
11
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
11


Example - SMA
March 5, 2019
12
Position	Value	SMA2	SMA3	SMA4	SMA5
1	0.1338				
2	0.4622				
3	0.1448	0.2980			
4	0.6538	0.3035	0.2469		
5	0.0752	0.3993	0.4203	0.3487	
6	0.2482	0.3645	0.2913	0.3340	0.2940
7	0.4114	0.1617	0.3257	0.2805	0.3168
8	0.3598	0.3298	0.2449	0.3472	0.3067
9	0.7809	0.3856	0.3398	0.2737	0.3497
10	0.5369	0.5704	0.5174	0.4501	0.3751
11	0.7889	0.6589	0.5592	0.5223	0.4674
12	0.1234	0.6629	0.7022	0.6166	0.5756
13	0.1335	0.4562	0.4831	0.5575	0.5180
14	0.3555	0.1285	0.3486	0.3957	0.4727
15	0.1444	0.2445	0.2041	0.3503	0.3876
Ave(0.1338,0.4622,0.1448)


12
March 5, 2019
13
Weighted Moving Average
Moving average where each value in the window is assigned a unique weight:
March 5, 2019
14
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
14


Selecting Weights
Sum is 1.0
More recent data is often more important
Other knowledge may skew weights
Equal weights is the same as single moving average (w=1/k)
March 5, 2019
15
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
15


Example - WMA
March 5, 2019
16
Position	Value	WMA2	WMA3	WMA4	WMA5
1	0.1338				
2	0.4622				
3	0.1448	0.3801			
4	0.6538	0.22415	0.2274		
5	0.0752	0.52655	0.4991	0.4128	
6	0.2482	0.21985	0.2353	0.2928	0.2967
7	0.4114	0.20495	0.2455	0.2692	0.2833
8	0.3598	0.3706	0.3327	0.3337	0.3161
9	0.7809	0.3727	0.3616	0.3138	0.3332
10	0.5369	0.675625	0.6306	0.5263	0.4797
11	0.7889	0.5979	0.5825	0.5497	0.5306
12	0.1234	0.7259	0.7215	0.6641	0.6392
13	0.1335	0.289775	0.3429	0.4678	0.4834
14	0.3555	0.130975	0.2003	0.3139	0.3566
15	0.1444	0.3	0.2723	0.3081	0.3201
0.106*0.1338+0.264*0.4622+0.630*0.1448
March 5, 2019
17
Exponential Moving Average
March 5, 2019
18
:	Forecast for period t
:	Last period forecast
:	Last period actual value
A higher α discounts older observations faster
Data Mining (BGU) - Lecture No. 10
March 5, 2019
Dr. Mark Last
18


Example - EMA
March 5, 2019
19
Position	Value	EMA 
1	0.1338	
2	0.4622	0.1338
3	0.1448	0.3232
4	0.6538	0.2203
5	0.0752	0.4703
6	0.2482	0.2424
7	0.4114	0.2458
8	0.3598	0.3413
9	0.7809	0.3520
10	0.5369	0.5994
11	0.7889	0.5633
12	0.1234	0.6934
13	0.1335	0.3647
14	0.3555	0.2313
15	0.1444	0.3029
		0.2115
		
		 
	Total	1.00
0.576*0.1448+0.423*0.323=0.2203
March 5, 2019
20
Discretization using Binning
Equal-frequency discretization (עומק שווה):
divides the sorted values into k intervals so that each interval contains approximately the same number of training instances.
Equal-width discretization (רוחב שווה): 
divides the number line between  and  into k intervals of equal width. 
Intervals width: w = ()/ 
Cut points: ;  … ; 
March 5, 2019
21
21


Example – equal width
Values of attribute ‘Age’: 0, 4, 12, 16, 16, 18, 23, 26, 28
Bin width = 10

March 5, 2019
22
Bin #	Bin elements	Bin boundaries
1	(0,4)	X <= 10
2	(12,16,16,18)	10 < x <= 20
3	(23,26,28)	X > 20
Values of attribute ‘Age’: 0, 4, 12, 16, 16, 18, 23, 26, 28
Bin depth = 3

March 5, 2019
23
Bin #	Bin elements	Bin boundaries
1	(0,4,12)	X <= 12
2	(16,16,18)	12 < x <= 18
3	(23,26,28)	X > 18
Example – equal depth
Binning Method for Data Smoothing
Values: 4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34
Partition into equal-depth bins:
Bin 1: 4, 8, 9, 15
Bin 2: 21, 21, 24, 25
Bin 3: 26, 28, 29, 34
Smoothing by bin means:
Bin 1: 9, 9, 9, 9
Bin 2: 23, 23, 23, 23
Bin 3: 29, 29, 29
Smoothing by bin boundaries:
Bin 1: 4, 4, 4, 15
Bin 2: 21, 21, 25, 25
Bin 3: 26, 26, 26, 34
March 5, 2019
24
Example
March 5, 2019
25
https://www.slideshare.net/sushil.kulkarni/ch-1-intro-to-data-mining-presentation
Example - Discretization
Equal-frequency (depth) discretization:
March 5, 2019
26
Sort

28
26


Equal-width discretization: 
March 5, 2019
27
Two intervals cut point: 25 + (35-25)/2 = 30
Width

Interval 1: X ≤ 30
Interval 2:  X > 30
Example - Discretization
27


 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\2--Information-theory\2--Information-theory.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
null 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\3---Statistics-and-decision-tree\3---Statistics-and-decision-tree.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
#3 – Statistical hypothesis testing and decision tree learning

nivah@post.bgu.ac.il


Data Science and Business Intelligence
 
3/25/2019
1

2
Model Estimation
2
Train accuracy (=100-Train error)
Test accuracy (=100-Test error)
Majority Rule
3/25/2019

3
Statistical Hypothesis Testing
: ההשערה השמרנית – מה שידוע לנו עד היום. מתייחסת לפרמטר מסוים באוכלוסייה על בסיס אינפורמציה/תיאוריה קודמת שיש לחוקר.
: ההשערה האלטרנטיבית - הטענה החדשה שאותה רוצים לאמת במחקר.
סטטיסטי המבחן: ערך המחושב מהמדגם, בעזרתו מחליטים אם לדחות את .
P-value: רמת המובהקות הקטנה ביותר עבורה נדחה את השערת האפס ע"ס תוצאה מדגמית.


3/25/2019
3

4
4
Confidence Interval for an Error Rate
Test error  is an estimate of the true error rate  on the population.
 is governed by Binomial distribution approximated by Normal when 
Assumption: n test samples are drawn randomly and independently from the entire population.
With the probability 1 - , the true error rate  lies in the confidence interval
3/25/2019
4

5
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
4
Data Mining (BGU) - Lecture No. 10
March 25, 2019
Dr. Mark Last
4


Confidence Interval - Example



















Error
Min.
Max.


3/25/2019
5

6
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
5
Data Mining (BGU) - Lecture No. 10
March 25, 2019
Dr. Mark Last
5

Alpha or alpha/2?
Explain z_alpha with a chart of the Normal distribution
6
Difference between Classifiers
Estimated difference between error rates

is governed by Binomial distribution approximated by Normal when n1, n2  30
With the probability 1 - , the true difference d lies in the confidence interval
3/25/2019
6

7
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
6
Data Mining (BGU) - Lecture No. 10
March 25, 2019
Dr. Mark Last
6

Alpha or alpha/2?
If α=0.01, we are not confident that model 1 is really more accurate than model 2
Difference between Classifiers - Example
7
 
 
3/25/2019
7

Error1
n1
Error2
n2
d
alpha
z_alpha
min.
max.
0.200
30
0.400
40
0.20
0.010
2.326
-0.046
0.448
0.200
30
0.400
40
0.20
0.050
1.645
0.025
0.375
0.200
30
0.400
40
0.20
0.100
1.282
0.064
0.336































8
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
7
Data Mining (BGU) - Lecture No. 10
March 25, 2019
Dr. Mark Last
7


Algorithm for Decision Tree Induction
Basic algorithm (greedy)
Tree is constructed in a top-down recursive divide-and-conquer manner
At start, all the training examples are at the root
Attributes are categorical (if continuous-valued, discretized in advance)
Examples are partitioned recursively based on selected attributes
Test attributes are selected on the basis of a heuristic or a statistical measure (e.g., information gain)
Conditions to stop partitioning
All samples for a given node belong to the same class
There are no remaining attributes – majority voting is employed for classifying the leaf
There are no samples left - majority voting 
8
3/25/2019

9
Basic Decision Tree Learning
ID3 Algorithm (based on Quinlan, Induction of Decision Trees, 1986)
Create a root node for the tree
If all examples have the same target value, return the single-node tree root labeled by that value
If there no attributes left, return the single-node tree root labeled by the most common value in the examples
Otherwise begin
9
3/25/2019

10
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
9


ID3 Algorithm (cont’d)
Select the attribute A that best classifies the examples 
Repeat for each possible value of A
Split the tree (add a new branch to the tree corresponding to each value v of the attribute A)
If there are no examples having value v for the attribute A
Add a leaf node below this branch labeled by the most common value in the examples
Else 
Create a subtree below this new branch 
10
3/25/2019

11
Data Mining (BGU) - Lecture No. 5
March 25, 2019
Dr. Mark Last
10


Attribute Selection in ID3 : Information Gain
Select the attribute with the highest information gain
Let  be the probability that an arbitrary tuple in  belongs to class , estimated by |
Expected information (entropy) needed to classify a tuple in :

Information (conditional entropy) needed (after using  to split  into  partitions) to classify D:

Information gained (mutual information) by branching on attribute 
11
3/25/2019

12
11


ID3
12
Root node
A1
A2
A3
Value 1
Value 2
Value 1
Value 2
Value 3
Value 1
Value 2
Value 1
Value 2
A3
A1
A2
A2
3/25/2019

13


Week

Weather
Parents
Money
Decision (Class)
W1
Sunny
Yes
Rich
Cinema
W2
Sunny
No
Rich
Tennis
W3
Windy
Yes
Rich
Cinema
W4
Rainy
Yes
Poor
Cinema
W5
Rainy
No
Rich
Stay in
W6
Rainy
Yes
Poor
Cinema
W7
Windy
No
Poor
Cinema
W8
Windy
No
Rich
Shopping
W9
Windy
Yes
Rich
Cinema
W10
Sunny
No
Rich
Tennis



































An Example…



13
Meaningless

3/25/2019

14
13


ID3 Example
Root node
C=6,T=2,Si=1, Sh=1 
3/25/2019
14

15
ID3 to build a decision tree
Figure out which attribute will be put into the node at the top of our tree: 
	Weather, Parents or Money. 
To do this, we first need to calculate: 
= 

15
3/25/2019

16
15


Now lets calculate the gain for the weather attribute:
Gain(D, weather) = 1.571 – Infoweather(D)
Infoweather(D) = (|Dsunny|/10)*Info(Dsunny) + (|Dwindy|/10)*Info(Dwindy) + (|Drainy|/10)*Info(Drainy) 
Where, 
Info(Dsunny) = -pcinema|sunnylog2(pcinema|sunny)-ptennis|sunnylog2(ptennis|sunny)
	= -(1/3)*log2 (1/3)-(2/3)*log2 (2/3) = 0.918
Info(Dwindy) = -pcinema|windylog2(pcinema|windy) –pshopping|windylog2(pshopping|windy)
	= -(3/4)*log2 (3/4)-(1/4)*log2 (1/4) = 0.811
Info(Drainy) = -pcinema|rainylog2(pcinema|wind) –pstay_in|rainylog2(pstay_in|rainy)
	= -(2/3)*log2 (2/3)-(1/3)*log2 (1/3) = 0.918
Gain(D, weather) = 1.571 - (3/10)*Info(Dsunny) - (4/10)*Info(Dwindy) - (3/10)*Info(Drainy) = 0.69
(2,1,0,0)
(3,0,1,0)
(2,0,0,1)

17
16


Now lets calculate the gain for the parents attribute:
Gain(D, parents) = 1.571 - (|Dyes|/10)*Info(Dyes) - (|Dno|/10)*Info(Dno)
Where, 
Info(Dyes) = -pcinema|yeslog2(pcinema|yes)
	= -(5/5)*log2(5/5)= 0
Info(Dno) = -ptennis|nolog2(ptennis|no) –pstay_in|nolog2(pstay_in|no) -pcinema|nolog2(pcinenema|no) pshopping|nolog2(pshopping|no)
	= -(2/5)*log2 (2/5)-(1/5)*log2 (1/5)- (1/5)*log2 (1/5)-(1/5)*log2(1/5) = 1.922
Gain(D, parents) = 1.571 - (5/10)*Info(Dyes) - (5/10)*Info(Dno) = 0.61
17
3/25/2019

18
17


Now lets calculate the gain for the money attribute:
Gain(D, money) = 1.571 - (|Drich|/10)*Info(Drich) - (|Dpoor|/10)*Info(Dpoor) 
Where, 
Info(Drich) = -pcinema|richlog2(pcinema|rich) -ptennis|richlog2(ptennis|rich) –pstay_in|richlog2(pstay_in|rich) -pshoppinglog2(pshopping) 
	= -(3/7)*log2(3/7)-(2/7)*log2(2/7)-(1/7)*log2(1/7)-(1/7)*log2(1/7)= 1.842
Info(Dpoor) = -pcinema|nolog2(pcinema|no) 
	= -(3/3)*log2 (3/3) = 0
Gain(D, money) = 1.571 - (7/10)* Info(Drich) - (3/10)* Info(Dpoor) = 0.28
18
3/25/2019

19
18


Information Gain	Attribute
0.69	Weather
0.61	Parents
0.28	Money
19
3/25/2019

20
The first node in the decision tree will be the weather attribute.
Now we look at the first branch. Ssunny = {W1, W2, W10}. 
We do not put a default categorization leaf node here (Why?).  
20
Sunny (n=3)
C=1, T=2, Sh=0, Si=0
Windy (n=4)
C=3, T=0, Sh=1, Si=0
Weather
Rainy
C=2, T=0, Sh=0, Si=1
Root node
C=6,T=2,Si=1, Sh=1 

3/25/2019

21
20


Now we have to fill in the choice of attribute , which we know cannot be weather, because we've already removed that from the list of attributes to use. 
So, we need to calculate the values for and 
.


Week
Weather
Parents
Money
Decision 
(Category)
W1
Sunny
Yes
Rich
Cinema
W2
Sunny
No
Rich
Tennis
W10
Sunny
No
Rich
Tennis






















21
3/25/2019

22
21


Hence we can calculate: 
Gain(Dsunny,parents) = 0.918 - (|Dyes|/|Dsunny|)*Info(Dyes) - (|Dno|/|Dsunny|)*Info(Dno)      
= 0.918 - (1/3)*0 - (2/3)*0 = 0.918 
Gain(Dsunny,money) = 0.918 - (|Drich|/|Dsunny|)*Info(Drich) - (|Dpoor|/|Dsunny|)*Info(Dpoor)
= 0.918 - (3/3)*0.918 - 0 = 0.918 - 0.918 = 0 
Note: Info(Dyes) and Info(Dno) were both zero, because Dyes and Dno contains examples which are all in the same category (cinema, tennis). 
This should make it more obvious why we use information gain to choose attributes. 
Info(Dsunny)
22
3/25/2019

23
22


Given our calculations, attribute  should be taken as parents. 
23
Root node
C=6,T=2,Si=1, Sh=1 
Sunny (n=3)
C=1, T=2, Sh=0, Si=0
Windy (n=4)
C=3, T=0, Sh=1, Si=0
Yes (n=1)
C=1
No (n=2)
T=2
Weather
Parents
Rainy
C=2, T=0, Sh=0, Si=1
3/25/2019

24
23


Now we need to calculate the values for Gain(Dwindy, parents) and Gain(Dwindy, money).
24
3/25/2019


Week
Weather
Parents
Money
Decision 
(Category)
W3
Windy
Yes
Rich
Cinema
W7
Windy
No
Poor
Cinema
W8
Windy
No
Rich
Shopping
W9
Windy
Yes
Rich
Cinema

























25
24


Gain(Dwindy,parents) = 0.811 - (|Dyes|/|Dwindy|)*Info(Dyes) - (|Dno|/|Dwindy|)*Info(Dno) 
= 0.811 - (2/4)*0 - (2/4)*1 = 0.311
Gain(Dwindy,money)=0.811- (|Drich|/|Dwindy|)*Info(Drich) - (|Dpoor|/|Dwindy|)*Info(Dpoor)
= 0.811 - (3/4)*0.918 - (1/4)*0 = 0.122
Meaning that this node will be split by parents too.
After calculating the Gain for rainy too, the final tree is…
25
3/25/2019

26
25


26
Root node
C=6,T=2,Si=1, Sh=1 
Sunny (n=3)
C=1, T=2, Sh=0, Si=0
Windy (n=4)
C=3, T=0, Sh=1, Si=0
Yes (n=2)
C=2
No (n=2)
C=1, Sh=1
Rich (n=1)
Sh=1
Yes (n=1)
C=1
No (n=2)
T=2
Weather
Parents
Parents
Rainy
C=2, T=0, Sh=0, Si=1
Poor (n=1)
C=1
Rich (n=1)
Si=1
Poor (n=2)
C=2
Money
Money
3/25/2019

27
26


Accuracy Estimation
Training Accuracy Rate
The percentage of training set samples that are correctly classified by the model.
Testing Accuracy Rate
The percentage of test set samples that are correctly classified by the model.
Test set is independent of training set, otherwise over-fitting will occur.
Majority Rule Accuracy
To select a class for a terminal node, select the class having the most examples (in training set).
Majority rule accuracy = |DA|/|D|
The classification model should be more accurate than the majority rule.
27
3/25/2019

28
27

תצפיות
Accuracy Estimation
In our example, training accuracy is 10/10 = 100 %
Take the following testing set for example:
Testing accuracy is: 3/4 = 75%
Majority rule accuracy = 6/10 = 60% 
28
3/25/2019

29
28


Fano’s Inequality
Fano’s Inequality: 

Interpretation: Relationship between the minimum prediction error Pe, the conditional entropy of the target H, and the number of classes m (“upper bound of predictability”(
Sunny (n=3)
C=1, T=2, Sh=0, Si=0
Windy (n=4)
C=3, T=0, Sh=1, Si=0
Weather
Rainy
C=2, T=0, Sh=0, Si=1
Root node
C=6,T=2,Si=1, Sh=1 


Pe ≈ 0.158   max test accuracy = 84.2%

30
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\4---Decision-tree\4---Decision-tree.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
null 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\5---Desicion-Trees\5---Desicion-Trees.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
#5 – Decision tree learning (cont.)

nivah@post.bgu.ac.il

1
Data Science and Business Intelligence
 
1


2
Splitting Rules
Possible splitting functions (rules)
Entropy (Information Gain and Gain Ratio)
Gini Index
Twoing


3
Information Gain Ratio
The problem with multi-valued and continuous attributes in noisy databases
The probability of  a subset of examples to have the same class increases monotonically with a decrease in the subset size
The extreme case is a subset of one example
The average size of a subset decreases with an increase in the total number of attribute values (e.g., attribute Date)
Conclusion
Information gain is biased towards multi-valued and continuous attributes 

Data Mining (BGU) - Lecture No. 5
April 10, 2019
Prof. Mark Last
3

ההסתברות לדגימה שכל הרשומות בה עם אותו סיווג גדלה ככל שגודל הדגימה קטן.
גודל הדגימה הממוצע קטן ככל שיש יותר ערכים אפשריים לתכונה (כשמחלקים את הנתונים לדגימות לפי אותה תכונה).
מסקנה – infoGain מוטה לטובת תכונות בעלות הרבה ערכים אפשריים. 
The Gain Ratio Approach
4
The Gain Ratio Approach
“Punish” the multi-valued attributes via dividing (normalizing) their information gain by the Split Information: 



The Split Information represents the entropy of the tested attribute (in contrast to the entropy of the target attribute)
The Gain Ratio: Gain(A)/SplitInfo(A)
4


The Gain Ratio Approach - Example
5
5

 IV =  intrinsic value = SplitInfo
The Gain Ratio Approach - Example
Info(Decision) = Info(1,2,3) =
    -1/6log21/6 -2/6log22/6 -3/6log23/6 = 1.46
InfoTemperature(Decision) = 1/6*0 + 2/6*1 + 1/6*0 +1/6*0 + 1/6*0 = 0.33
Gain(Temperature) = 1.46-0.33 = 1.13
The Gain Ratio: Gain(A) / IV(A)
Gain Ratio (Temperature) = 1.13/2.25 = 0.502

6
6


The Gain Ratio Approach - Example
InfoHumidity(Decision) = 2/6*0 + 1/6*0 + 2/6*0 +1/6*0 = 0
Gain(Humidity) = 1.46-0 = 1.46
Gain Ratio (Humidity) = 1.46/1.92 = 0.76
0.76 > 0.502  We’ll prefer the Humidity attribute.

7
7


The Gain Ratio Split
8


Record	Attribute	Class
1	1	0
2	1.5	1
3	1.5	0
4	1.7	1
5	2.1	1
The Gain Ratio Split
9
Record	Attribute	Class
1	1	0
2	1.5	1
3	1.5	0
4	1.7	1
5	2.1	1
N(L,R)	0		1		Total	
	L	R	L	R	L	R
1	1	1	0	3	1	4
1.5	2	0	1	2	3	2
1.7	2	0	2	1	4	1
2.1	2	0	3	0	5	0

Classes
The Gain Ratio Split
P(L,R)+	0		1		Entropy		Gain	G.R.
	L	R	L	R	L	R		
1 (1,4)	1	=(1/4) =0.25	0	0.75	0	0.81	0.32	0.17
1.5 (3,2)	0.67	0	0.33	1	0.92	0	0.42	0.22
1.7 (4,1)	0.5	0	=(2/4) =0.5	1	1	0	0.17	0.09
2.1 (5,0)	0.4	 0	0.6	 0	0.97	0 	0	0
10
-0.670.67 -0.330.33
0.971-(3/5)*0.92-(2/5)*0
0.17/1.92
Gini Index
11
Looks for the largest class in the data set and strives to isolate it from all other classes
Gini Index
If a data set T contains examples from n classes, gini index, gini(T) is defined as:

  where pj is the relative frequency of class j in T.
If a data set T is split into two subsets T1 and T2 with sizes N1 and N2 respectively, the gini index of the split data contains examples from n classes, the gini index gini(T) is defined as:

The attribute provides the smallest ginisplit(T) is chosen to split the node (need to enumerate all possible splitting points for each attribute).
Reduction in Impurity:

12


Gini Index - Example
Record	Attribute	Class
1	1	0
2	1.5	1
3	1.5	0
4	1.7	1
5	2.1	1
N(L,R)	0		1		Total	
	L	R	L	R	L	R
1	1	1	0	3	1	4
1.5	2	0	1	2	3	2
1.7	2	0	2	1	4	1
2.1	2	0	3	0	5	0
13

Classes
P(L,R)+	0		1		Gini(A)		Gini
Split
	L	R	L	R	L	R	
1 (1,4)	=1/1	1/4	=0/1	=3/4	0	0.38	0.3
1.5 (3,2)	0.67	0	0.33	1	0.44	0	0.27
1.7 (4,1)	0.5	0	=2/4	1	0.5	0	0.4
2.1 (5,0)	=2/5	0 	=3/5=0.6	 0	0.48	1 	0.48


14
((3/5)*0.44+(2/5)*0)
1-(2/5)2+(3/5)2)
1-((1/4)2+(3/4)2)
Twoing
15
Attempts to find groups of up to 50% of the data each
The Twoing Split Example
P(L,R)+	0		1		|PL-PR|		Total	Twoing
	L	R	L	R	0	1		
1 (1,4)	1	0.25	0	0.75	0.75	0.75	1.5	0.09
1.5 (3,2)	0.67	0	0.33	1	0.67	0.67	1.33	0.107
1.7 (4,1)	0.5	0	0.50	1	0.5	0.5	1	0.04
2.1 (5,0)	0.4	 0	0.60	 0	 0.4	0.6 	1	0
16
=|1-0.25|

17
CART – Classification and Regression Tree
Objective: minimizing the cost-complexity function

T - a tree
R (T) - the training error rate of a tree
R (T) - the cost-complexity of a tree
     - number of terminal nodes in a tree
 - complexity parameter (real number, greater than zero)


CART - Example
18
Week	Weather	Parents	Money	Decision
W1	Sunny	Yes	Rich	Cinema
W2	Sunny	No	Rich	Tennis
W3	Windy	Yes	Rich	Cinema
W4	Rainy	Yes	Poor	Cinema
W5	Rainy	No	Rich	Stay in 
W6	Rainy	Yes	Poor	Cinema
W7	Windy	No	Poor	Cinema
W8	Windy	No	Rich	Shopping
W9	Windy	Yes	Rich	Cinema
W10	Sunny	No	Rich	Tennis
W11	Windy	No	Rich	Shopping
W12	Windy	No	Rich	Shopping
W13	Windy	Yes	Poor	Cinema
18


CART – Example Maximal Tree ( = 0)
19

Weather
Parents
Parents
Money
Money
Sunny
Windy
Rainy
Yes
No
No
Yes
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Rich
Poor
Cinema
(W7)
Shopping
(W8, W11, W12)
Cinema
(W3,W9, W13)
Stay in
(W5)
Cinema
(W4,W6)
Error:
Before pruning: 0% 
After pruning: 33%

CART – Example
Cost-complexity of the single node t
R({t}) = R(t) + *1 = 0.33 +   // After Pruning
Cost-complexity of the branch Tt
R(Tt) = R(Tt) + *|Ťt| = 0 +*2 // Before Pruning
The critical value of 
R({t}) = R(Tt) 
0.33 +  = 2  
 = 0.33

20
CART – Example Maximal Tree ( = 0)
21

Weather
Parents
Parents
Money
Money
Sunny
Windy
Rainy
Yes
No
No
Yes
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Rich
Poor
Cinema
(W7)
Cinema
(W3,W9, W13)
Stay in
(W5)
Cinema
(W4,W6)
Error:
Before pruning: 0% 
After pruning: 25%

Shopping
(W8, W11, W12)
CART – Example
Cost-complexity of the single node t
R({t}) = R(t) + *1 = 0.25 +  
Cost-complexity of the branch Tt
R(Tt) = R(Tt) + *|Ťt| = 0 +*2 
The critical value of 
R({t}) = R(Tt) 
0.25 +  = 2  
 = 0.25

22
CART – Example Maximal Tree ( = 0)
23

Weather
Parents
Parents
Money
Money
Sunny
Windy
Rainy
Yes
No
No
Yes
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Rich
Poor
Cinema
(W7)
Cinema
(W3,W9, W13)
Stay in
(W5)
Cinema
(W4,W6)
Error:
Before pruning: 0% 
After pruning: 33%

Shopping
(W8, W11, W12)
CART – Example
Cost-complexity of the single node t
R({t}) = R(t) + *1 = 0.33 +  
Cost-complexity of the branch Tt
R(Tt) = R(Tt) + *|Ťt| = 0 +*2 
The critical value of 
R({t}) = R(Tt) 
0.33 +  = 2  
 = 0.33

24
CART – Example Maximal Tree ( = 0)
25
CART – Example Optimal Tree #1( = 0.25)
26
Error:
Before pruning: ~14% 
After pruning: ~43%
CART – Example
Cost-complexity of the single node t
R({t}) = R(t) + *1 = 0.43 +  
Cost-complexity of the branch Tt
R(Tt) = R(Tt) + *|Ťt| = 0.14 +*2 
The critical value of 
R({t}) = R(Tt) 
0.43 +  = 0.14 + 2  
 = 0.29

27
CART – Example Optimal Tree #1( = 0.25)
28
CART – Example Optimal Tree #2( = 0.29)
29
CART – Example Optimal Tree #3 ( = 0.33)
30
Compare Trees using Test Set
31



Weekend 
(Example)
Weather
Parents
Money
Decision 
(Class)
W14
Sunny
Yes
Poor
Cinema
W15
Sunny
No
Rich
Tennis
W16
Windy
No
Rich
Cinema
W17
Rainy
Yes
Poor
Cinema












































#1
Test Set Accuracy = ¾ = 75 % 


Weather
Parents
Parents
Money
Sunny
Windy
Rainy
Yes
No
No
Yes
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Cinema
(W3,W9, W13)
Stay in
(W5)
Cinema
(W4,W6)
Shopping
Compare Trees using Test Set
32
#2
Test Set Accuracy = 4/4 = 100 % 
Stay in

Weather
Parents
Money
Sunny
Windy
Rainy
Yes
No
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Cinema

Stay in
(W5)
Cinema
(W4,W6)



Weekend 
(Example)
Weather
Parents
Money
Decision 
(Class)
W14
Sunny
Yes
Poor
Cinema
W15
Sunny
No
Rich
Tennis
W16
Windy
No
Rich
Cinema
W17
Rainy
Yes
Poor
Cinema












































33
#3
Test Set Accuracy =3/4 = 75% 
Stay in



Weekend 
(Example)
Weather
Parents
Money
Decision 
(Class)
W14
Sunny
Yes
Poor
Cinema
W15
Sunny
No
Rich
Tennis
W16
Windy
No
Rich
Cinema
W17
Rainy
Yes
Poor
Cinema














































Weather
Sunny
Windy
Rainy
Tennis

Cinema

Cinema
Compare Trees using Test Set
34
Original Tree
Test Set Accuracy =3/4 = 75% 
Stay in



Weekend 
(Example)
Weather
Parents
Money
Decision 
(Class)
W14
Sunny
Yes
Poor
Cinema
W15
Sunny
No
Rich
Tennis
W16
Windy
No
Rich
Cinema
W17
Rainy
Yes
Poor
Cinema














































Weather
Parents
Parents
Money
Money
Sunny
Windy
Rainy
Yes
No
No
Yes
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Rich
Poor
Cinema
(W7)
Shopping
(W8, W11, W12)
Cinema
(W3,W9, W13)
Stay in
(W5)
Cinema
(W4,W6)
Compare Trees using Test Set
The Optimal Tree
35
Tree # 2

Weather
Parents
Money
Sunny
Windy
Rainy
Yes
No
Rich
Poor
Cinema
(W1)
Tennis
(W2,W10)
Cinema

Stay in
(W5)
Cinema
(W4,W6)
 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\6---IFN\6---IFN.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
#6 – IFN - Info Fuzzy Network 

nivah@post.bgu.ac.il

1
Data Science and Business Intelligence
 
1


IFN - Network Construction Procedure
IFN מייצר מודל של רשת נוירונים. 
האלגוריתם מבצע גיזום מוקדם.
בכל אחת מהרמות השונות של הרשת יש התחשבות בכל התצפיות ב-dataset. 
כל רמה ברשת תפוצל לפי תכונה אחת בלבד.
2
3




Layer No. 0
(the root node)
0
1
2
3
Connection Weights

Target Layer
(Target Attribute)
3 Values



1
2
3



Layer No. 1
(First input attribute)
3 values




1,1
1,2
3,1
3,2
















Layer No. 2
(Second input attribute)
2 values



IFN - Network Construction Procedure
3


4
Network Induction Algorithm
Input:
Extended relation schema (partition of attribute set)
Set of training records
Minimum significance level (default = 0.1%)
Output:
Set of selected input attributes
Information-theoretic network
Step 1 - Initialize the information-theoretic network.
Step 2 - While the maximum number of hidden layers is not exceeded:
Step 2.1 - Find a candidate input attribute maximizing the statistically significant conditional mutual information (“the best candidate attribute”).
Step 2.2 - If the maximum conditional mutual information is greater than zero, make the best candidate attribute an input attribute and define a new layer of hidden nodes; else stop.
Step 3 – Return the set of selected attributes and the network structure
4


IFN: Conditional Mutual Information at Node z

MI (Ai’ ; Ai / z) = 
Ai  - target attribute No. i
Ai’  - candidate input attribute No. i’
Vij - value No. j of attribute Ai
z - network node (representing a conjunction of input attribute values)
P (Vij/ z) - an estimated conditional (a posteriori) probability of Vij, given the node z
P (Vi’j’ij/ z) - an estimated conditional (a posteriori) probability of Vi’j’ and Vij, given the node z
P (Vij; Vi’j’; z) - an estimated joint probability of Vi’j’Vij, and the node z 
Data Mining (BGU) - Lecture No. 7
01 May 2019
Prof. Mark Last
5


6
IFN: Likelihood-Ratio Statistic
A - target attribute 
Ai  - candidate input attribute No. i
z - network node (representing a conjunction of input attribute values)
E* - total number of training cases
MI (Ai ; A | z)  - conditional mutual information 
Statistical value
6


7
IFN: Likelihood-Ratio Statistic (cont.)



If                   Reject H0
A node is split if H0 is rejected at the significance level specified (0.1%)




Null Hypothesis: attribute Ai is irrelevant to classifying the records in data set.
Alternative Hypothesis: attribute Ai affects the class distribution in data set.
NI i (z)  - number of values of a candidate input attribute i at node z
NT (z) - number of values of a target attribute at node z 
7


Decision
8
Example - Credit Approval Dataset 
Source: http://www.ics.uci.edu/~mlearn/MLRepository.html

14 attributes

8


9

9
0
Root node
Reject
Accept
Iteration No. 0: no input attributes, no hidden layers
Target layer
(Class)
0
1
9


10
MI (Ai’ =0 ; A =0 | z)= (306/690)*log2((306/690)/((329/690)*(383/690))) = 0.3303
MI (Ai’ =1 ; A =0 | z)= (77/690)*log2((77/690)/((361/690)*(383/690))) = -0.1540
MI (Ai’ =0 ; A =1 | z)= (23/690)*log2((23/690)/((329/690)*(307/690))) =  0.089-
MI (Ai’ =1 ; A =1 | z)= (284/690)*log2((284/690)/((361/690)*(307/690))) = 0.3384
Conditional mutual information - MI (Ai’ ; Ai | z) = 0.3303 - 0.1540 0.089- + 0.3384 = 0.426 bits
G2= 2*LN(2)*690*0.426 = 407.48
Significance: 20.001(11) = 10.82 < 407.48  Thus:
Conclusion: reject H0 (consider Other Investments as the next input attribute)

Values of 
“Other Invest.”

j/ j’
0
Cond.
Joint
1
Cond.
Joint
Total
Cond.
0
306
=306/690
=306/690
23
=23/690
=23/690
329
=329/690
1
77
=77/690
=77/690
284
=284/690
=284/690
361
=361/690
Total
383
=383/690
307
=307/690
690

























































10


11
IFN: Example - Credit Dataset
Layer 0




not significant
11


12
IFN Construction Procedure (1) 
Credit Approval Dataset

12
0
1
2
0
1


Other investments = No
Other investments = Yes
Layer 1
(Other investments )
Root node
Hidden node
Reject
Accept
Target layer
(Class)
Iteration No. 1: one input attribute, one hidden layer
12


13
IFN: Example - Credit Dataset
Contingency Table: Balance (Node 1, T = 445)
Other Investments = No
MI(Ai’ <=445; Ai =0|z)= (271/690)*log2((271/329)/((291/329)*(306/329))) = 0.000719
MI(Ai’ > 445; Ai =0|z)= (35/690)*log2((35/329)/((38/329)*(306/329))) = -0.000714
MI(Ai’ <=445; Ai =1|z)= (20/690)*log2((20/329)/((291/329)*(23/329))) = -0.000712
MI(Ai’ > 445; Ai =1|z) = (3/690)*log2((3/329)/((38/329)*(23/329))) = 0.000762

Conditional mutual information - MI (Ai’ ; Ai | z) = 0.0000546 bits

G2 = 2*LN(2)*690*0.0000546=0.0522
Significance: 20.001(11) = 10.82 > 0.052,  Thus:
Conclusion: do not reject H0 (do not split the Balance attribute at this node for the specified threshold).
A possible threshold

















































































j' / j
0
Cond.
Joint
1
Cond.
Joint
Total
Cond.
<=445
271
=271/329
=271/690
20
=20/329
=20/690
291
=291/329
>445
35
=35/329
=35/690
3
=3/329
=3/690
38
=38/329
Total
306
=306/329
=306/690
23
=23/329
329

































































13


14
IFN: Example  - Credit Dataset
Contingency Table: Balance (Node 2, T = 445)
 Other Investments = Yes
MI (Ai’ <=445 ; Ai =0 | z) = (74/690)*log2((74/361)/((230/361)*(77/361))) =0.0636
MI (Ai’ > 445 ; Ai =0 | z) =(3/690)*log2((3/361)/((131/361)*(77/361)))= -0.0140
MI (Ai’ <=445 ; Ai =1 | z)=(156/690)*log2((156/361)/((230/361)*(284/361)))= -0.0484
MI (Ai’ > 445 ; Ai =1 | z) = (128/690)*log2((128/361)/((131/361)*(284/361)))= 0.0580
Conditional mutual information MI (Ai’ ; Ai |z) = 0.0592 bits

G2 = 2*LN(2)*690*0.0592 = 56.6564
Significance: 20.001(11) = 10.82 < 56.6564,  Thus:
Conclusion: reject H0 (split the Balance attribute at this node for the specified threshold)
Suppose that Tmax = 445













































































































j' / j
0
Cond.
Joint
1
Cond.
Joint
Total
Cond.
<=445
74
=74/361
=74/690
156
=156/361
=156/690
230
230/361
>445
3
=3/361
=3/690
128
=128/361
=128/690
131
=131/361
Total
77
=77/361
=77/690
284
=284/361
=284/690
361

































































14


15
IFN: Example - Credit Dataset
Layer 1
15


16
IFN Construction Procedure (2) 
Credit Approval Dataset

0
1
3
4
2
0
1




Other investments = No
Other investments = Yes
Balance between $1 and $445
Balance> $445
Layer 1
(Other investments )
Layer 2
(Balance)


Root node
Hidden node
-0.089
0.3303
Reject
Accept
Target layer
(Class)
Iteration No. 2: two input attributes, two hidden layers
Terminal node
16


17
IFN: Example 1 - Credit Dataset
Layer 2
17


18
IFN Construction Procedure (3) 
Credit Approval Dataset

18
0
1
3
4
5
6
2
0
1






Other investments = No
Other investments = Yes
Balance between $1 and $445
Balance> $445
Bank account=Yes
Bank account=No
Layer 1
(Other investments )
Layer 2
(Balance)
Layer 3
(Bank Account)




Root node
Hidden node
-0.089
0.3303
-0.02
0.2106
Reject
Accept
Target layer
(Class)
Iteration No. 3: three input attributes, three hidden layers
18


19
IFN: Example 1 - Credit Dataset
Layer 3
19


20
Example - Credit Approval Dataset
Info-Fuzzy Network (IFN)
0
1
3
4
5
6
2
0
1
Target layer
(Class)






Other investments = No
Other investments = Yes
Balance between $1 and $445
Balance> $445
Bank account=Yes
Bank account=No
Layer 1
(Other investments )
Layer 2
(Balance)
Layer 3
(Bank Account)








Root node
Hidden node
Terminal node
-0.089
0.3303
-0.02
0.2106
-0.0141
0.016
-0.0492
0.1313
Reject
Accept
20


21
Connection Weight:
Interpretation: mutual information between a node z and the value j of the target attribute A.
Rule Extraction and Scoring
wzij > 0: If z then Vij
wzij < 0: If z then not Vij
Vij - value No. j of target attribute Ai
P (Vij) - an estimated unconditional (a priori) probability of Vij, 
P (Vij/ z) - an estimated conditional (a posteriori) probability of Vij, given the node z.
21


22
IFN: Example - Credit Dataset

Other investments = No: 329 records
Other investments = No and Class = Reject: 306 records
Class = Reject: 383 records
Total records: 690
P (Vij) = 383 / 690 = 0.5551
P (Vij | z) = 306 / 329 = 0.9301
P (Vij ; z) = 306 / 690 = 0.4435



Calculating weight between node 1 and “reject”

22


23
IFN: Example - Credit Dataset

Balance > 445 : 131 records
Balance > 445 and Class = Accept and Z: 128 records
Class = Accept: 307 records
Total records: 690
P (Vij) = 307 / 690
P (Vij | z) = 128/ 131
P (Vij ; z) = 128 / 690


Calculating weight between node 4 and “accept”

23


24

IFN: Example - Credit Dataset 
Extracted Rules

Data Mining (BGU) - Lecture No. 7
01 May 2019
Prof. Mark Last
24


Prediction
25
Predicted Value (maximum a posteriori):
(of the target attribute Ai at the node z)

0
1
3
4
5
6
2
0
1






Other investments = No
Other investments = Yes
Balance between $1 and $445
Balance> $445
Bank account=Yes
Bank account=No








0.07
0.93
0.023
0.977
0.491
0.509
0.161
0.839
Reject
Accept
=306/329


 
</TEXT>
</DOC>
<DOC>
<DOCNO> C:\Dan\UNI\Jarta.Projects\Hackathon\searchEngine\corpus\MoodleFiles\מדעי-הנתונים-ובינה-עסקית-סמ-2\תרגולים\8---Naive-bayes\8---Naive-bayes.pptx </DOCNO>
<F P=104> מדעי-הנתונים-ובינה-עסקית-סמ-2 </F>
<TEXT>
#8 – Naïve Bayes
nivah@post.bgu.ac.il

1
Data Science and Business Intelligence
 
1


Bayesian Theorem: Basics
P(H) (prior probability): the initial probability that the hypothesis H is correct
P(X) (evidence): probability to observe a given record
P(X|H) (likelihood): the probability to observe record X, given that the hypothesis holds

2
3
Naïve Bayes Classifier
Let X be a data sample whose class label is unknown
Xk – The value of attribute k in the data sample
Ci – Class i
If P(X|Ci) is known, assign X to the class with maximum P(X|Ci)*P(Ci)
3

מסווג naïve bayes מניח כי האפקט של ההערך של ה-attribute על קלאס מסוים הוא בלתי-תלוי בערך של ה-attributes האחרים. מטרת הנחה זו היא לפשט את החישובים הדרושים ובמובן זה האלגוריתם נחשב "נאיבי". 

4
m-estimate



    
nc : number of examples for which v = vj and c = ci
n : number of training examples for which c = cj
m : equivalent sample size (constant, usually 2)
p : uniform prior (1/M, M – number of diff. values)

If m = 0, the m-estimate is equivalent to
4


5
Laplacian Estimate
Laplacian correction (or Laplacian estimate)
Add 1 to the numerator and K to the denominator




K – the number of different values in the attribute
The “corrected” prob. estimates are close to their “uncorrected” counterparts
5
Data Mining (BGU) - Lecture No. 6
May 15, 2019
Prof. Mark Last
5


6
Naïve Bayes Classifier - Example
Decision
6


7
Data Sample: X = (Credit history = unknown, Debt = high, Collateral = Adequate, Income >35)
Probabilities (from the data):
P(high) = 5/14
P(moderate) = 4/14
P(low) = 5/14
Naïve Bayes Classifier - Example
7


8
Calculate P(xk|Ci) for each k and i:
k = Credit History, Xk = unknown:


P(Credit history unknown | Risk high) =(2+2/3)/(5+2) =0.38


P(Credit history unknown | Risk moderate) =(1+2/3)/(4+2) = 0.28

P(Credit history unknown | Risk low) = (2+2/3)/(5+2) = 0.38

2 samples with both values
m = 2 , p = 1/3
5 samples with Risk = high
Using m-estimate
M=3 (Bad, Good, Unknown)
8


9
Calculate P(xk|Ci) for each k and i:
k = Debt, xk = high:

P(Debt high | Risk high)=(4+1)/(5+2) = 0.71

P(Debt high | Risk moderate)=(1+1)/(4+2)=0.33

P(Debt high | Risk low)=(2+1)/(5+2)=0.43
M=2 (Low, High)


9
10
Calculate P(xk|Ci) for each k and i:
k = Collateral, xk = adequate:

P(Collateral adequate | Risk high)=(0+1)/(5+2)=0.14

P(Collateral adequate | Risk moderate)=(1+1)/(4+2) =0.33

P(Collateral adequate | Risk low)=(2+1)/(5+2)=0.43

M=2 (None, Adequate)
10


11
Calculate P(xk|Ci) for each k and i:
k = Income, xk = > 35 :

P(income >35 | Risk high) = (0+2/3)/(5+2)= 0.1

P(income >35 | Risk moderate) = (1+2/3)/(4+2) = 0.28

P(income >35 | Risk low) = (5+2/3)/(5+2) = 0.81
M=3 (0-15, 15-35, >35)
12
Calculate P(X | Ci) for each i:


P(X | Risk high) = 0.38*0.71*0.14*0.1=0.0037


P(X | Risk moderate) = 0.28*0.33*0.33*0.28=0.0085

P(X | Risk low) = 0.38*0.43*0.43*0.81=0.0569

P(Credit history = unknown | Risk = high)
P(Debt = high | Risk  = high)
P(Collateral = adequate | Risk  = high)
P(income >35 | Risk = high)
12


13
Calculate P(Ci)*P(X | Ci)

P(Risk high)*P(X | Risk high) = 5/14*0.0037 = 0.0013

P(Risk moderate)*P(X | Risk moderate) = 4/14*0.0085 = 0.0024

P(Risk low)*P(X | Risk low) = 5/14*0.0569 = 0.0203
	
Decision: X belongs to the class: Risk = low
14
Bayesian Belief Network
DAG (directed acyclic graph)
Nodes - set of random variables
Directed arrow from X to Y means that X has a direct influence on Y.
Each node has a CPT (conditional probability table) that quantifies the effects that the parents have on the node.  The parents of a node are all those nodes that have arrows pointing to it.
15
Example
Smoke and radiation both cause cancer (but not only...). 
Cancer cause fever and/or pain.
Fever and pain also caused by other diseases.
16
Example: Topology of Belief Network

Smoke

Radiation 

Cancer

Fever

Pain




17
Example: CPT for Cancer Node

Smoke
Radiation
P(Cancer|Smoke,Radiation)         True          False
True              True                               0.750            0.250
True              False                              0.700            0.300
False             True                               0.600            0.400
False             False                     	   0.010            0.990        
18
Example: Complete Belief Network

Smoke

Radiation

Cancer

Fever

Pain





P(S)
0.100


P(R)
0.002

S       R    P(C|S,R)
  T       T        0.75  
  T       F        0.70
  F       T        0.60
  F       F        0.01


C      P(F|C)
T         0.50
F         0.80

C       P(P|C)
T         0.70
F         0.20

19
Classification
 A generic entry in the joint probability distribution is the probability of a conjunction of particular assignments to each variable, such as:
20
Example: Probability Calculation
Calculate the probability of the event that the patient has a cancer but he never smoked and never got radiation, and felt both fever and pain.
   P(F ^ P ^ C ^ ~S ^ ~R) 
   = P(F|C) P(P|C) P(C|~S,~R) P(~S) P(~R)
   = 0.50 * 0.70 * 0.010 * 0.900 * 0.998 
   = 0.0031437
20


20
 
</TEXT>
</DOC>
